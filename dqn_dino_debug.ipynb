{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e391e333",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "012b3b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d64457",
   "metadata": {},
   "source": [
    "### Device agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56dbaaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x7d667a58f7c0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53a7d0",
   "metadata": {},
   "source": [
    "### ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54104574",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268da25",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "baeae425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # For the wrapper with image_size=(84, 84) and k=4 frames:\n",
    "        # After conv layers: 84 -> 20 -> 9 -> 7, so 64 * 7 * 7 = 3136\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3136, 256), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(256, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Input should be [batch, channels, height, width]\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1bde37",
   "metadata": {},
   "source": [
    "### Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "672b41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dino\n",
    "\n",
    "# Create environment with training mode for better learning\n",
    "env = gym.make('Dino-v0', render_mode='rgb_array', game_mode='train', train_frame_limit=5000)\n",
    "# Use the wrapper from the reference implementation for frame stacking\n",
    "env = dino.Wrapper(env, k=4, image_size=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0e2dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make('Dino-v0', render_mode='rgb_array', game_mode='train')\n",
    "test_env_wrapper = dino.Wrapper(test_env, k=4, image_size=(84, 84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c688c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((512, 1024, 3), (4, 84, 84))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "test_env.reset()\n",
    "test_env_wrapper.reset()\n",
    "for _ in range(55):\n",
    "    ts, *_ = test_env.step(0)\n",
    "    sss, *_ = test_env_wrapper.step(0)\n",
    "ts.shape, sss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "feb9169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Original RGB observation from test environment\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"Original RGB Observation\\n(Test Environment)\")\n",
    "plt.imshow(ts)\n",
    "plt.axis('off')\n",
    "\n",
    "# Show all 4 stacked frames from wrapped environment\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"Frame 1 (Oldest)\\n(Wrapped Environment)\")\n",
    "plt.imshow(sss[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Frame 2\\n(Wrapped Environment)\")\n",
    "plt.imshow(sss[1], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Frame 3\\n(Wrapped Environment)\")\n",
    "plt.imshow(sss[2], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"Frame 4 (Newest)\\n(Wrapped Environment)\")\n",
    "plt.imshow(sss[3], cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# Show the shape information\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.1, 0.8, f\"Original shape: {ts.shape}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.6, f\"Wrapped shape: {sss.shape}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.4, f\"Frame shape: {sss[0].shape}\", fontsize=12, transform=plt.gca().transAxes)\n",
    "plt.text(0.1, 0.2, \"Frame stacking: 4 consecutive frames\\nfor temporal information\", fontsize=10, transform=plt.gca().transAxes)\n",
    "plt.title(\"Shape Information\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a92fd0",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a5c5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped observation shape: (4, 84, 84)\n",
      "Processed observation shape: torch.Size([4, 84, 84])\n",
      "Processed observation range: [0.290, 1.000]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_observation(obs):\n",
    "    \"\"\"\n",
    "    Preprocess observation from the wrapped environment\n",
    "    The wrapper already handles grayscale conversion and stacking\n",
    "    Input: (k, height, width) where k=4 stacked frames\n",
    "    Output: Tensor ready for the neural network\n",
    "    \"\"\"\n",
    "    # Convert to tensor if it's numpy array\n",
    "    if isinstance(obs, np.ndarray):\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Normalize to [0, 1] if not already normalized\n",
    "    if obs.max() > 1.0:\n",
    "        obs = obs / 255.0\n",
    "\n",
    "    return obs\n",
    "\n",
    "# Test the preprocessing function with the wrapped environment\n",
    "test_obs, _ = env.reset()\n",
    "print(f\"Wrapped observation shape: {test_obs.shape}\")\n",
    "processed_obs = preprocess_observation(test_obs)\n",
    "print(f\"Processed observation shape: {processed_obs.shape}\")\n",
    "print(f\"Processed observation range: [{processed_obs.min():.3f}, {processed_obs.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07bf92e",
   "metadata": {},
   "source": [
    "#### Setting up hyperparameters, networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ffc924fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 3\n",
      "Wrapped state shape: (4, 84, 84)\n",
      "Number of observation channels: 4\n",
      "✅ Networks recreated with correct architecture!\n",
      "   Policy network: 881827 parameters\n",
      "   Target network: 881827 parameters\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters based on the reference implementation\n",
    "BATCH_SIZE = 8  # Smaller batch size as in reference\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0  # Start with full exploration\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 1000  # Longer decay period\n",
    "TAU = 0.005  # Keep target network update rate\n",
    "LR = 1e-4  # Lower learning rate as in reference\n",
    "\n",
    "# Experience replay settings\n",
    "REPLAY_SIZE = 1000\n",
    "LEARNING_START = 1000  # Start learning after collecting enough experiences\n",
    "TARGET_UPDATE_FREQ = 1  # Update target network every 1 step\n",
    "OPTIMIZE_FREQ = 1  # Optimize every step\n",
    "\n",
    "# Get environment information\n",
    "n_actions = env.action_space.n\n",
    "print(f\"Number of actions: {n_actions}\")\n",
    "\n",
    "# Get observation space - should be (k, height, width) after wrapper\n",
    "state, info = env.reset()\n",
    "print(f\"Wrapped state shape: {state.shape}\")\n",
    "n_observations = state.shape[0]  # Number of stacked frames (k=4)\n",
    "print(f\"Number of observation channels: {n_observations}\")\n",
    "\n",
    "# Initialize networks with correct architecture\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "# Experience replay memory\n",
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "\n",
    "# Training state\n",
    "steps_done = 0\n",
    "\n",
    "print(f\"✅ Networks recreated with correct architecture!\")\n",
    "print(f\"   Policy network: {sum(p.numel() for p in policy_net.parameters())} parameters\")\n",
    "print(f\"   Target network: {sum(p.numel() for p in target_net.parameters())} parameters\")\n",
    "\n",
    "def select_action(state):   # Using epsilon-greedy policy\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # Convert state to tensor if needed and add batch dimension\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            if len(state.shape) == 3:  # [C, H, W] -> [1, C, H, W]\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            # Get Q-values and select best action\n",
    "            q_values = policy_net(state)\n",
    "            action = q_values.max(1)[1]\n",
    "            return action\n",
    "    else:\n",
    "        return torch.tensor([env.action_space.sample()], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d4928",
   "metadata": {},
   "source": [
    "#### Optimize the model after OPTIMIZE_FREQ steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "797aaf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "        \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    # print(non_final_mask)\n",
    "    print(f\"non_final_mask.shape: {non_final_mask.shape}\")\n",
    "    # Process states - convert to tensors if needed\n",
    "    state_batch = []\n",
    "    for state in batch.state:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        state_batch.append(state)\n",
    "    print(f\"State batch before stack length: {len(state_batch)}\")\n",
    "    state_batch = torch.stack(state_batch)\n",
    "    print(f\"State batch after stack shape: {state_batch.shape}\")\n",
    "    \n",
    "    # Process actions\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    if len(action_batch.shape) == 1:\n",
    "        action_batch = action_batch.unsqueeze(1)\n",
    "    print(f\"Action batch: {action_batch}, shape: {action_batch.shape}\")\n",
    "    # Process rewards\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    print(f\"reward batch: {reward_batch}, shape: {reward_batch.shape}\")\n",
    "\n",
    "    # Process next states (only non-final ones)\n",
    "    non_final_next_states = []\n",
    "    for state in batch.next_state:\n",
    "        if state is not None:\n",
    "            if isinstance(state, np.ndarray):\n",
    "                state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            non_final_next_states.append(state)\n",
    "    \n",
    "    if len(non_final_next_states) > 0:\n",
    "        non_final_next_states = torch.stack(non_final_next_states)\n",
    "    else:\n",
    "        non_final_next_states = torch.empty(0, n_observations, 84, 84, device=device)\n",
    "    print(f\"Non-final next states shape: {non_final_next_states.shape}\")\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    print(f\"policy_net(state): {policy_net(state_batch)}\")\n",
    "    print(f\"Q Value: {state_action_values}, shape: {state_action_values.shape}\")\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    print(f\"Next state values: {next_state_values}, shape: {next_state_values.shape}\")\n",
    "    # Compute the expected Q values using Bellman equation\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    print(f\"Expected Q values: {expected_state_action_values}, shape: {expected_state_action_values.shape}\")\n",
    "    # Compute Huber loss (SmoothL1Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient clipping to prevent exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "19046230",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de5f60e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Additional utility functions for training monitoring\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def get_epsilon():\n",
    "    \"\"\"Get current epsilon value for monitoring exploration\"\"\"\n",
    "    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "\n",
    "def print_training_stats():\n",
    "    \"\"\"Print current training statistics\"\"\"\n",
    "    if len(episode_durations) > 0:\n",
    "        avg_duration = np.mean(episode_durations[-10:])  # Last 10 episodes\n",
    "        current_eps = get_epsilon()\n",
    "        print(f\"Steps: {steps_done}, Epsilon: {current_eps:.3f}, Avg Duration (last 10): {avg_duration:.1f}\")\n",
    "\n",
    "def save_obs_result(episode_i: int, obs_arr: list[np.ndarray]):\n",
    "        frames = [Image.fromarray(obs, \"RGB\") for obs in obs_arr]\n",
    "        file_path = os.path.join(\"models\", f\"episode-{episode_i}.gif\")\n",
    "\n",
    "        frames[0].save(\n",
    "            file_path,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            optimize=True,\n",
    "            duration=100,\n",
    "            loop=0,\n",
    "        )\n",
    "\n",
    "# This cell is ready for additional monitoring utilities as needed\n",
    "print(\"Training utilities loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "edc23f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 1 episodes...\n",
      "Device: cuda\n",
      "Learning will start after 1000 steps\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0289, -0.0022,  0.0098],\n",
      "        [-0.0282, -0.0026,  0.0098],\n",
      "        [-0.0287, -0.0020,  0.0111],\n",
      "        [-0.0297, -0.0029,  0.0091],\n",
      "        [-0.0284, -0.0018,  0.0097],\n",
      "        [-0.0284, -0.0028,  0.0096],\n",
      "        [-0.0282, -0.0022,  0.0096],\n",
      "        [-0.0287, -0.0020,  0.0093]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0022],\n",
      "        [ 0.0098],\n",
      "        [ 0.0111],\n",
      "        [ 0.0091],\n",
      "        [ 0.0097],\n",
      "        [-0.0284],\n",
      "        [ 0.0096],\n",
      "        [-0.0287]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0098, 0.0098, 0.0111, 0.0091, 0.0097, 0.0096, 0.0096, 0.0093],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0097, 0.0097, 0.0110, 0.0090, 0.0096, 0.0095, 0.0095, 0.0092],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[0.0161, 0.0094, 0.0107],\n",
      "        [0.0162, 0.0090, 0.0106],\n",
      "        [0.0155, 0.0094, 0.0112],\n",
      "        [0.0154, 0.0092, 0.0097],\n",
      "        [0.0160, 0.0090, 0.0113],\n",
      "        [0.0159, 0.0089, 0.0100],\n",
      "        [0.0164, 0.0093, 0.0110],\n",
      "        [0.0161, 0.0097, 0.0107]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[0.0161],\n",
      "        [0.0106],\n",
      "        [0.0094],\n",
      "        [0.0154],\n",
      "        [0.0113],\n",
      "        [0.0100],\n",
      "        [0.0110],\n",
      "        [0.0107]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0093, 0.0094, 0.0100, 0.0089, 0.0099, 0.0093, 0.0100, 0.0098],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0092, -0.9907,  0.0099,  0.0088,  0.0098,  0.0092, -0.9901,  0.0097],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0145,  0.0048, -0.0274],\n",
      "        [ 0.0145,  0.0048, -0.0271],\n",
      "        [ 0.0143,  0.0051, -0.0267],\n",
      "        [ 0.0140,  0.0052, -0.0266],\n",
      "        [ 0.0141,  0.0052, -0.0269],\n",
      "        [ 0.0139,  0.0045, -0.0272],\n",
      "        [ 0.0138,  0.0051, -0.0268],\n",
      "        [ 0.0140,  0.0054, -0.0265]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0274],\n",
      "        [-0.0271],\n",
      "        [-0.0267],\n",
      "        [ 0.0140],\n",
      "        [-0.0269],\n",
      "        [-0.0272],\n",
      "        [ 0.0138],\n",
      "        [ 0.0054]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0091, 0.0095, 0.0095, 0.0093, 0.0092, 0.0094, 0.0093, 0.0100],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0090, 0.0094, 0.0094, 0.0092, 0.0091, 0.0093, 0.0092, 0.0099],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0101,  0.0004, -0.0479],\n",
      "        [ 0.0096,  0.0010, -0.0471],\n",
      "        [ 0.0109,  0.0005, -0.0483],\n",
      "        [ 0.0108,  0.0005, -0.0482],\n",
      "        [ 0.0108,  0.0006, -0.0480],\n",
      "        [ 0.0109,  0.0005, -0.0481],\n",
      "        [ 0.0109,  0.0004, -0.0484],\n",
      "        [ 0.0098,  0.0007, -0.0476]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0004],\n",
      "        [ 0.0010],\n",
      "        [-0.0483],\n",
      "        [-0.0482],\n",
      "        [ 0.0006],\n",
      "        [-0.0481],\n",
      "        [-0.0484],\n",
      "        [ 0.0007]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0093, 0.0095, 0.0091, 0.0091, 0.0091, 0.0091, 0.0089, 0.0093],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0092, 0.0094, 0.0090, 0.0091, 0.0090, 0.0090, 0.0088, 0.0092],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1., -1.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0084,  0.0125, -0.0568],\n",
      "        [ 0.0084,  0.0126, -0.0565],\n",
      "        [ 0.0082,  0.0124, -0.0570],\n",
      "        [ 0.0082,  0.0125, -0.0565],\n",
      "        [ 0.0083,  0.0125, -0.0565],\n",
      "        [ 0.0084,  0.0124, -0.0570],\n",
      "        [ 0.0076,  0.0125, -0.0564],\n",
      "        [ 0.0077,  0.0130, -0.0564]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0568],\n",
      "        [-0.0565],\n",
      "        [-0.0570],\n",
      "        [-0.0565],\n",
      "        [-0.0565],\n",
      "        [-0.0570],\n",
      "        [ 0.0076],\n",
      "        [-0.0564]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0087, 0.0086, 0.0083, 0.0091, 0.0086, 0.0086, 0.0085, 0.0089],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0087, -0.9915, -0.9918, -0.9910,  0.0085, -0.9915,  0.0085,  0.0088],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0056,  0.0159, -0.0837],\n",
      "        [ 0.0059,  0.0159, -0.0828],\n",
      "        [ 0.0060,  0.0161, -0.0834],\n",
      "        [ 0.0053,  0.0165, -0.0836],\n",
      "        [ 0.0056,  0.0154, -0.0843],\n",
      "        [ 0.0059,  0.0156, -0.0837],\n",
      "        [ 0.0065,  0.0157, -0.0835],\n",
      "        [ 0.0059,  0.0158, -0.0837]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0056],\n",
      "        [ 0.0159],\n",
      "        [-0.0834],\n",
      "        [ 0.0053],\n",
      "        [ 0.0056],\n",
      "        [-0.0837],\n",
      "        [-0.0835],\n",
      "        [-0.0837]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0081, 0.0088, 0.0072, 0.0081, 0.0073, 0.0081, 0.0080, 0.0082],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9919,  0.0087,  0.0071,  0.0080,  1.0073, -0.9920,  0.0080,  0.0081],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0028,  0.0178, -0.1107],\n",
      "        [ 0.0042,  0.0169, -0.1109],\n",
      "        [ 0.0025,  0.0182, -0.1111],\n",
      "        [ 0.0027,  0.0179, -0.1108],\n",
      "        [ 0.0029,  0.0176, -0.1116],\n",
      "        [ 0.0031,  0.0184, -0.1102],\n",
      "        [ 0.0030,  0.0177, -0.1116],\n",
      "        [ 0.0033,  0.0179, -0.1113]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0178],\n",
      "        [ 0.0169],\n",
      "        [ 0.0182],\n",
      "        [ 0.0179],\n",
      "        [-0.1116],\n",
      "        [-0.1102],\n",
      "        [-0.1116],\n",
      "        [ 0.0179]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0085, 0.0069, 0.0072, 0.0074, 0.0073, 0.0081, 0.0074, 0.0071],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0084, 0.0068, 0.0071, 0.0073, 0.0073, 0.0080, 0.0073, 0.0071],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0004,  0.0186, -0.1327],\n",
      "        [ 0.0002,  0.0180, -0.1325],\n",
      "        [ 0.0003,  0.0185, -0.1323],\n",
      "        [ 0.0002,  0.0185, -0.1330],\n",
      "        [ 0.0002,  0.0184, -0.1329],\n",
      "        [-0.0005,  0.0190, -0.1325],\n",
      "        [-0.0006,  0.0189, -0.1327],\n",
      "        [-0.0003,  0.0181, -0.1332]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1327],\n",
      "        [-0.1325],\n",
      "        [-0.1323],\n",
      "        [-0.1330],\n",
      "        [-0.1329],\n",
      "        [ 0.0190],\n",
      "        [ 0.0189],\n",
      "        [-0.1332]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0066, 0.0067, 0.0071, 0.0066, 0.0067, 0.0069, 0.0067, 0.0065],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0066, 0.0066, 0.0070, 0.0065, 0.0066, 0.0068, 0.0066, 0.0065],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0022,  0.0194, -0.1425],\n",
      "        [-0.0028,  0.0202, -0.1429],\n",
      "        [-0.0021,  0.0194, -0.1425],\n",
      "        [-0.0023,  0.0194, -0.1424],\n",
      "        [-0.0028,  0.0193, -0.1425],\n",
      "        [-0.0024,  0.0192, -0.1424],\n",
      "        [-0.0020,  0.0196, -0.1426],\n",
      "        [-0.0018,  0.0197, -0.1419]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1425],\n",
      "        [-0.0028],\n",
      "        [-0.1425],\n",
      "        [-0.1424],\n",
      "        [ 0.0193],\n",
      "        [-0.1424],\n",
      "        [-0.1426],\n",
      "        [ 0.0197]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0057, 0.0048, 0.0057, 0.0057, 0.0060, 0.0056, 0.0055, 0.0057],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0056,  0.0047,  0.0056,  0.0057,  0.0059, -0.9945,  0.0054,  0.0057],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0034,  0.0202, -0.1534],\n",
      "        [-0.0034,  0.0205, -0.1527],\n",
      "        [-0.0031,  0.0205, -0.1530],\n",
      "        [-0.0036,  0.0197, -0.1543],\n",
      "        [-0.0035,  0.0200, -0.1540],\n",
      "        [-0.0045,  0.0202, -0.1533],\n",
      "        [-0.0037,  0.0204, -0.1533],\n",
      "        [-0.0041,  0.0203, -0.1541]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1534],\n",
      "        [-0.1527],\n",
      "        [-0.1530],\n",
      "        [-0.1543],\n",
      "        [-0.1540],\n",
      "        [ 0.0202],\n",
      "        [ 0.0204],\n",
      "        [-0.0041]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0048, 0.0055, 0.0050, 0.0040, 0.0048, 0.0043, 0.0053, 0.0044],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0048, 0.0055, 0.0050, 0.0040, 0.0048, 0.0043, 0.0053, 0.0044],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 1., 0., 0., 1., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0044,  0.0220, -0.1472],\n",
      "        [-0.0041,  0.0216, -0.1481],\n",
      "        [-0.0036,  0.0215, -0.1479],\n",
      "        [-0.0041,  0.0215, -0.1482],\n",
      "        [-0.0037,  0.0218, -0.1479],\n",
      "        [-0.0045,  0.0218, -0.1483],\n",
      "        [-0.0048,  0.0216, -0.1482],\n",
      "        [-0.0046,  0.0220, -0.1475]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1472],\n",
      "        [-0.1481],\n",
      "        [-0.1479],\n",
      "        [-0.1482],\n",
      "        [-0.0037],\n",
      "        [-0.0045],\n",
      "        [-0.1482],\n",
      "        [-0.1475]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0046, 0.0038, 0.0041, 0.0038, 0.0036, 0.0033, 0.0037, 0.0044],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0046, 0.0038, 1.0040, 0.0037, 0.0036, 1.0032, 0.0036, 0.0044],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0011,  0.0244, -0.1317],\n",
      "        [ 0.0004,  0.0241, -0.1312],\n",
      "        [-0.0004,  0.0239, -0.1320],\n",
      "        [-0.0008,  0.0249, -0.1318],\n",
      "        [-0.0006,  0.0241, -0.1322],\n",
      "        [-0.0011,  0.0241, -0.1311],\n",
      "        [-0.0010,  0.0243, -0.1315],\n",
      "        [-0.0006,  0.0237, -0.1324]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0244],\n",
      "        [-0.1312],\n",
      "        [-0.1320],\n",
      "        [-0.0008],\n",
      "        [-0.1322],\n",
      "        [-0.1311],\n",
      "        [ 0.0243],\n",
      "        [-0.1324]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0030, 0.0032, 0.0027, 0.0024, 0.0029, 0.0031, 0.0029, 0.0021],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0030, 0.0032, 0.0027, 0.0023, 0.0029, 0.0030, 0.0028, 0.0021],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0011,  0.0254, -0.1148],\n",
      "        [ 0.0024,  0.0264, -0.1136],\n",
      "        [ 0.0012,  0.0252, -0.1144],\n",
      "        [ 0.0013,  0.0254, -0.1139],\n",
      "        [ 0.0009,  0.0259, -0.1145],\n",
      "        [ 0.0015,  0.0254, -0.1137],\n",
      "        [ 0.0012,  0.0253, -0.1140],\n",
      "        [ 0.0013,  0.0255, -0.1139]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1148],\n",
      "        [ 0.0024],\n",
      "        [-0.1144],\n",
      "        [ 0.0013],\n",
      "        [ 0.0009],\n",
      "        [ 0.0254],\n",
      "        [-0.1140],\n",
      "        [-0.1139]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0019, 0.0019, 0.0018, 0.0018, 0.0017, 0.0019, 0.0025, 0.0018],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0019, 0.0019, 0.0017, 0.0018, 0.0017, 0.0019, 0.0025, 0.0018],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0020,  0.0263, -0.0974],\n",
      "        [ 0.0020,  0.0261, -0.0980],\n",
      "        [ 0.0014,  0.0268, -0.0977],\n",
      "        [ 0.0023,  0.0262, -0.0973],\n",
      "        [ 0.0033,  0.0273, -0.0971],\n",
      "        [ 0.0027,  0.0261, -0.0978],\n",
      "        [ 0.0021,  0.0264, -0.0974],\n",
      "        [ 0.0021,  0.0265, -0.0973]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0974],\n",
      "        [-0.0980],\n",
      "        [ 0.0268],\n",
      "        [ 0.0262],\n",
      "        [ 0.0033],\n",
      "        [ 0.0261],\n",
      "        [-0.0974],\n",
      "        [-0.0973]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0011, 0.0010, 0.0013, 0.0013, 0.0010, 0.0012, 0.0010, 0.0009],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0011, 0.0009, 0.0013, 0.0013, 0.0010, 0.0012, 0.0010, 0.0009],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0018,  0.0272, -0.0817],\n",
      "        [ 0.0020,  0.0269, -0.0823],\n",
      "        [ 0.0015,  0.0269, -0.0820],\n",
      "        [ 0.0020,  0.0268, -0.0815],\n",
      "        [ 0.0018,  0.0270, -0.0816],\n",
      "        [ 0.0021,  0.0268, -0.0818],\n",
      "        [ 0.0018,  0.0271, -0.0821],\n",
      "        [ 0.0023,  0.0269, -0.0822]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0272],\n",
      "        [ 0.0269],\n",
      "        [ 0.0015],\n",
      "        [-0.0815],\n",
      "        [-0.0816],\n",
      "        [-0.0818],\n",
      "        [ 0.0018],\n",
      "        [-0.0822]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([ 4.7091e-05, -3.9974e-04,  3.8771e-04,  3.7734e-04,  6.4621e-04,\n",
      "         5.4371e-04,  1.1781e-04, -1.9962e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 4.6620e-05, -3.9574e-04,  3.8383e-04, -9.9963e-01,  6.3975e-04,\n",
      "         5.3827e-04,  1.1664e-04, -1.9763e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0026,  0.0271, -0.0723],\n",
      "        [ 0.0029,  0.0263, -0.0731],\n",
      "        [ 0.0030,  0.0263, -0.0719],\n",
      "        [ 0.0027,  0.0266, -0.0723],\n",
      "        [ 0.0024,  0.0268, -0.0726],\n",
      "        [ 0.0027,  0.0267, -0.0724],\n",
      "        [ 0.0023,  0.0270, -0.0725],\n",
      "        [ 0.0026,  0.0271, -0.0719]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0271],\n",
      "        [-0.0731],\n",
      "        [-0.0719],\n",
      "        [-0.0723],\n",
      "        [ 0.0024],\n",
      "        [-0.0724],\n",
      "        [ 0.0023],\n",
      "        [-0.0719]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([ 0.0005, -0.0002, -0.0006,  0.0002,  0.0005, -0.0003,  0.0002,  0.0009],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 4.6634e-04, -2.0224e-04, -6.1084e-04, -9.9977e-01,  4.8732e-04,\n",
      "        -2.9771e-04,  2.3577e-04,  8.9046e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0033,  0.0264, -0.0671],\n",
      "        [ 0.0038,  0.0261, -0.0674],\n",
      "        [ 0.0034,  0.0263, -0.0672],\n",
      "        [ 0.0033,  0.0264, -0.0670],\n",
      "        [ 0.0035,  0.0261, -0.0675],\n",
      "        [ 0.0031,  0.0264, -0.0666],\n",
      "        [ 0.0032,  0.0264, -0.0674],\n",
      "        [ 0.0039,  0.0265, -0.0672]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0264],\n",
      "        [-0.0674],\n",
      "        [ 0.0034],\n",
      "        [ 0.0264],\n",
      "        [-0.0675],\n",
      "        [-0.0666],\n",
      "        [ 0.0032],\n",
      "        [-0.0672]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([ 3.1670e-04,  2.5347e-04,  6.0096e-04, -1.0229e-04,  1.9932e-04,\n",
      "        -5.5548e-05,  5.6975e-04,  9.0035e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 3.1353e-04,  2.5093e-04,  5.9495e-04, -1.0127e-04, -9.9980e-01,\n",
      "        -5.4993e-05,  5.6405e-04,  8.9134e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0045,  0.0253, -0.0658],\n",
      "        [ 0.0047,  0.0250, -0.0653],\n",
      "        [ 0.0047,  0.0249, -0.0651],\n",
      "        [ 0.0044,  0.0253, -0.0656],\n",
      "        [ 0.0044,  0.0254, -0.0654],\n",
      "        [ 0.0048,  0.0253, -0.0657],\n",
      "        [ 0.0043,  0.0255, -0.0654],\n",
      "        [ 0.0049,  0.0253, -0.0656]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0253],\n",
      "        [-0.0653],\n",
      "        [-0.0651],\n",
      "        [-0.0656],\n",
      "        [-0.0654],\n",
      "        [-0.0657],\n",
      "        [-0.0654],\n",
      "        [-0.0656]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([ 0.0009, -0.0001, -0.0002,  0.0007,  0.0007,  0.0006,  0.0010,  0.0004],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0009, -0.0001, -0.0002,  0.0007,  0.0007,  0.0006,  0.0010,  0.0003],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0056,  0.0245, -0.0621],\n",
      "        [ 0.0053,  0.0248, -0.0628],\n",
      "        [ 0.0058,  0.0245, -0.0627],\n",
      "        [ 0.0052,  0.0246, -0.0625],\n",
      "        [ 0.0051,  0.0248, -0.0628],\n",
      "        [ 0.0051,  0.0248, -0.0624],\n",
      "        [ 0.0049,  0.0243, -0.0630],\n",
      "        [ 0.0051,  0.0250, -0.0625]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0621],\n",
      "        [-0.0628],\n",
      "        [-0.0627],\n",
      "        [-0.0625],\n",
      "        [-0.0628],\n",
      "        [ 0.0248],\n",
      "        [-0.0630],\n",
      "        [ 0.0051]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([8.2637e-05, 7.1085e-04, 5.1768e-04, 8.0937e-04, 6.4430e-04, 8.4535e-04,\n",
      "        5.8571e-04, 1.1652e-03], device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 8.1811e-05,  7.0374e-04,  5.1250e-04,  8.0128e-04, -9.9936e-01,\n",
      "         8.3689e-04,  5.7985e-04,  1.1536e-03], device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0061,  0.0241, -0.0619],\n",
      "        [ 0.0060,  0.0240, -0.0618],\n",
      "        [ 0.0064,  0.0233, -0.0616],\n",
      "        [ 0.0059,  0.0236, -0.0621],\n",
      "        [ 0.0062,  0.0242, -0.0619],\n",
      "        [ 0.0060,  0.0241, -0.0620],\n",
      "        [ 0.0062,  0.0237, -0.0614],\n",
      "        [ 0.0064,  0.0236, -0.0613]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0061],\n",
      "        [ 0.0240],\n",
      "        [-0.0616],\n",
      "        [ 0.0236],\n",
      "        [ 0.0062],\n",
      "        [ 0.0060],\n",
      "        [ 0.0237],\n",
      "        [-0.0613]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0014, 0.0010, 0.0004, 0.0003, 0.0014, 0.0015, 0.0008, 0.0007],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.4283e-03, 1.0252e-03, 3.7757e-04, 1.0003e+00, 1.4173e-03, 1.5129e-03,\n",
      "        7.7441e-04, 7.1539e-04], device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0069,  0.0255, -0.0607],\n",
      "        [ 0.0068,  0.0257, -0.0606],\n",
      "        [ 0.0066,  0.0254, -0.0610],\n",
      "        [ 0.0069,  0.0257, -0.0611],\n",
      "        [ 0.0069,  0.0255, -0.0608],\n",
      "        [ 0.0065,  0.0257, -0.0605],\n",
      "        [ 0.0071,  0.0258, -0.0610],\n",
      "        [ 0.0066,  0.0255, -0.0605]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0607],\n",
      "        [ 0.0257],\n",
      "        [ 0.0254],\n",
      "        [-0.0611],\n",
      "        [-0.0608],\n",
      "        [ 0.0257],\n",
      "        [-0.0610],\n",
      "        [-0.0605]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0014, 0.0011, 0.0006, 0.0013, 0.0014, 0.0015, 0.0016, 0.0017],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0014,  0.0011,  1.0006, -0.9987,  0.0014,  0.0015,  0.0016,  0.0017],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  1.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0077,  0.0287, -0.0612],\n",
      "        [ 0.0080,  0.0290, -0.0620],\n",
      "        [ 0.0077,  0.0290, -0.0616],\n",
      "        [ 0.0078,  0.0286, -0.0614],\n",
      "        [ 0.0088,  0.0284, -0.0616],\n",
      "        [ 0.0079,  0.0291, -0.0617],\n",
      "        [ 0.0081,  0.0288, -0.0615],\n",
      "        [ 0.0079,  0.0289, -0.0614]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0077],\n",
      "        [-0.0620],\n",
      "        [-0.0616],\n",
      "        [ 0.0286],\n",
      "        [-0.0616],\n",
      "        [ 0.0079],\n",
      "        [-0.0615],\n",
      "        [ 0.0289]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0020, 0.0016, 0.0016, 0.0020, 0.0012, 0.0020, 0.0017, 0.0016],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0020,  0.0016,  0.0016,  0.0019,  1.0012,  0.0020, -0.9983,  0.0016],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0087,  0.0315, -0.0616],\n",
      "        [ 0.0089,  0.0317, -0.0610],\n",
      "        [ 0.0088,  0.0310, -0.0615],\n",
      "        [ 0.0088,  0.0318, -0.0612],\n",
      "        [ 0.0088,  0.0315, -0.0616],\n",
      "        [ 0.0087,  0.0315, -0.0616],\n",
      "        [ 0.0087,  0.0315, -0.0616],\n",
      "        [ 0.0085,  0.0316, -0.0614]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0616],\n",
      "        [-0.0610],\n",
      "        [ 0.0088],\n",
      "        [ 0.0088],\n",
      "        [-0.0616],\n",
      "        [-0.0616],\n",
      "        [-0.0616],\n",
      "        [ 0.0316]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0021, 0.0025, 0.0014, 0.0028, 0.0021, 0.0020, 0.0020, 0.0021],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0021, 0.0025, 0.0014, 0.0028, 0.0021, 0.0020, 0.0020, 0.0021],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  1., -1.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0090,  0.0339, -0.0603],\n",
      "        [ 0.0089,  0.0340, -0.0606],\n",
      "        [ 0.0089,  0.0339, -0.0607],\n",
      "        [ 0.0085,  0.0338, -0.0602],\n",
      "        [ 0.0092,  0.0334, -0.0603],\n",
      "        [ 0.0089,  0.0340, -0.0606],\n",
      "        [ 0.0087,  0.0341, -0.0603],\n",
      "        [ 0.0090,  0.0342, -0.0610]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0603],\n",
      "        [-0.0606],\n",
      "        [-0.0607],\n",
      "        [ 0.0338],\n",
      "        [ 0.0092],\n",
      "        [-0.0606],\n",
      "        [ 0.0341],\n",
      "        [-0.0610]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0031, 0.0022, 0.0025, 0.0024, 0.0017, 0.0024, 0.0023, 0.0024],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0031,  0.0022, -0.9975,  0.0024,  1.0017, -0.9976,  0.0023, -0.9977],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0125,  0.0359, -0.0673],\n",
      "        [ 0.0129,  0.0357, -0.0665],\n",
      "        [ 0.0126,  0.0357, -0.0673],\n",
      "        [ 0.0123,  0.0361, -0.0675],\n",
      "        [ 0.0122,  0.0363, -0.0673],\n",
      "        [ 0.0124,  0.0351, -0.0667],\n",
      "        [ 0.0126,  0.0358, -0.0675],\n",
      "        [ 0.0121,  0.0359, -0.0670]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0125],\n",
      "        [-0.0665],\n",
      "        [-0.0673],\n",
      "        [ 0.0123],\n",
      "        [ 0.0122],\n",
      "        [ 0.0124],\n",
      "        [-0.0675],\n",
      "        [ 0.0121]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0030, 0.0029, 0.0028, 0.0032, 0.0032, 0.0019, 0.0026, 0.0032],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9971,  0.0029,  0.0028,  0.0032, -0.9968,  0.0019,  0.0026,  0.0032],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0114,  0.0372, -0.0713],\n",
      "        [ 0.0124,  0.0365, -0.0710],\n",
      "        [ 0.0111,  0.0369, -0.0709],\n",
      "        [ 0.0119,  0.0367, -0.0713],\n",
      "        [ 0.0114,  0.0369, -0.0714],\n",
      "        [ 0.0111,  0.0366, -0.0711],\n",
      "        [ 0.0114,  0.0368, -0.0704],\n",
      "        [ 0.0118,  0.0368, -0.0710]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0713],\n",
      "        [-0.0710],\n",
      "        [ 0.0369],\n",
      "        [ 0.0367],\n",
      "        [-0.0714],\n",
      "        [ 0.0366],\n",
      "        [ 0.0368],\n",
      "        [ 0.0368]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0033, 0.0025, 0.0035, 0.0029, 0.0031, 0.0032, 0.0031, 0.0029],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0032,  0.0025,  0.0034,  0.0029, -0.9969,  0.0032,  0.0031,  0.0029],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0108,  0.0380, -0.0771],\n",
      "        [ 0.0106,  0.0379, -0.0773],\n",
      "        [ 0.0108,  0.0378, -0.0769],\n",
      "        [ 0.0106,  0.0380, -0.0774],\n",
      "        [ 0.0110,  0.0372, -0.0769],\n",
      "        [ 0.0103,  0.0375, -0.0769],\n",
      "        [ 0.0106,  0.0379, -0.0775],\n",
      "        [ 0.0111,  0.0377, -0.0769]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0108],\n",
      "        [ 0.0106],\n",
      "        [ 0.0378],\n",
      "        [ 0.0106],\n",
      "        [-0.0769],\n",
      "        [-0.0769],\n",
      "        [ 0.0106],\n",
      "        [-0.0769]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0042, 0.0038, 0.0041, 0.0038, 0.0028, 0.0040, 0.0038, 0.0040],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0042, 0.0038, 0.0040, 0.0038, 0.0028, 0.0040, 0.0037, 0.0039],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0102,  0.0382, -0.0819],\n",
      "        [ 0.0097,  0.0387, -0.0820],\n",
      "        [ 0.0100,  0.0383, -0.0818],\n",
      "        [ 0.0103,  0.0383, -0.0817],\n",
      "        [ 0.0098,  0.0385, -0.0821],\n",
      "        [ 0.0103,  0.0384, -0.0821],\n",
      "        [ 0.0094,  0.0387, -0.0817],\n",
      "        [ 0.0096,  0.0384, -0.0818]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0382],\n",
      "        [ 0.0097],\n",
      "        [-0.0818],\n",
      "        [ 0.0383],\n",
      "        [-0.0821],\n",
      "        [-0.0821],\n",
      "        [ 0.0387],\n",
      "        [ 0.0384]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0037, 0.0043, 0.0035, 0.0037, 0.0038, 0.0034, 0.0047, 0.0040],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0037,  0.0042,  1.0035,  0.0037, -0.9962,  0.0034,  0.0047,  0.0040],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0088,  0.0390, -0.0860],\n",
      "        [ 0.0088,  0.0389, -0.0857],\n",
      "        [ 0.0089,  0.0388, -0.0858],\n",
      "        [ 0.0087,  0.0390, -0.0859],\n",
      "        [ 0.0090,  0.0389, -0.0858],\n",
      "        [ 0.0086,  0.0385, -0.0854],\n",
      "        [ 0.0091,  0.0389, -0.0854],\n",
      "        [ 0.0087,  0.0387, -0.0859]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0860],\n",
      "        [-0.0857],\n",
      "        [-0.0858],\n",
      "        [ 0.0087],\n",
      "        [-0.0858],\n",
      "        [-0.0854],\n",
      "        [-0.0854],\n",
      "        [-0.0859]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0042, 0.0042, 0.0041, 0.0046, 0.0042, 0.0044, 0.0046, 0.0040],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0041, 0.0042, 0.0041, 0.0045, 0.0042, 0.0044, 0.0046, 0.0039],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  1.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0078,  0.0387, -0.0875],\n",
      "        [ 0.0079,  0.0376, -0.0869],\n",
      "        [ 0.0074,  0.0390, -0.0867],\n",
      "        [ 0.0071,  0.0387, -0.0872],\n",
      "        [ 0.0078,  0.0386, -0.0860],\n",
      "        [ 0.0074,  0.0392, -0.0868],\n",
      "        [ 0.0073,  0.0393, -0.0865],\n",
      "        [ 0.0077,  0.0390, -0.0869]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0875],\n",
      "        [-0.0869],\n",
      "        [-0.0867],\n",
      "        [ 0.0387],\n",
      "        [-0.0860],\n",
      "        [ 0.0392],\n",
      "        [ 0.0073],\n",
      "        [ 0.0077]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0050, 0.0045, 0.0050, 0.0041, 0.0040, 0.0051, 0.0058, 0.0045],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0050,  0.0044,  0.0049,  1.0040,  0.0040,  0.0051,  0.0057, -0.9956],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0037,  0.0413, -0.0858],\n",
      "        [ 0.0046,  0.0397, -0.0866],\n",
      "        [ 0.0044,  0.0411, -0.0870],\n",
      "        [ 0.0040,  0.0412, -0.0867],\n",
      "        [ 0.0047,  0.0410, -0.0863],\n",
      "        [ 0.0049,  0.0411, -0.0864],\n",
      "        [ 0.0046,  0.0410, -0.0864],\n",
      "        [ 0.0041,  0.0415, -0.0862]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0413],\n",
      "        [-0.0866],\n",
      "        [-0.0870],\n",
      "        [ 0.0412],\n",
      "        [ 0.0047],\n",
      "        [ 0.0049],\n",
      "        [-0.0864],\n",
      "        [-0.0862]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0052, 0.0049, 0.0051, 0.0051, 0.0048, 0.0051, 0.0057, 0.0053],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0051, 0.0048, 0.0050, 1.0051, 0.0048, 0.0050, 0.0056, 0.0053],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0015,  0.0448, -0.0853],\n",
      "        [ 0.0016,  0.0449, -0.0850],\n",
      "        [ 0.0015,  0.0449, -0.0851],\n",
      "        [ 0.0010,  0.0451, -0.0856],\n",
      "        [ 0.0012,  0.0450, -0.0854],\n",
      "        [ 0.0012,  0.0456, -0.0856],\n",
      "        [ 0.0008,  0.0450, -0.0853],\n",
      "        [ 0.0011,  0.0451, -0.0855]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0853],\n",
      "        [ 0.0016],\n",
      "        [-0.0851],\n",
      "        [-0.0856],\n",
      "        [-0.0854],\n",
      "        [ 0.0012],\n",
      "        [ 0.0450],\n",
      "        [-0.0855]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0052, 0.0051, 0.0053, 0.0056, 0.0054, 0.0061, 0.0053, 0.0055],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0051, 1.0051, 0.0053, 0.0055, 0.0053, 0.0061, 0.0053, 0.0055],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0006,  0.0483, -0.0834],\n",
      "        [ 0.0005,  0.0482, -0.0833],\n",
      "        [ 0.0005,  0.0484, -0.0836],\n",
      "        [ 0.0006,  0.0484, -0.0830],\n",
      "        [ 0.0008,  0.0488, -0.0826],\n",
      "        [ 0.0006,  0.0482, -0.0835],\n",
      "        [ 0.0006,  0.0483, -0.0835],\n",
      "        [ 0.0004,  0.0483, -0.0834]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0834],\n",
      "        [-0.0833],\n",
      "        [-0.0836],\n",
      "        [ 0.0484],\n",
      "        [ 0.0488],\n",
      "        [-0.0835],\n",
      "        [-0.0835],\n",
      "        [-0.0834]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0059, 0.0058, 0.0058, 0.0066, 0.0069, 0.0058, 0.0058, 0.0058],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0059, -0.9942, -0.9943,  0.0065,  0.0068,  0.0058,  0.0058,  0.0057],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0011,  0.0514, -0.0853],\n",
      "        [ 0.0003,  0.0510, -0.0855],\n",
      "        [ 0.0003,  0.0514, -0.0857],\n",
      "        [ 0.0006,  0.0514, -0.0860],\n",
      "        [ 0.0003,  0.0509, -0.0856],\n",
      "        [ 0.0002,  0.0514, -0.0857],\n",
      "        [ 0.0004,  0.0514, -0.0861],\n",
      "        [ 0.0003,  0.0515, -0.0855]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0853],\n",
      "        [ 0.0510],\n",
      "        [-0.0857],\n",
      "        [-0.0860],\n",
      "        [-0.0856],\n",
      "        [ 0.0514],\n",
      "        [-0.0861],\n",
      "        [-0.0855]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0062, 0.0067, 0.0066, 0.0064, 0.0066, 0.0066, 0.0064, 0.0066],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0061,  0.0066,  0.0065,  0.0063,  0.0066,  0.0065,  0.0063, -0.9935],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0005,  0.0541, -0.0898],\n",
      "        [ 0.0004,  0.0544, -0.0897],\n",
      "        [ 0.0006,  0.0544, -0.0892],\n",
      "        [ 0.0006,  0.0542, -0.0898],\n",
      "        [ 0.0006,  0.0542, -0.0898],\n",
      "        [ 0.0016,  0.0536, -0.0889],\n",
      "        [ 0.0005,  0.0534, -0.0893],\n",
      "        [ 0.0006,  0.0539, -0.0890]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0898],\n",
      "        [ 0.0004],\n",
      "        [-0.0892],\n",
      "        [-0.0898],\n",
      "        [-0.0898],\n",
      "        [-0.0889],\n",
      "        [ 0.0005],\n",
      "        [-0.0890]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0069, 0.0072, 0.0072, 0.0068, 0.0068, 0.0069, 0.0075, 0.0081],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0068,  0.0072,  0.0072, -0.9933,  0.0068,  0.0068,  0.0074,  0.0081],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 1., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0007,  0.0570, -0.0950],\n",
      "        [ 0.0006,  0.0564, -0.0942],\n",
      "        [ 0.0007,  0.0563, -0.0943],\n",
      "        [ 0.0010,  0.0568, -0.0943],\n",
      "        [ 0.0001,  0.0571, -0.0938],\n",
      "        [ 0.0003,  0.0573, -0.0945],\n",
      "        [ 0.0007,  0.0571, -0.0949],\n",
      "        [ 0.0018,  0.0563, -0.0937]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0950],\n",
      "        [ 0.0564],\n",
      "        [-0.0943],\n",
      "        [-0.0943],\n",
      "        [ 0.0571],\n",
      "        [ 0.0573],\n",
      "        [-0.0949],\n",
      "        [-0.0937]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0073, 0.0076, 0.0076, 0.0074, 0.0073, 0.0076, 0.0073, 0.0073],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0072, 0.0075, 1.0075, 1.0073, 0.0073, 0.0075, 0.0072, 0.0073],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 6.5218e-04,  5.7772e-02, -9.1063e-02],\n",
      "        [-1.1724e-04,  5.8165e-02, -9.0835e-02],\n",
      "        [-2.0970e-04,  5.8217e-02, -9.0946e-02],\n",
      "        [ 6.7059e-04,  5.7736e-02, -9.1010e-02],\n",
      "        [ 1.6505e-04,  5.8218e-02, -9.1345e-02],\n",
      "        [ 5.2730e-05,  5.8040e-02, -9.1254e-02],\n",
      "        [ 3.0641e-04,  5.8100e-02, -9.1371e-02],\n",
      "        [ 5.3151e-05,  5.8276e-02, -9.1345e-02]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0578],\n",
      "        [ 0.0582],\n",
      "        [ 0.0582],\n",
      "        [ 0.0007],\n",
      "        [-0.0913],\n",
      "        [-0.0913],\n",
      "        [-0.0914],\n",
      "        [-0.0913]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0072, 0.0080, 0.0080, 0.0073, 0.0078, 0.0077, 0.0077, 0.0077],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0071, -0.9921,  0.0079,  0.0072,  0.0078, -0.9924,  0.0076,  0.0077],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0007,  0.0562, -0.0890],\n",
      "        [-0.0011,  0.0570, -0.0894],\n",
      "        [-0.0001,  0.0568, -0.0896],\n",
      "        [-0.0003,  0.0571, -0.0899],\n",
      "        [-0.0007,  0.0571, -0.0897],\n",
      "        [-0.0006,  0.0567, -0.0895],\n",
      "        [-0.0008,  0.0569, -0.0894],\n",
      "        [-0.0007,  0.0564, -0.0892]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0890],\n",
      "        [ 0.0570],\n",
      "        [-0.0896],\n",
      "        [-0.0899],\n",
      "        [-0.0007],\n",
      "        [ 0.0567],\n",
      "        [ 0.0569],\n",
      "        [-0.0007]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0087, 0.0088, 0.0082, 0.0083, 0.0086, 0.0079, 0.0086, 0.0086],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0086, 0.0087, 0.0082, 0.0082, 0.0085, 0.0078, 0.0085, 0.0085],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-6.9654e-05,  5.5067e-02, -8.6842e-02],\n",
      "        [-9.8844e-04,  5.5182e-02, -8.7495e-02],\n",
      "        [-1.4060e-03,  5.5591e-02, -8.7034e-02],\n",
      "        [-6.3193e-04,  5.5415e-02, -8.6745e-02],\n",
      "        [-1.0911e-03,  5.5558e-02, -8.7329e-02],\n",
      "        [-1.6703e-03,  5.5273e-02, -8.6738e-02],\n",
      "        [-1.0179e-03,  5.5202e-02, -8.7565e-02],\n",
      "        [-5.7328e-04,  5.5193e-02, -8.6847e-02]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0551],\n",
      "        [-0.0875],\n",
      "        [ 0.0556],\n",
      "        [-0.0867],\n",
      "        [-0.0873],\n",
      "        [ 0.0553],\n",
      "        [-0.0876],\n",
      "        [-0.0868]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0087, 0.0086, 0.0090, 0.0086, 0.0090, 0.0092, 0.0087, 0.0084],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0086, -0.9915,  0.0089,  0.0086,  0.0089,  0.0091,  0.0086,  0.0083],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0017,  0.0533, -0.0862],\n",
      "        [-0.0016,  0.0536, -0.0864],\n",
      "        [-0.0021,  0.0538, -0.0870],\n",
      "        [-0.0014,  0.0536, -0.0871],\n",
      "        [-0.0018,  0.0539, -0.0869],\n",
      "        [-0.0011,  0.0534, -0.0861],\n",
      "        [-0.0018,  0.0533, -0.0860],\n",
      "        [-0.0010,  0.0532, -0.0864]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0533],\n",
      "        [-0.0016],\n",
      "        [-0.0870],\n",
      "        [-0.0871],\n",
      "        [-0.0018],\n",
      "        [ 0.0534],\n",
      "        [-0.0860],\n",
      "        [-0.0864]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0096, 0.0090, 0.0094, 0.0092, 0.0096, 0.0091, 0.0097, 0.0093],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0095, -0.9911, -0.9907,  0.0091,  0.0096,  0.0090,  0.0096,  0.0092],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 1., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0040,  0.0520, -0.0874],\n",
      "        [-0.0039,  0.0516, -0.0870],\n",
      "        [-0.0036,  0.0519, -0.0880],\n",
      "        [-0.0031,  0.0524, -0.0871],\n",
      "        [-0.0037,  0.0520, -0.0878],\n",
      "        [-0.0034,  0.0519, -0.0877],\n",
      "        [-0.0037,  0.0515, -0.0872],\n",
      "        [-0.0041,  0.0519, -0.0874]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0520],\n",
      "        [-0.0870],\n",
      "        [-0.0880],\n",
      "        [-0.0031],\n",
      "        [-0.0878],\n",
      "        [-0.0877],\n",
      "        [-0.0872],\n",
      "        [ 0.0519]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0101, 0.0099, 0.0096, 0.0109, 0.0098, 0.0096, 0.0100, 0.0098],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0100, 0.0098, 0.0095, 0.0108, 0.0097, 0.0095, 1.0099, 1.0097],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0059,  0.0542, -0.0851],\n",
      "        [-0.0056,  0.0540, -0.0854],\n",
      "        [-0.0057,  0.0535, -0.0843],\n",
      "        [-0.0057,  0.0536, -0.0841],\n",
      "        [-0.0057,  0.0535, -0.0847],\n",
      "        [-0.0059,  0.0542, -0.0851],\n",
      "        [-0.0050,  0.0539, -0.0844],\n",
      "        [-0.0052,  0.0540, -0.0842]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0059],\n",
      "        [-0.0854],\n",
      "        [-0.0843],\n",
      "        [ 0.0536],\n",
      "        [-0.0847],\n",
      "        [-0.0059],\n",
      "        [-0.0844],\n",
      "        [ 0.0540]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0107, 0.0101, 0.0105, 0.0104, 0.0097, 0.0107, 0.0099, 0.0101],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0106, 1.0100, 0.0104, 1.0103, 0.0096, 0.0106, 0.0098, 0.0100],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0070,  0.0566, -0.0790],\n",
      "        [-0.0076,  0.0567, -0.0792],\n",
      "        [-0.0076,  0.0567, -0.0791],\n",
      "        [-0.0074,  0.0566, -0.0792],\n",
      "        [-0.0073,  0.0566, -0.0793],\n",
      "        [-0.0070,  0.0567, -0.0791],\n",
      "        [-0.0076,  0.0568, -0.0782],\n",
      "        [-0.0072,  0.0562, -0.0796]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0790],\n",
      "        [-0.0792],\n",
      "        [-0.0791],\n",
      "        [-0.0792],\n",
      "        [-0.0793],\n",
      "        [-0.0791],\n",
      "        [-0.0076],\n",
      "        [-0.0796]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0111, 0.0109, 0.0109, 0.0111, 0.0107, 0.0111, 0.0119, 0.0105],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0110, 0.0108, 0.0108, 0.0109, 0.0105, 0.0110, 0.0118, 0.0104],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0089,  0.0580, -0.0721],\n",
      "        [-0.0092,  0.0587, -0.0721],\n",
      "        [-0.0088,  0.0581, -0.0712],\n",
      "        [-0.0087,  0.0585, -0.0723],\n",
      "        [-0.0085,  0.0585, -0.0720],\n",
      "        [-0.0083,  0.0584, -0.0714],\n",
      "        [-0.0096,  0.0586, -0.0722],\n",
      "        [-0.0089,  0.0581, -0.0711]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0721],\n",
      "        [-0.0092],\n",
      "        [ 0.0581],\n",
      "        [-0.0723],\n",
      "        [-0.0720],\n",
      "        [-0.0714],\n",
      "        [-0.0722],\n",
      "        [-0.0711]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0110, 0.0116, 0.0115, 0.0114, 0.0115, 0.0109, 0.0112, 0.0115],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0109,  0.0115,  0.0114,  0.0113,  0.0114,  0.0108, -0.9889,  0.0114],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0102,  0.0605, -0.0678],\n",
      "        [-0.0104,  0.0607, -0.0678],\n",
      "        [-0.0100,  0.0598, -0.0666],\n",
      "        [-0.0099,  0.0604, -0.0674],\n",
      "        [-0.0100,  0.0604, -0.0677],\n",
      "        [-0.0101,  0.0601, -0.0662],\n",
      "        [-0.0097,  0.0602, -0.0675],\n",
      "        [-0.0099,  0.0604, -0.0676]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0678],\n",
      "        [-0.0104],\n",
      "        [-0.0666],\n",
      "        [-0.0674],\n",
      "        [-0.0677],\n",
      "        [-0.0662],\n",
      "        [-0.0675],\n",
      "        [-0.0676]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0119, 0.0122, 0.0119, 0.0120, 0.0117, 0.0120, 0.0121, 0.0117],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0118, 0.0121, 0.0118, 0.0119, 0.0116, 0.0119, 0.0119, 0.0116],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0114,  0.0609, -0.0614],\n",
      "        [-0.0110,  0.0614, -0.0623],\n",
      "        [-0.0111,  0.0609, -0.0611],\n",
      "        [-0.0113,  0.0617, -0.0621],\n",
      "        [-0.0110,  0.0615, -0.0615],\n",
      "        [-0.0113,  0.0614, -0.0621],\n",
      "        [-0.0109,  0.0613, -0.0621],\n",
      "        [-0.0113,  0.0615, -0.0620]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0609],\n",
      "        [-0.0623],\n",
      "        [-0.0611],\n",
      "        [-0.0113],\n",
      "        [-0.0615],\n",
      "        [-0.0113],\n",
      "        [-0.0621],\n",
      "        [-0.0620]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0124, 0.0122, 0.0126, 0.0126, 0.0123, 0.0125, 0.0124, 0.0125],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0123, 0.0121, 0.0125, 0.0125, 1.0121, 0.0124, 0.0123, 0.0124],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0122,  0.0615, -0.0543],\n",
      "        [-0.0112,  0.0598, -0.0537],\n",
      "        [-0.0121,  0.0614, -0.0538],\n",
      "        [-0.0117,  0.0610, -0.0542],\n",
      "        [-0.0120,  0.0608, -0.0540],\n",
      "        [-0.0116,  0.0611, -0.0535],\n",
      "        [-0.0117,  0.0612, -0.0547],\n",
      "        [-0.0120,  0.0610, -0.0548]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0543],\n",
      "        [-0.0537],\n",
      "        [-0.0121],\n",
      "        [-0.0542],\n",
      "        [-0.0540],\n",
      "        [-0.0535],\n",
      "        [-0.0547],\n",
      "        [-0.0120]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0131, 0.0125, 0.0136, 0.0131, 0.0122, 0.0126, 0.0128, 0.0131],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0129, 0.0123, 0.0134, 0.0130, 1.0121, 0.0125, 0.0126, 0.0130],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0129,  0.0603, -0.0452],\n",
      "        [-0.0128,  0.0597, -0.0442],\n",
      "        [-0.0129,  0.0603, -0.0453],\n",
      "        [-0.0131,  0.0600, -0.0443],\n",
      "        [-0.0125,  0.0600, -0.0441],\n",
      "        [-0.0133,  0.0601, -0.0448],\n",
      "        [-0.0128,  0.0603, -0.0452],\n",
      "        [-0.0127,  0.0599, -0.0440]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0452],\n",
      "        [ 0.0597],\n",
      "        [-0.0453],\n",
      "        [-0.0443],\n",
      "        [ 0.0600],\n",
      "        [ 0.0601],\n",
      "        [-0.0452],\n",
      "        [ 0.0599]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0133, 0.0137, 0.0133, 0.0138, 0.0133, 0.0136, 0.0133, 0.0141],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0132, 0.0136, 0.0131, 0.0136, 0.0132, 0.0135, 0.0132, 0.0140],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1., -1.,  1.,  1.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0146,  0.0593, -0.0364],\n",
      "        [-0.0136,  0.0591, -0.0368],\n",
      "        [-0.0142,  0.0593, -0.0367],\n",
      "        [-0.0139,  0.0588, -0.0364],\n",
      "        [-0.0142,  0.0592, -0.0365],\n",
      "        [-0.0136,  0.0589, -0.0365],\n",
      "        [-0.0133,  0.0580, -0.0359],\n",
      "        [-0.0135,  0.0588, -0.0360]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0146],\n",
      "        [-0.0368],\n",
      "        [-0.0142],\n",
      "        [ 0.0588],\n",
      "        [-0.0142],\n",
      "        [-0.0365],\n",
      "        [-0.0359],\n",
      "        [ 0.0588]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0150, 0.0138, 0.0144, 0.0140, 0.0145, 0.0139, 0.0141, 0.0137],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9852, -0.9863,  1.0143,  1.0138,  0.0144, -0.9863,  0.0140,  0.0135],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0138,  0.0598, -0.0333],\n",
      "        [-0.0141,  0.0599, -0.0330],\n",
      "        [-0.0140,  0.0599, -0.0336],\n",
      "        [-0.0139,  0.0596, -0.0326],\n",
      "        [-0.0143,  0.0604, -0.0336],\n",
      "        [-0.0141,  0.0599, -0.0332],\n",
      "        [-0.0139,  0.0602, -0.0337],\n",
      "        [-0.0138,  0.0599, -0.0332]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0598],\n",
      "        [ 0.0599],\n",
      "        [ 0.0599],\n",
      "        [-0.0139],\n",
      "        [-0.0143],\n",
      "        [ 0.0599],\n",
      "        [-0.0337],\n",
      "        [ 0.0599]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0146, 0.0148, 0.0149, 0.0147, 0.0150, 0.0147, 0.0145, 0.0147],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0145, 0.0147, 0.0148, 0.0146, 0.0148, 0.0145, 0.0143, 0.0145],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0136,  0.0597, -0.0291],\n",
      "        [-0.0145,  0.0606, -0.0307],\n",
      "        [-0.0142,  0.0605, -0.0304],\n",
      "        [-0.0141,  0.0605, -0.0304],\n",
      "        [-0.0140,  0.0605, -0.0307],\n",
      "        [-0.0135,  0.0599, -0.0295],\n",
      "        [-0.0139,  0.0602, -0.0292],\n",
      "        [-0.0142,  0.0605, -0.0305]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0597],\n",
      "        [-0.0307],\n",
      "        [-0.0304],\n",
      "        [-0.0304],\n",
      "        [-0.0140],\n",
      "        [ 0.0599],\n",
      "        [-0.0292],\n",
      "        [-0.0305]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0152, 0.0153, 0.0151, 0.0151, 0.0155, 0.0150, 0.0161, 0.0150],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0150, 0.0151, 0.0149, 0.0150, 0.0154, 0.0148, 0.0160, 0.0149],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0145,  0.0605, -0.0271],\n",
      "        [-0.0140,  0.0602, -0.0264],\n",
      "        [-0.0146,  0.0605, -0.0274],\n",
      "        [-0.0148,  0.0608, -0.0272],\n",
      "        [-0.0145,  0.0601, -0.0265],\n",
      "        [-0.0148,  0.0607, -0.0269],\n",
      "        [-0.0144,  0.0608, -0.0271],\n",
      "        [-0.0145,  0.0605, -0.0272]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0271],\n",
      "        [-0.0140],\n",
      "        [-0.0274],\n",
      "        [-0.0148],\n",
      "        [-0.0265],\n",
      "        [-0.0148],\n",
      "        [-0.0271],\n",
      "        [-0.0272]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0156, 0.0155, 0.0159, 0.0170, 0.0154, 0.0165, 0.0158, 0.0156],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0155, 0.0154, 0.0157, 0.0168, 0.0152, 0.0164, 0.0156, 0.0155],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0., -1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0152,  0.0607, -0.0240],\n",
      "        [-0.0149,  0.0604, -0.0240],\n",
      "        [-0.0144,  0.0603, -0.0241],\n",
      "        [-0.0148,  0.0605, -0.0238],\n",
      "        [-0.0151,  0.0606, -0.0237],\n",
      "        [-0.0147,  0.0604, -0.0238],\n",
      "        [-0.0141,  0.0601, -0.0229],\n",
      "        [-0.0147,  0.0602, -0.0231]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0152],\n",
      "        [-0.0240],\n",
      "        [-0.0241],\n",
      "        [-0.0238],\n",
      "        [-0.0151],\n",
      "        [-0.0238],\n",
      "        [ 0.0601],\n",
      "        [ 0.0602]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0168, 0.0158, 0.0160, 0.0162, 0.0165, 0.0162, 0.0161, 0.0160],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9834,  0.0157, -0.9841, -0.9840,  0.0164,  0.0160,  0.0160,  0.0158],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0161,  0.0608, -0.0245],\n",
      "        [-0.0165,  0.0599, -0.0235],\n",
      "        [-0.0160,  0.0605, -0.0241],\n",
      "        [-0.0160,  0.0604, -0.0242],\n",
      "        [-0.0161,  0.0598, -0.0232],\n",
      "        [-0.0163,  0.0605, -0.0244],\n",
      "        [-0.0158,  0.0606, -0.0242],\n",
      "        [-0.0162,  0.0607, -0.0239]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0245],\n",
      "        [-0.0235],\n",
      "        [-0.0241],\n",
      "        [-0.0242],\n",
      "        [-0.0232],\n",
      "        [-0.0244],\n",
      "        [-0.0242],\n",
      "        [-0.0239]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0168, 0.0173, 0.0168, 0.0168, 0.0171, 0.0169, 0.0172, 0.0171],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0167,  0.0172,  0.0166,  0.0166,  0.0169, -0.9833,  0.0170,  0.0169],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0175,  0.0612, -0.0262],\n",
      "        [-0.0170,  0.0607, -0.0255],\n",
      "        [-0.0178,  0.0611, -0.0258],\n",
      "        [-0.0164,  0.0607, -0.0255],\n",
      "        [-0.0167,  0.0607, -0.0253],\n",
      "        [-0.0163,  0.0605, -0.0252],\n",
      "        [-0.0169,  0.0609, -0.0259],\n",
      "        [-0.0170,  0.0603, -0.0251]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0175],\n",
      "        [ 0.0607],\n",
      "        [-0.0178],\n",
      "        [-0.0255],\n",
      "        [-0.0167],\n",
      "        [ 0.0605],\n",
      "        [-0.0259],\n",
      "        [-0.0251]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0180, 0.0181, 0.0183, 0.0169, 0.0171, 0.0173, 0.0174, 0.0177],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0178,  0.0179, -0.9818,  0.0167,  0.0170,  0.0171,  0.0172,  0.0176],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0191,  0.0608, -0.0268],\n",
      "        [-0.0191,  0.0606, -0.0266],\n",
      "        [-0.0188,  0.0612, -0.0272],\n",
      "        [-0.0191,  0.0608, -0.0267],\n",
      "        [-0.0194,  0.0612, -0.0270],\n",
      "        [-0.0186,  0.0608, -0.0260],\n",
      "        [-0.0186,  0.0610, -0.0264],\n",
      "        [-0.0193,  0.0611, -0.0269]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0608],\n",
      "        [ 0.0606],\n",
      "        [-0.0188],\n",
      "        [-0.0267],\n",
      "        [-0.0194],\n",
      "        [-0.0260],\n",
      "        [-0.0264],\n",
      "        [-0.0193]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0176, 0.0181, 0.0175, 0.0186, 0.0185, 0.0180, 0.0183, 0.0183],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0174, 1.0179, 0.0173, 0.0185, 0.0183, 0.0178, 0.0181, 0.0181],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0203,  0.0628, -0.0276],\n",
      "        [-0.0205,  0.0624, -0.0272],\n",
      "        [-0.0196,  0.0620, -0.0268],\n",
      "        [-0.0204,  0.0628, -0.0276],\n",
      "        [-0.0200,  0.0614, -0.0272],\n",
      "        [-0.0204,  0.0628, -0.0274],\n",
      "        [-0.0202,  0.0628, -0.0274],\n",
      "        [-0.0197,  0.0623, -0.0269]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0276],\n",
      "        [ 0.0624],\n",
      "        [ 0.0620],\n",
      "        [-0.0276],\n",
      "        [-0.0272],\n",
      "        [-0.0204],\n",
      "        [-0.0274],\n",
      "        [-0.0269]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0185, 0.0188, 0.0185, 0.0185, 0.0183, 0.0190, 0.0189, 0.0185],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0183, -0.9814,  0.0183,  0.0183,  0.0182,  0.0188,  0.0187,  0.0183],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0217,  0.0619, -0.0277],\n",
      "        [-0.0223,  0.0625, -0.0275],\n",
      "        [-0.0218,  0.0618, -0.0273],\n",
      "        [-0.0218,  0.0622, -0.0278],\n",
      "        [-0.0217,  0.0621, -0.0278],\n",
      "        [-0.0222,  0.0624, -0.0278],\n",
      "        [-0.0215,  0.0619, -0.0276],\n",
      "        [-0.0216,  0.0619, -0.0276]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0277],\n",
      "        [-0.0223],\n",
      "        [ 0.0618],\n",
      "        [-0.0278],\n",
      "        [-0.0278],\n",
      "        [-0.0222],\n",
      "        [-0.0276],\n",
      "        [-0.0276]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0190, 0.0197, 0.0198, 0.0194, 0.0190, 0.0195, 0.0196, 0.0190],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9812,  0.0195,  0.0196,  0.0192,  0.0188,  0.0193,  0.0194, -0.9812],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0227,  0.0618, -0.0298],\n",
      "        [-0.0223,  0.0624, -0.0295],\n",
      "        [-0.0224,  0.0620, -0.0295],\n",
      "        [-0.0223,  0.0618, -0.0311],\n",
      "        [-0.0223,  0.0619, -0.0303],\n",
      "        [-0.0230,  0.0620, -0.0303],\n",
      "        [-0.0233,  0.0623, -0.0302],\n",
      "        [-0.0225,  0.0621, -0.0306]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0618],\n",
      "        [-0.0295],\n",
      "        [ 0.0620],\n",
      "        [-0.0311],\n",
      "        [-0.0303],\n",
      "        [-0.0303],\n",
      "        [-0.0233],\n",
      "        [-0.0306]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0199, 0.0203, 0.0195, 0.0197, 0.0192, 0.0199, 0.0201, 0.0194],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9803,  0.0201,  0.0193,  0.0195, -0.9810,  0.0197,  0.0199,  0.0192],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0234,  0.0605, -0.0329],\n",
      "        [-0.0233,  0.0599, -0.0321],\n",
      "        [-0.0223,  0.0610, -0.0320],\n",
      "        [-0.0231,  0.0603, -0.0328],\n",
      "        [-0.0231,  0.0604, -0.0328],\n",
      "        [-0.0233,  0.0605, -0.0328],\n",
      "        [-0.0233,  0.0604, -0.0329],\n",
      "        [-0.0235,  0.0607, -0.0328]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0329],\n",
      "        [-0.0233],\n",
      "        [-0.0320],\n",
      "        [-0.0328],\n",
      "        [-0.0328],\n",
      "        [-0.0328],\n",
      "        [-0.0329],\n",
      "        [-0.0235]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0202, 0.0206, 0.0200, 0.0207, 0.0205, 0.0203, 0.0202, 0.0211],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9800,  0.0204,  0.0198,  0.0204,  0.0203,  0.0200,  0.0200,  0.0209],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0., -1.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0243,  0.0596, -0.0368],\n",
      "        [-0.0239,  0.0590, -0.0366],\n",
      "        [-0.0237,  0.0592, -0.0369],\n",
      "        [-0.0245,  0.0594, -0.0368],\n",
      "        [-0.0239,  0.0588, -0.0361],\n",
      "        [-0.0239,  0.0594, -0.0369],\n",
      "        [-0.0240,  0.0597, -0.0367],\n",
      "        [-0.0243,  0.0596, -0.0368]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0243],\n",
      "        [ 0.0590],\n",
      "        [-0.0369],\n",
      "        [-0.0245],\n",
      "        [-0.0361],\n",
      "        [-0.0369],\n",
      "        [-0.0240],\n",
      "        [-0.0243]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0211, 0.0211, 0.0205, 0.0212, 0.0210, 0.0207, 0.0215, 0.0210],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9791,  0.0209, -0.9797,  0.0210,  0.0208,  0.0205,  0.0213, -0.9792],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0267,  0.0582, -0.0415],\n",
      "        [-0.0262,  0.0584, -0.0413],\n",
      "        [-0.0261,  0.0583, -0.0406],\n",
      "        [-0.0262,  0.0583, -0.0413],\n",
      "        [-0.0264,  0.0581, -0.0408],\n",
      "        [-0.0261,  0.0583, -0.0407],\n",
      "        [-0.0266,  0.0586, -0.0413],\n",
      "        [-0.0267,  0.0586, -0.0413]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0582],\n",
      "        [-0.0413],\n",
      "        [-0.0261],\n",
      "        [-0.0413],\n",
      "        [-0.0408],\n",
      "        [ 0.0583],\n",
      "        [-0.0266],\n",
      "        [-0.0267]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0218, 0.0212, 0.0212, 0.0215, 0.0206, 0.0217, 0.0216, 0.0216],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0216, -0.9790,  1.0210,  0.0213,  0.0204,  0.0215,  0.0213,  0.0214],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0269,  0.0579, -0.0473],\n",
      "        [-0.0268,  0.0573, -0.0463],\n",
      "        [-0.0268,  0.0578, -0.0471],\n",
      "        [-0.0270,  0.0575, -0.0467],\n",
      "        [-0.0268,  0.0574, -0.0460],\n",
      "        [-0.0274,  0.0585, -0.0466],\n",
      "        [-0.0273,  0.0581, -0.0471],\n",
      "        [-0.0273,  0.0583, -0.0471]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0473],\n",
      "        [ 0.0573],\n",
      "        [-0.0471],\n",
      "        [ 0.0575],\n",
      "        [-0.0460],\n",
      "        [-0.0274],\n",
      "        [-0.0273],\n",
      "        [-0.0273]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0218, 0.0220, 0.0216, 0.0220, 0.0220, 0.0221, 0.0221, 0.0223],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0215,  0.0218, -0.9786,  0.0217,  0.0218,  0.0219,  0.0219,  0.0220],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0282,  0.0576, -0.0542],\n",
      "        [-0.0275,  0.0572, -0.0536],\n",
      "        [-0.0278,  0.0579, -0.0539],\n",
      "        [-0.0273,  0.0571, -0.0532],\n",
      "        [-0.0273,  0.0567, -0.0532],\n",
      "        [-0.0275,  0.0572, -0.0537],\n",
      "        [-0.0274,  0.0574, -0.0541],\n",
      "        [-0.0272,  0.0576, -0.0542]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0282],\n",
      "        [ 0.0572],\n",
      "        [-0.0278],\n",
      "        [-0.0273],\n",
      "        [-0.0532],\n",
      "        [ 0.0572],\n",
      "        [-0.0541],\n",
      "        [-0.0542]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0219, 0.0225, 0.0227, 0.0224, 0.0224, 0.0224, 0.0221, 0.0227],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9783,  0.0223,  0.0225,  0.0222,  0.0222,  0.0222,  0.0219,  0.0224],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0295,  0.0571, -0.0598],\n",
      "        [-0.0290,  0.0569, -0.0587],\n",
      "        [-0.0290,  0.0567, -0.0593],\n",
      "        [-0.0290,  0.0566, -0.0592],\n",
      "        [-0.0290,  0.0569, -0.0598],\n",
      "        [-0.0292,  0.0566, -0.0594],\n",
      "        [-0.0287,  0.0571, -0.0576],\n",
      "        [-0.0289,  0.0570, -0.0598]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0295],\n",
      "        [-0.0587],\n",
      "        [ 0.0567],\n",
      "        [ 0.0566],\n",
      "        [-0.0598],\n",
      "        [ 0.0566],\n",
      "        [-0.0576],\n",
      "        [-0.0598]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0231, 0.0225, 0.0230, 0.0230, 0.0226, 0.0224, 0.0233, 0.0226],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0228, 0.0223, 0.0227, 0.0228, 0.0224, 0.0222, 0.0231, 0.0224],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0302,  0.0558, -0.0635],\n",
      "        [-0.0300,  0.0555, -0.0631],\n",
      "        [-0.0304,  0.0561, -0.0635],\n",
      "        [-0.0300,  0.0554, -0.0631],\n",
      "        [-0.0301,  0.0556, -0.0631],\n",
      "        [-0.0299,  0.0558, -0.0637],\n",
      "        [-0.0300,  0.0552, -0.0631],\n",
      "        [-0.0300,  0.0558, -0.0638]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0302],\n",
      "        [ 0.0555],\n",
      "        [-0.0304],\n",
      "        [ 0.0554],\n",
      "        [ 0.0556],\n",
      "        [-0.0637],\n",
      "        [ 0.0552],\n",
      "        [-0.0638]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0230, 0.0234, 0.0235, 0.0234, 0.0232, 0.0230, 0.0233, 0.0232],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9773,  0.0232,  0.0233,  0.0232,  0.0229,  0.0228,  0.0231,  0.0230],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0., -1., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0320,  0.0536, -0.0658],\n",
      "        [-0.0326,  0.0543, -0.0654],\n",
      "        [-0.0328,  0.0544, -0.0654],\n",
      "        [-0.0326,  0.0541, -0.0668],\n",
      "        [-0.0328,  0.0538, -0.0659],\n",
      "        [-0.0329,  0.0545, -0.0661],\n",
      "        [-0.0326,  0.0541, -0.0668],\n",
      "        [-0.0332,  0.0544, -0.0671]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0658],\n",
      "        [ 0.0543],\n",
      "        [ 0.0544],\n",
      "        [-0.0668],\n",
      "        [ 0.0538],\n",
      "        [ 0.0545],\n",
      "        [-0.0668],\n",
      "        [-0.0671]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0235, 0.0234, 0.0236, 0.0235, 0.0238, 0.0238, 0.0235, 0.0236],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0233,  0.0232,  0.0233, -0.9768,  0.0235,  0.0236, -0.9768, -0.9766],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0358,  0.0551, -0.0766],\n",
      "        [-0.0353,  0.0545, -0.0748],\n",
      "        [-0.0353,  0.0541, -0.0752],\n",
      "        [-0.0361,  0.0551, -0.0760],\n",
      "        [-0.0353,  0.0539, -0.0756],\n",
      "        [-0.0356,  0.0546, -0.0764],\n",
      "        [-0.0358,  0.0549, -0.0754],\n",
      "        [-0.0355,  0.0550, -0.0763]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0766],\n",
      "        [-0.0748],\n",
      "        [-0.0752],\n",
      "        [-0.0361],\n",
      "        [ 0.0539],\n",
      "        [-0.0764],\n",
      "        [-0.0358],\n",
      "        [-0.0763]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0240, 0.0241, 0.0240, 0.0244, 0.0233, 0.0235, 0.0238, 0.0239],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0238,  0.0239,  0.0238,  0.0242,  1.0231, -0.9768,  0.0236,  0.0237],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  1.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0375,  0.0579, -0.0868],\n",
      "        [-0.0379,  0.0583, -0.0869],\n",
      "        [-0.0379,  0.0584, -0.0872],\n",
      "        [-0.0383,  0.0587, -0.0878],\n",
      "        [-0.0390,  0.0585, -0.0874],\n",
      "        [-0.0378,  0.0584, -0.0882],\n",
      "        [-0.0377,  0.0579, -0.0875],\n",
      "        [-0.0372,  0.0579, -0.0868]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0868],\n",
      "        [-0.0869],\n",
      "        [ 0.0584],\n",
      "        [-0.0383],\n",
      "        [-0.0874],\n",
      "        [-0.0882],\n",
      "        [-0.0875],\n",
      "        [-0.0868]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0247, 0.0243, 0.0248, 0.0248, 0.0251, 0.0243, 0.0239, 0.0240],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0245,  1.0240,  0.0245,  0.0245, -0.9751,  0.0240,  0.0236,  0.0238],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0397,  0.0608, -0.0959],\n",
      "        [-0.0407,  0.0610, -0.0961],\n",
      "        [-0.0400,  0.0611, -0.0966],\n",
      "        [-0.0389,  0.0603, -0.0960],\n",
      "        [-0.0402,  0.0612, -0.0967],\n",
      "        [-0.0398,  0.0606, -0.0961],\n",
      "        [-0.0398,  0.0609, -0.0971],\n",
      "        [-0.0396,  0.0602, -0.0956]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0608],\n",
      "        [-0.0961],\n",
      "        [-0.0400],\n",
      "        [-0.0960],\n",
      "        [-0.0402],\n",
      "        [ 0.0606],\n",
      "        [-0.0971],\n",
      "        [-0.0956]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0251, 0.0250, 0.0251, 0.0249, 0.0252, 0.0251, 0.0248, 0.0249],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0248, 0.0248, 0.0249, 0.0247, 0.0249, 0.0249, 0.0246, 0.0247],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0410,  0.0619, -0.1015],\n",
      "        [-0.0408,  0.0617, -0.1026],\n",
      "        [-0.0411,  0.0616, -0.1016],\n",
      "        [-0.0408,  0.0621, -0.1018],\n",
      "        [-0.0409,  0.0618, -0.1027],\n",
      "        [-0.0405,  0.0614, -0.1011],\n",
      "        [-0.0406,  0.0617, -0.1018],\n",
      "        [-0.0406,  0.0617, -0.1028]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1015],\n",
      "        [-0.1026],\n",
      "        [ 0.0616],\n",
      "        [ 0.0621],\n",
      "        [-0.1027],\n",
      "        [-0.1011],\n",
      "        [ 0.0617],\n",
      "        [-0.1028]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0252, 0.0252, 0.0256, 0.0255, 0.0253, 0.0253, 0.0252, 0.0250],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0249, -0.9750,  0.0253,  0.0253,  0.0250,  0.0250,  0.0250, -0.9752],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0419,  0.0639, -0.1129],\n",
      "        [-0.0414,  0.0633, -0.1126],\n",
      "        [-0.0414,  0.0632, -0.1125],\n",
      "        [-0.0410,  0.0638, -0.1124],\n",
      "        [-0.0414,  0.0637, -0.1127],\n",
      "        [-0.0415,  0.0636, -0.1140],\n",
      "        [-0.0417,  0.0635, -0.1131],\n",
      "        [-0.0422,  0.0639, -0.1138]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1129],\n",
      "        [-0.1126],\n",
      "        [-0.1125],\n",
      "        [-0.1124],\n",
      "        [-0.1127],\n",
      "        [-0.1140],\n",
      "        [ 0.0635],\n",
      "        [-0.0422]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0257, 0.0258, 0.0250, 0.0255, 0.0251, 0.0260, 0.0259, 0.0260],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0255,  0.0255,  0.0248,  0.0252,  0.0248,  0.0257,  0.0256, -0.9742],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0., -1.,  0., -1.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0450,  0.0646, -0.1190],\n",
      "        [-0.0454,  0.0652, -0.1207],\n",
      "        [-0.0452,  0.0650, -0.1204],\n",
      "        [-0.0458,  0.0652, -0.1202],\n",
      "        [-0.0454,  0.0650, -0.1205],\n",
      "        [-0.0453,  0.0650, -0.1205],\n",
      "        [-0.0453,  0.0650, -0.1207],\n",
      "        [-0.0453,  0.0651, -0.1209]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0646],\n",
      "        [-0.1207],\n",
      "        [-0.1204],\n",
      "        [-0.0458],\n",
      "        [-0.1205],\n",
      "        [-0.1205],\n",
      "        [-0.1207],\n",
      "        [-0.1209]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0261, 0.0261, 0.0258, 0.0264, 0.0261, 0.0259, 0.0260, 0.0259],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0258,  0.0258,  0.0256, -0.9739,  0.0258, -0.9744,  0.0257, -0.9744],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0514,  0.0711, -0.1332],\n",
      "        [-0.0518,  0.0708, -0.1335],\n",
      "        [-0.0515,  0.0710, -0.1334],\n",
      "        [-0.0519,  0.0707, -0.1321],\n",
      "        [-0.0510,  0.0708, -0.1325],\n",
      "        [-0.0513,  0.0709, -0.1321],\n",
      "        [-0.0515,  0.0710, -0.1335],\n",
      "        [-0.0518,  0.0712, -0.1329]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1332],\n",
      "        [-0.1335],\n",
      "        [-0.1334],\n",
      "        [ 0.0707],\n",
      "        [-0.1325],\n",
      "        [-0.1321],\n",
      "        [-0.1335],\n",
      "        [-0.0518]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0265, 0.0261, 0.0263, 0.0268, 0.0257, 0.0261, 0.0265, 0.0267],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0262, 0.0259, 0.0261, 1.0265, 0.0254, 0.0259, 0.0262, 0.0264],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  1.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0578,  0.0787, -0.1398],\n",
      "        [-0.0573,  0.0783, -0.1394],\n",
      "        [-0.0579,  0.0787, -0.1399],\n",
      "        [-0.0574,  0.0785, -0.1403],\n",
      "        [-0.0574,  0.0782, -0.1391],\n",
      "        [-0.0579,  0.0783, -0.1387],\n",
      "        [-0.0571,  0.0783, -0.1388],\n",
      "        [-0.0574,  0.0785, -0.1394]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0578],\n",
      "        [-0.1394],\n",
      "        [-0.0579],\n",
      "        [-0.1403],\n",
      "        [ 0.0782],\n",
      "        [-0.0579],\n",
      "        [-0.1388],\n",
      "        [ 0.0785]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0272, 0.0262, 0.0273, 0.0268, 0.0272, 0.0267, 0.0270, 0.0271],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0270,  0.0259, -0.9730,  0.0265,  0.0270,  1.0264,  1.0267,  0.0268],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0622,  0.0827, -0.1384],\n",
      "        [-0.0619,  0.0823, -0.1367],\n",
      "        [-0.0615,  0.0816, -0.1371],\n",
      "        [-0.0626,  0.0824, -0.1374],\n",
      "        [-0.0611,  0.0819, -0.1369],\n",
      "        [-0.0620,  0.0825, -0.1381],\n",
      "        [-0.0615,  0.0821, -0.1375],\n",
      "        [-0.0628,  0.0826, -0.1374]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1384],\n",
      "        [-0.1367],\n",
      "        [-0.1371],\n",
      "        [-0.0626],\n",
      "        [ 0.0819],\n",
      "        [-0.1381],\n",
      "        [-0.0615],\n",
      "        [-0.0628]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0274, 0.0272, 0.0271, 0.0281, 0.0274, 0.0272, 0.0269, 0.0278],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0271, 0.0269, 0.0268, 0.0278, 0.0271, 0.0270, 1.0266, 0.0275],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0623,  0.0838, -0.1324],\n",
      "        [-0.0618,  0.0834, -0.1302],\n",
      "        [-0.0617,  0.0837, -0.1310],\n",
      "        [-0.0616,  0.0839, -0.1325],\n",
      "        [-0.0620,  0.0838, -0.1300],\n",
      "        [-0.0617,  0.0839, -0.1324],\n",
      "        [-0.0618,  0.0839, -0.1324],\n",
      "        [-0.0619,  0.0840, -0.1324]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1324],\n",
      "        [ 0.0834],\n",
      "        [-0.1310],\n",
      "        [-0.1325],\n",
      "        [-0.1300],\n",
      "        [-0.1324],\n",
      "        [-0.1324],\n",
      "        [-0.1324]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0275, 0.0271, 0.0275, 0.0276, 0.0282, 0.0277, 0.0277, 0.0277],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0272,  0.0268,  0.0272, -0.9727,  0.0279,  0.0274,  0.0274,  0.0274],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0617,  0.0850, -0.1266],\n",
      "        [-0.0622,  0.0852, -0.1256],\n",
      "        [-0.0623,  0.0849, -0.1268],\n",
      "        [-0.0618,  0.0848, -0.1265],\n",
      "        [-0.0615,  0.0850, -0.1264],\n",
      "        [-0.0610,  0.0848, -0.1255],\n",
      "        [-0.0614,  0.0846, -0.1253],\n",
      "        [-0.0611,  0.0843, -0.1247]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1266],\n",
      "        [-0.0622],\n",
      "        [-0.1268],\n",
      "        [-0.1265],\n",
      "        [-0.1264],\n",
      "        [-0.1255],\n",
      "        [-0.1253],\n",
      "        [-0.1247]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0282, 0.0291, 0.0281, 0.0281, 0.0284, 0.0288, 0.0279, 0.0277],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0280,  0.0288, -0.9722, -0.9721,  0.0281,  0.0285,  0.0277,  0.0274],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0604,  0.0863, -0.1263],\n",
      "        [-0.0612,  0.0868, -0.1270],\n",
      "        [-0.0613,  0.0864, -0.1255],\n",
      "        [-0.0612,  0.0868, -0.1269],\n",
      "        [-0.0613,  0.0869, -0.1270],\n",
      "        [-0.0612,  0.0864, -0.1267],\n",
      "        [-0.0602,  0.0863, -0.1256],\n",
      "        [-0.0613,  0.0866, -0.1259]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0604],\n",
      "        [-0.1270],\n",
      "        [-0.0613],\n",
      "        [-0.1269],\n",
      "        [-0.1270],\n",
      "        [ 0.0864],\n",
      "        [ 0.0863],\n",
      "        [ 0.0866]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0282, 0.0285, 0.0283, 0.0285, 0.0285, 0.0288, 0.0286, 0.0289],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0279, 0.0282, 0.0280, 0.0282, 0.0282, 0.0286, 0.0283, 1.0286],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0583,  0.0902, -0.1259],\n",
      "        [-0.0584,  0.0899, -0.1254],\n",
      "        [-0.0583,  0.0902, -0.1259],\n",
      "        [-0.0581,  0.0901, -0.1259],\n",
      "        [-0.0583,  0.0901, -0.1259],\n",
      "        [-0.0572,  0.0892, -0.1244],\n",
      "        [-0.0582,  0.0898, -0.1250],\n",
      "        [-0.0574,  0.0895, -0.1245]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1259],\n",
      "        [-0.1254],\n",
      "        [-0.1259],\n",
      "        [-0.1259],\n",
      "        [-0.1259],\n",
      "        [-0.0572],\n",
      "        [-0.1250],\n",
      "        [-0.0574]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0289, 0.0289, 0.0290, 0.0290, 0.0290, 0.0289, 0.0290, 0.0290],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0286, 0.0286, 0.0287, 0.0287, 0.0287, 0.0286, 0.0287, 1.0287],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0532,  0.0915, -0.1194],\n",
      "        [-0.0531,  0.0916, -0.1192],\n",
      "        [-0.0529,  0.0909, -0.1188],\n",
      "        [-0.0528,  0.0909, -0.1192],\n",
      "        [-0.0532,  0.0908, -0.1192],\n",
      "        [-0.0528,  0.0909, -0.1191],\n",
      "        [-0.0530,  0.0910, -0.1193],\n",
      "        [-0.0531,  0.0913, -0.1206]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0915],\n",
      "        [ 0.0916],\n",
      "        [-0.1188],\n",
      "        [-0.1192],\n",
      "        [ 0.0908],\n",
      "        [-0.1191],\n",
      "        [ 0.0910],\n",
      "        [-0.1206]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0298, 0.0298, 0.0295, 0.0291, 0.0289, 0.0292, 0.0297, 0.0295],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0295,  0.0295,  0.0292,  0.0288,  0.0286,  0.0289,  0.0294, -0.9708],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  1., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0486,  0.0919, -0.1172],\n",
      "        [-0.0491,  0.0921, -0.1169],\n",
      "        [-0.0486,  0.0916, -0.1156],\n",
      "        [-0.0481,  0.0916, -0.1160],\n",
      "        [-0.0484,  0.0918, -0.1174],\n",
      "        [-0.0490,  0.0917, -0.1158],\n",
      "        [-0.0488,  0.0921, -0.1165],\n",
      "        [-0.0486,  0.0919, -0.1172]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1172],\n",
      "        [-0.0491],\n",
      "        [-0.1156],\n",
      "        [-0.1160],\n",
      "        [-0.1174],\n",
      "        [ 0.0917],\n",
      "        [-0.1165],\n",
      "        [-0.1172]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0298, 0.0302, 0.0300, 0.0293, 0.0297, 0.0302, 0.0301, 0.0298],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0295,  0.0299,  0.0297,  1.0290, -0.9706,  0.0299,  0.0298,  0.0295],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0449,  0.0913, -0.1115],\n",
      "        [-0.0451,  0.0910, -0.1101],\n",
      "        [-0.0443,  0.0908, -0.1100],\n",
      "        [-0.0449,  0.0910, -0.1096],\n",
      "        [-0.0448,  0.0912, -0.1102],\n",
      "        [-0.0443,  0.0907, -0.1100],\n",
      "        [-0.0450,  0.0909, -0.1103],\n",
      "        [-0.0449,  0.0908, -0.1097]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1115],\n",
      "        [ 0.0910],\n",
      "        [ 0.0908],\n",
      "        [-0.1096],\n",
      "        [-0.1102],\n",
      "        [ 0.0907],\n",
      "        [ 0.0909],\n",
      "        [-0.1097]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0303, 0.0305, 0.0298, 0.0306, 0.0306, 0.0296, 0.0305, 0.0303],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0300, 0.0302, 0.0295, 0.0303, 0.0302, 0.0293, 0.0302, 0.0300],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0415,  0.0880, -0.1003],\n",
      "        [-0.0424,  0.0883, -0.1002],\n",
      "        [-0.0422,  0.0876, -0.0993],\n",
      "        [-0.0423,  0.0879, -0.1001],\n",
      "        [-0.0422,  0.0874, -0.0999],\n",
      "        [-0.0421,  0.0883, -0.1005],\n",
      "        [-0.0429,  0.0879, -0.1007],\n",
      "        [-0.0416,  0.0877, -0.1007]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.1003],\n",
      "        [-0.0424],\n",
      "        [-0.0993],\n",
      "        [ 0.0879],\n",
      "        [ 0.0874],\n",
      "        [-0.0421],\n",
      "        [-0.0429],\n",
      "        [-0.1007]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0306, 0.0314, 0.0309, 0.0308, 0.0307, 0.0315, 0.0309, 0.0310],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0302, 0.0311, 0.0306, 0.0305, 0.0304, 0.0312, 0.0306, 0.0307],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1., -1., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0395,  0.0840, -0.0898],\n",
      "        [-0.0396,  0.0843, -0.0906],\n",
      "        [-0.0394,  0.0840, -0.0908],\n",
      "        [-0.0395,  0.0841, -0.0905],\n",
      "        [-0.0394,  0.0841, -0.0908],\n",
      "        [-0.0393,  0.0841, -0.0913],\n",
      "        [-0.0395,  0.0841, -0.0907],\n",
      "        [-0.0395,  0.0839, -0.0897]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0840],\n",
      "        [-0.0396],\n",
      "        [-0.0908],\n",
      "        [-0.0905],\n",
      "        [-0.0908],\n",
      "        [-0.0913],\n",
      "        [-0.0907],\n",
      "        [ 0.0839]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0313, 0.0314, 0.0310, 0.0311, 0.0310, 0.0312, 0.0312, 0.0314],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0309,  0.0311, -0.9693, -0.9692, -0.9693,  0.0309,  0.0309,  0.0311],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0369,  0.0831, -0.0891],\n",
      "        [-0.0362,  0.0825, -0.0879],\n",
      "        [-0.0367,  0.0829, -0.0891],\n",
      "        [-0.0372,  0.0834, -0.0885],\n",
      "        [-0.0369,  0.0828, -0.0890],\n",
      "        [-0.0368,  0.0829, -0.0890],\n",
      "        [-0.0370,  0.0828, -0.0881],\n",
      "        [-0.0367,  0.0824, -0.0880]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0891],\n",
      "        [ 0.0825],\n",
      "        [-0.0891],\n",
      "        [-0.0372],\n",
      "        [-0.0890],\n",
      "        [-0.0890],\n",
      "        [ 0.0828],\n",
      "        [-0.0880]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0318, 0.0308, 0.0317, 0.0322, 0.0316, 0.0316, 0.0318, 0.0310],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0315, 0.0305, 0.0314, 0.0319, 0.0313, 0.0312, 0.0315, 0.0307],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0345,  0.0808, -0.0863],\n",
      "        [-0.0348,  0.0805, -0.0844],\n",
      "        [-0.0345,  0.0807, -0.0845],\n",
      "        [-0.0345,  0.0810, -0.0860],\n",
      "        [-0.0346,  0.0811, -0.0860],\n",
      "        [-0.0348,  0.0810, -0.0857],\n",
      "        [-0.0346,  0.0810, -0.0859],\n",
      "        [-0.0346,  0.0809, -0.0859]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0863],\n",
      "        [ 0.0805],\n",
      "        [-0.0845],\n",
      "        [-0.0860],\n",
      "        [-0.0860],\n",
      "        [-0.0857],\n",
      "        [-0.0859],\n",
      "        [-0.0859]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0321, 0.0322, 0.0317, 0.0319, 0.0321, 0.0322, 0.0321, 0.0320],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9682,  0.0319,  1.0314,  0.0316,  0.0317,  0.0319,  0.0318,  0.0317],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0326,  0.0783, -0.0812],\n",
      "        [-0.0328,  0.0781, -0.0800],\n",
      "        [-0.0330,  0.0787, -0.0815],\n",
      "        [-0.0332,  0.0785, -0.0816],\n",
      "        [-0.0332,  0.0783, -0.0815],\n",
      "        [-0.0328,  0.0785, -0.0815],\n",
      "        [-0.0327,  0.0788, -0.0807],\n",
      "        [-0.0328,  0.0785, -0.0814]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0812],\n",
      "        [-0.0800],\n",
      "        [-0.0815],\n",
      "        [-0.0816],\n",
      "        [-0.0815],\n",
      "        [-0.0815],\n",
      "        [-0.0807],\n",
      "        [-0.0814]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0327, 0.0325, 0.0328, 0.0326, 0.0322, 0.0324, 0.0325, 0.0322],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0324,  0.0322,  0.0324, -0.9678,  0.0319,  0.0321,  0.0321,  0.0318],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0314,  0.0764, -0.0779],\n",
      "        [-0.0313,  0.0764, -0.0779],\n",
      "        [-0.0313,  0.0764, -0.0779],\n",
      "        [-0.0313,  0.0764, -0.0779],\n",
      "        [-0.0314,  0.0765, -0.0781],\n",
      "        [-0.0318,  0.0766, -0.0776],\n",
      "        [-0.0314,  0.0761, -0.0766],\n",
      "        [-0.0321,  0.0766, -0.0769]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0779],\n",
      "        [-0.0779],\n",
      "        [-0.0779],\n",
      "        [-0.0779],\n",
      "        [-0.0781],\n",
      "        [-0.0318],\n",
      "        [-0.0766],\n",
      "        [ 0.0766]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0328, 0.0329, 0.0328, 0.0328, 0.0329, 0.0332, 0.0324, 0.0335],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0325, 0.0325, 0.0325, 0.0325, 1.0325, 0.0328, 0.0321, 0.0332],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0303,  0.0733, -0.0702],\n",
      "        [-0.0303,  0.0733, -0.0715],\n",
      "        [-0.0311,  0.0738, -0.0720],\n",
      "        [-0.0304,  0.0726, -0.0703],\n",
      "        [-0.0302,  0.0733, -0.0716],\n",
      "        [-0.0301,  0.0731, -0.0701],\n",
      "        [-0.0301,  0.0737, -0.0708],\n",
      "        [-0.0303,  0.0732, -0.0716]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0733],\n",
      "        [-0.0715],\n",
      "        [-0.0311],\n",
      "        [ 0.0726],\n",
      "        [-0.0716],\n",
      "        [-0.0301],\n",
      "        [-0.0708],\n",
      "        [-0.0716]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0339, 0.0333, 0.0340, 0.0328, 0.0332, 0.0326, 0.0339, 0.0332],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0335,  0.0329,  0.0336,  0.0324,  0.0329,  0.0323,  0.0336, -0.9671],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0296,  0.0710, -0.0671],\n",
      "        [-0.0296,  0.0709, -0.0670],\n",
      "        [-0.0291,  0.0708, -0.0672],\n",
      "        [-0.0293,  0.0709, -0.0671],\n",
      "        [-0.0293,  0.0708, -0.0668],\n",
      "        [-0.0296,  0.0710, -0.0671],\n",
      "        [-0.0292,  0.0712, -0.0671],\n",
      "        [-0.0286,  0.0705, -0.0660]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0296],\n",
      "        [-0.0296],\n",
      "        [-0.0672],\n",
      "        [-0.0671],\n",
      "        [ 0.0708],\n",
      "        [-0.0296],\n",
      "        [-0.0671],\n",
      "        [-0.0660]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0340, 0.0339, 0.0336, 0.0336, 0.0338, 0.0340, 0.0338, 0.0333],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0336,  0.0335,  0.0333, -0.9667,  0.0334,  0.0336,  0.0335,  0.0329],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0279,  0.0689, -0.0648],\n",
      "        [-0.0279,  0.0688, -0.0650],\n",
      "        [-0.0282,  0.0687, -0.0641],\n",
      "        [-0.0280,  0.0689, -0.0647],\n",
      "        [-0.0282,  0.0689, -0.0649],\n",
      "        [-0.0281,  0.0687, -0.0638],\n",
      "        [-0.0280,  0.0686, -0.0634],\n",
      "        [-0.0284,  0.0691, -0.0645]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0648],\n",
      "        [-0.0650],\n",
      "        [ 0.0687],\n",
      "        [-0.0647],\n",
      "        [-0.0649],\n",
      "        [ 0.0687],\n",
      "        [-0.0634],\n",
      "        [-0.0284]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0340, 0.0341, 0.0341, 0.0340, 0.0339, 0.0333, 0.0336, 0.0344],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0337, -0.9663,  0.0337,  0.0336, -0.9664,  0.0330,  0.0333,  0.0341],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0267,  0.0675, -0.0636],\n",
      "        [-0.0268,  0.0680, -0.0646],\n",
      "        [-0.0267,  0.0672, -0.0636],\n",
      "        [-0.0266,  0.0677, -0.0652],\n",
      "        [-0.0270,  0.0679, -0.0639],\n",
      "        [-0.0269,  0.0677, -0.0636],\n",
      "        [-0.0267,  0.0674, -0.0636],\n",
      "        [-0.0267,  0.0678, -0.0650]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0636],\n",
      "        [-0.0646],\n",
      "        [-0.0636],\n",
      "        [-0.0652],\n",
      "        [-0.0639],\n",
      "        [ 0.0677],\n",
      "        [ 0.0674],\n",
      "        [-0.0650]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0344, 0.0345, 0.0341, 0.0341, 0.0344, 0.0340, 0.0344, 0.0344],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0341,  0.0342,  0.0338, -0.9662,  0.0340,  0.0337,  1.0341,  0.0340],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0257,  0.0689, -0.0658],\n",
      "        [-0.0255,  0.0691, -0.0658],\n",
      "        [-0.0266,  0.0694, -0.0658],\n",
      "        [-0.0260,  0.0691, -0.0659],\n",
      "        [-0.0255,  0.0690, -0.0664],\n",
      "        [-0.0255,  0.0690, -0.0663],\n",
      "        [-0.0257,  0.0690, -0.0657],\n",
      "        [-0.0253,  0.0688, -0.0651]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0689],\n",
      "        [-0.0658],\n",
      "        [-0.0658],\n",
      "        [-0.0260],\n",
      "        [-0.0664],\n",
      "        [-0.0663],\n",
      "        [ 0.0690],\n",
      "        [-0.0651]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0348, 0.0348, 0.0353, 0.0352, 0.0348, 0.0347, 0.0351, 0.0343],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0344,  0.0345, -0.9650,  0.0348,  0.0345,  0.0344,  0.0348,  0.0339],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0245,  0.0703, -0.0676],\n",
      "        [-0.0247,  0.0703, -0.0674],\n",
      "        [-0.0246,  0.0704, -0.0659],\n",
      "        [-0.0245,  0.0703, -0.0682],\n",
      "        [-0.0241,  0.0706, -0.0674],\n",
      "        [-0.0246,  0.0703, -0.0679],\n",
      "        [-0.0244,  0.0702, -0.0687],\n",
      "        [-0.0241,  0.0700, -0.0674]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0703],\n",
      "        [ 0.0703],\n",
      "        [-0.0659],\n",
      "        [-0.0682],\n",
      "        [-0.0674],\n",
      "        [-0.0246],\n",
      "        [-0.0687],\n",
      "        [ 0.0700]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0354, 0.0357, 0.0350, 0.0353, 0.0355, 0.0354, 0.0350, 0.0351],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0350,  0.0353,  0.0347,  0.0350,  0.0352,  0.0351, -0.9654,  0.0348],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  1.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0230,  0.0719, -0.0709],\n",
      "        [-0.0233,  0.0717, -0.0707],\n",
      "        [-0.0232,  0.0720, -0.0710],\n",
      "        [-0.0228,  0.0718, -0.0718],\n",
      "        [-0.0230,  0.0715, -0.0701],\n",
      "        [-0.0230,  0.0719, -0.0714],\n",
      "        [-0.0234,  0.0719, -0.0710],\n",
      "        [-0.0237,  0.0720, -0.0708]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0230],\n",
      "        [ 0.0717],\n",
      "        [ 0.0720],\n",
      "        [-0.0718],\n",
      "        [-0.0701],\n",
      "        [-0.0714],\n",
      "        [-0.0234],\n",
      "        [-0.0237]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0363, 0.0356, 0.0355, 0.0355, 0.0353, 0.0358, 0.0358, 0.0358],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0359,  1.0352,  0.0352, -0.9648,  0.0349,  0.0355,  0.0355,  0.0355],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 1., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0214,  0.0754, -0.0760],\n",
      "        [-0.0218,  0.0753, -0.0757],\n",
      "        [-0.0211,  0.0745, -0.0755],\n",
      "        [-0.0218,  0.0757, -0.0744],\n",
      "        [-0.0214,  0.0749, -0.0747],\n",
      "        [-0.0216,  0.0750, -0.0746],\n",
      "        [-0.0219,  0.0754, -0.0755],\n",
      "        [-0.0211,  0.0748, -0.0749]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0760],\n",
      "        [-0.0218],\n",
      "        [-0.0755],\n",
      "        [ 0.0757],\n",
      "        [-0.0747],\n",
      "        [-0.0746],\n",
      "        [-0.0219],\n",
      "        [-0.0211]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0362, 0.0362, 0.0353, 0.0364, 0.0361, 0.0356, 0.0363, 0.0358],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0358, 0.0358, 1.0350, 0.0361, 0.0358, 0.0353, 0.0359, 1.0354],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0188,  0.0768, -0.0766],\n",
      "        [-0.0188,  0.0771, -0.0766],\n",
      "        [-0.0196,  0.0777, -0.0774],\n",
      "        [-0.0193,  0.0771, -0.0763],\n",
      "        [-0.0192,  0.0774, -0.0762],\n",
      "        [-0.0191,  0.0776, -0.0777],\n",
      "        [-0.0193,  0.0769, -0.0766],\n",
      "        [-0.0192,  0.0776, -0.0778]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0768],\n",
      "        [ 0.0771],\n",
      "        [-0.0196],\n",
      "        [-0.0763],\n",
      "        [-0.0762],\n",
      "        [-0.0777],\n",
      "        [ 0.0769],\n",
      "        [-0.0778]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0359, 0.0363, 0.0368, 0.0360, 0.0362, 0.0363, 0.0359, 0.0363],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0355, 0.0360, 0.0365, 0.0357, 0.0358, 0.0360, 1.0356, 0.0359],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0170,  0.0809, -0.0787],\n",
      "        [-0.0169,  0.0810, -0.0790],\n",
      "        [-0.0169,  0.0810, -0.0791],\n",
      "        [-0.0169,  0.0811, -0.0791],\n",
      "        [-0.0174,  0.0810, -0.0778],\n",
      "        [-0.0167,  0.0808, -0.0776],\n",
      "        [-0.0168,  0.0810, -0.0790],\n",
      "        [-0.0172,  0.0811, -0.0787]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0809],\n",
      "        [-0.0790],\n",
      "        [-0.0791],\n",
      "        [-0.0791],\n",
      "        [-0.0778],\n",
      "        [ 0.0808],\n",
      "        [-0.0790],\n",
      "        [-0.0172]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0369, 0.0367, 0.0367, 0.0366, 0.0370, 0.0363, 0.0366, 0.0371],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0365, 0.0363, 0.0364, 0.0362, 0.0366, 0.0359, 0.0363, 0.0367],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0146,  0.0857, -0.0798],\n",
      "        [-0.0147,  0.0858, -0.0791],\n",
      "        [-0.0150,  0.0857, -0.0787],\n",
      "        [-0.0147,  0.0859, -0.0797],\n",
      "        [-0.0146,  0.0857, -0.0797],\n",
      "        [-0.0150,  0.0854, -0.0798],\n",
      "        [-0.0149,  0.0857, -0.0787],\n",
      "        [-0.0149,  0.0855, -0.0789]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0798],\n",
      "        [-0.0791],\n",
      "        [ 0.0857],\n",
      "        [-0.0797],\n",
      "        [-0.0797],\n",
      "        [-0.0798],\n",
      "        [ 0.0857],\n",
      "        [ 0.0855]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0371, 0.0373, 0.0374, 0.0374, 0.0371, 0.0370, 0.0374, 0.0369],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0368, 0.0369, 0.0371, 0.0370, 0.0368, 0.0366, 0.0371, 0.0365],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0125,  0.0889, -0.0787],\n",
      "        [-0.0128,  0.0893, -0.0791],\n",
      "        [-0.0132,  0.0893, -0.0776],\n",
      "        [-0.0132,  0.0892, -0.0788],\n",
      "        [-0.0128,  0.0892, -0.0789],\n",
      "        [-0.0125,  0.0892, -0.0787],\n",
      "        [-0.0125,  0.0885, -0.0777],\n",
      "        [-0.0128,  0.0889, -0.0774]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0787],\n",
      "        [-0.0791],\n",
      "        [ 0.0893],\n",
      "        [-0.0132],\n",
      "        [-0.0789],\n",
      "        [-0.0787],\n",
      "        [-0.0777],\n",
      "        [-0.0774]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0379, 0.0377, 0.0379, 0.0380, 0.0376, 0.0378, 0.0376, 0.0373],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0375, 0.0374, 0.0376, 1.0376, 0.0372, 0.0374, 0.0372, 0.0369],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0102,  0.0920, -0.0766],\n",
      "        [-0.0101,  0.0916, -0.0754],\n",
      "        [-0.0105,  0.0917, -0.0767],\n",
      "        [-0.0099,  0.0914, -0.0755],\n",
      "        [-0.0111,  0.0917, -0.0762],\n",
      "        [-0.0100,  0.0912, -0.0752],\n",
      "        [-0.0105,  0.0920, -0.0761],\n",
      "        [-0.0101,  0.0920, -0.0767]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0766],\n",
      "        [-0.0754],\n",
      "        [-0.0767],\n",
      "        [-0.0755],\n",
      "        [-0.0111],\n",
      "        [-0.0752],\n",
      "        [ 0.0920],\n",
      "        [-0.0767]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0381, 0.0381, 0.0378, 0.0380, 0.0384, 0.0383, 0.0382, 0.0380],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0377,  0.0377,  0.0374,  0.0376, -0.9620,  0.0380,  0.0378,  0.0376],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0097,  0.0932, -0.0723],\n",
      "        [-0.0095,  0.0937, -0.0729],\n",
      "        [-0.0092,  0.0936, -0.0730],\n",
      "        [-0.0090,  0.0937, -0.0734],\n",
      "        [-0.0090,  0.0936, -0.0729],\n",
      "        [-0.0093,  0.0935, -0.0731],\n",
      "        [-0.0091,  0.0932, -0.0718],\n",
      "        [-0.0092,  0.0936, -0.0732]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0932],\n",
      "        [-0.0095],\n",
      "        [-0.0730],\n",
      "        [-0.0734],\n",
      "        [-0.0729],\n",
      "        [-0.0093],\n",
      "        [-0.0718],\n",
      "        [-0.0732]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0380, 0.0388, 0.0386, 0.0385, 0.0387, 0.0386, 0.0382, 0.0384],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0376,  0.0385,  0.0382,  0.0382,  0.0383, -0.9618,  0.0378,  0.0381],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0091,  0.0942, -0.0687],\n",
      "        [-0.0096,  0.0941, -0.0683],\n",
      "        [-0.0100,  0.0938, -0.0682],\n",
      "        [-0.0097,  0.0942, -0.0683],\n",
      "        [-0.0092,  0.0939, -0.0673],\n",
      "        [-0.0093,  0.0940, -0.0689],\n",
      "        [-0.0095,  0.0942, -0.0686],\n",
      "        [-0.0091,  0.0935, -0.0676]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0687],\n",
      "        [ 0.0941],\n",
      "        [ 0.0938],\n",
      "        [-0.0097],\n",
      "        [ 0.0939],\n",
      "        [-0.0689],\n",
      "        [-0.0686],\n",
      "        [-0.0676]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0393, 0.0388, 0.0390, 0.0393, 0.0384, 0.0388, 0.0392, 0.0388],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0389,  0.0384,  1.0386, -0.9611,  0.0380,  0.0384,  0.0388,  0.0384],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0104,  0.0960, -0.0643],\n",
      "        [-0.0107,  0.0959, -0.0632],\n",
      "        [-0.0108,  0.0959, -0.0634],\n",
      "        [-0.0104,  0.0953, -0.0631],\n",
      "        [-0.0110,  0.0959, -0.0632],\n",
      "        [-0.0102,  0.0954, -0.0632],\n",
      "        [-0.0109,  0.0959, -0.0634],\n",
      "        [-0.0116,  0.0957, -0.0638]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0643],\n",
      "        [ 0.0959],\n",
      "        [ 0.0959],\n",
      "        [-0.0631],\n",
      "        [ 0.0959],\n",
      "        [-0.0632],\n",
      "        [ 0.0959],\n",
      "        [-0.0116]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0394, 0.0399, 0.0397, 0.0391, 0.0391, 0.0393, 0.0398, 0.0397],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0390,  0.0395,  0.0393,  0.0387,  0.0387,  0.0389,  0.0394, -0.9607],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0130,  0.0962, -0.0586],\n",
      "        [-0.0128,  0.0961, -0.0582],\n",
      "        [-0.0127,  0.0961, -0.0584],\n",
      "        [-0.0128,  0.0965, -0.0595],\n",
      "        [-0.0128,  0.0965, -0.0595],\n",
      "        [-0.0133,  0.0964, -0.0591],\n",
      "        [-0.0128,  0.0961, -0.0582],\n",
      "        [-0.0128,  0.0966, -0.0596]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0962],\n",
      "        [ 0.0961],\n",
      "        [ 0.0961],\n",
      "        [-0.0595],\n",
      "        [-0.0595],\n",
      "        [-0.0133],\n",
      "        [-0.0582],\n",
      "        [-0.0596]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0400, 0.0399, 0.0397, 0.0398, 0.0399, 0.0401, 0.0398, 0.0399],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0396, 0.0395, 0.0393, 0.0394, 0.0395, 0.0397, 0.0394, 0.0395],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0149,  0.0960, -0.0533],\n",
      "        [-0.0154,  0.0966, -0.0541],\n",
      "        [-0.0150,  0.0965, -0.0541],\n",
      "        [-0.0151,  0.0965, -0.0545],\n",
      "        [-0.0148,  0.0964, -0.0551],\n",
      "        [-0.0149,  0.0963, -0.0545],\n",
      "        [-0.0145,  0.0958, -0.0549],\n",
      "        [-0.0153,  0.0964, -0.0552]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0533],\n",
      "        [-0.0154],\n",
      "        [-0.0150],\n",
      "        [-0.0545],\n",
      "        [-0.0551],\n",
      "        [-0.0545],\n",
      "        [-0.0549],\n",
      "        [-0.0552]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0404, 0.0408, 0.0406, 0.0404, 0.0404, 0.0402, 0.0402, 0.0406],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0400,  0.0404,  0.0402,  0.0400, -0.9600,  0.0398,  0.0398, -0.9598],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0168,  0.0969, -0.0517],\n",
      "        [-0.0169,  0.0968, -0.0517],\n",
      "        [-0.0168,  0.0967, -0.0524],\n",
      "        [-0.0165,  0.0963, -0.0508],\n",
      "        [-0.0168,  0.0971, -0.0522],\n",
      "        [-0.0164,  0.0965, -0.0525],\n",
      "        [-0.0164,  0.0967, -0.0527],\n",
      "        [-0.0164,  0.0965, -0.0514]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0969],\n",
      "        [ 0.0968],\n",
      "        [-0.0524],\n",
      "        [-0.0165],\n",
      "        [-0.0168],\n",
      "        [-0.0525],\n",
      "        [-0.0527],\n",
      "        [-0.0514]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0410, 0.0411, 0.0405, 0.0404, 0.0411, 0.0410, 0.0405, 0.0408],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0406, 1.0407, 0.0401, 0.0400, 0.0407, 0.0406, 0.0400, 0.0404],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0178,  0.0993, -0.0507],\n",
      "        [-0.0176,  0.0989, -0.0494],\n",
      "        [-0.0178,  0.0991, -0.0505],\n",
      "        [-0.0173,  0.0986, -0.0499],\n",
      "        [-0.0179,  0.0993, -0.0509],\n",
      "        [-0.0179,  0.0993, -0.0506],\n",
      "        [-0.0174,  0.0986, -0.0498],\n",
      "        [-0.0179,  0.0990, -0.0500]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0178],\n",
      "        [-0.0494],\n",
      "        [ 0.0991],\n",
      "        [ 0.0986],\n",
      "        [-0.0179],\n",
      "        [-0.0179],\n",
      "        [-0.0498],\n",
      "        [ 0.0990]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0412, 0.0413, 0.0415, 0.0411, 0.0415, 0.0414, 0.0411, 0.0411],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0408,  0.0409,  0.0410,  0.0407, -0.9590, -0.9590,  0.0406,  0.0407],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0203,  0.1005, -0.0487],\n",
      "        [-0.0203,  0.1000, -0.0475],\n",
      "        [-0.0201,  0.1009, -0.0483],\n",
      "        [-0.0200,  0.0998, -0.0473],\n",
      "        [-0.0215,  0.1003, -0.0479],\n",
      "        [-0.0209,  0.1005, -0.0482],\n",
      "        [-0.0204,  0.1005, -0.0486],\n",
      "        [-0.0207,  0.1003, -0.0477]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0487],\n",
      "        [-0.0475],\n",
      "        [-0.0201],\n",
      "        [-0.0473],\n",
      "        [-0.0215],\n",
      "        [ 0.1005],\n",
      "        [-0.0486],\n",
      "        [ 0.1003]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0418, 0.0418, 0.0423, 0.0414, 0.0421, 0.0418, 0.0416, 0.0413],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0414,  0.0413,  0.0419,  0.0410,  0.0417,  0.0414, -0.9588,  0.0409],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0228,  0.1017, -0.0473],\n",
      "        [-0.0229,  0.1017, -0.0473],\n",
      "        [-0.0223,  0.1020, -0.0469],\n",
      "        [-0.0223,  0.1013, -0.0465],\n",
      "        [-0.0225,  0.1017, -0.0478],\n",
      "        [-0.0226,  0.1016, -0.0477],\n",
      "        [-0.0226,  0.1011, -0.0463],\n",
      "        [-0.0224,  0.1015, -0.0476]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0228],\n",
      "        [-0.0229],\n",
      "        [-0.0469],\n",
      "        [ 0.1013],\n",
      "        [-0.0478],\n",
      "        [-0.0477],\n",
      "        [-0.0463],\n",
      "        [-0.0476]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0426, 0.0425, 0.0427, 0.0417, 0.0423, 0.0422, 0.0419, 0.0423],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0422, 0.0421, 0.0423, 0.0413, 0.0418, 0.0418, 0.0415, 0.0419],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0244,  0.1023, -0.0462],\n",
      "        [-0.0246,  0.1023, -0.0461],\n",
      "        [-0.0249,  0.1025, -0.0455],\n",
      "        [-0.0249,  0.1017, -0.0444],\n",
      "        [-0.0253,  0.1022, -0.0455],\n",
      "        [-0.0250,  0.1023, -0.0457],\n",
      "        [-0.0245,  0.1024, -0.0462],\n",
      "        [-0.0249,  0.1023, -0.0456]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0462],\n",
      "        [-0.0461],\n",
      "        [-0.0249],\n",
      "        [-0.0444],\n",
      "        [-0.0253],\n",
      "        [-0.0250],\n",
      "        [-0.0462],\n",
      "        [-0.0249]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0428, 0.0427, 0.0434, 0.0425, 0.0431, 0.0430, 0.0427, 0.0430],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0423, -0.9577,  0.0429,  0.0421,  0.0427,  0.0426,  0.0423, -0.9574],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 1., 0., 1., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0266,  0.1029, -0.0445],\n",
      "        [-0.0270,  0.1029, -0.0444],\n",
      "        [-0.0275,  0.1032, -0.0451],\n",
      "        [-0.0266,  0.1026, -0.0446],\n",
      "        [-0.0266,  0.1029, -0.0445],\n",
      "        [-0.0273,  0.1031, -0.0450],\n",
      "        [-0.0272,  0.1027, -0.0435],\n",
      "        [-0.0273,  0.1033, -0.0451]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1029],\n",
      "        [-0.0444],\n",
      "        [ 0.1032],\n",
      "        [-0.0266],\n",
      "        [-0.0445],\n",
      "        [ 0.1031],\n",
      "        [-0.0435],\n",
      "        [-0.0273]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0427, 0.0428, 0.0433, 0.0428, 0.0426, 0.0433, 0.0431, 0.0434],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0422, 1.0423, 0.0429, 1.0423, 0.0422, 1.0429, 0.0426, 0.0430],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0286,  0.1055, -0.0432],\n",
      "        [-0.0281,  0.1049, -0.0428],\n",
      "        [-0.0285,  0.1056, -0.0434],\n",
      "        [-0.0283,  0.1054, -0.0439],\n",
      "        [-0.0279,  0.1049, -0.0427],\n",
      "        [-0.0281,  0.1047, -0.0427],\n",
      "        [-0.0281,  0.1053, -0.0439],\n",
      "        [-0.0279,  0.1050, -0.0425]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0286],\n",
      "        [-0.0281],\n",
      "        [-0.0285],\n",
      "        [-0.0439],\n",
      "        [-0.0279],\n",
      "        [-0.0427],\n",
      "        [-0.0439],\n",
      "        [-0.0425]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0440, 0.0431, 0.0443, 0.0437, 0.0435, 0.0431, 0.0438, 0.0436],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0436, 0.0426, 0.0438, 0.0433, 0.0431, 0.0426, 0.0434, 0.0432],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0293,  0.1070, -0.0416],\n",
      "        [-0.0288,  0.1062, -0.0409],\n",
      "        [-0.0287,  0.1068, -0.0406],\n",
      "        [-0.0291,  0.1067, -0.0409],\n",
      "        [-0.0290,  0.1072, -0.0420],\n",
      "        [-0.0287,  0.1069, -0.0410],\n",
      "        [-0.0290,  0.1065, -0.0407],\n",
      "        [-0.0289,  0.1069, -0.0406]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1070],\n",
      "        [-0.0409],\n",
      "        [-0.0287],\n",
      "        [-0.0409],\n",
      "        [-0.0420],\n",
      "        [-0.0287],\n",
      "        [-0.0290],\n",
      "        [-0.0406]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0442, 0.0436, 0.0436, 0.0443, 0.0442, 0.0438, 0.0436, 0.0440],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0437, 0.0432, 0.0431, 1.0438, 0.0437, 0.0434, 0.0432, 0.0436],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0298,  0.1080, -0.0385],\n",
      "        [-0.0298,  0.1079, -0.0383],\n",
      "        [-0.0302,  0.1080, -0.0388],\n",
      "        [-0.0301,  0.1077, -0.0376],\n",
      "        [-0.0299,  0.1079, -0.0381],\n",
      "        [-0.0297,  0.1079, -0.0385],\n",
      "        [-0.0297,  0.1073, -0.0367],\n",
      "        [-0.0303,  0.1077, -0.0383]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0385],\n",
      "        [-0.0383],\n",
      "        [-0.0388],\n",
      "        [ 0.1077],\n",
      "        [-0.0381],\n",
      "        [-0.0385],\n",
      "        [-0.0367],\n",
      "        [ 0.1077]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0446, 0.0446, 0.0449, 0.0451, 0.0446, 0.0446, 0.0443, 0.0444],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0442,  0.0441, -0.9555,  0.0447,  0.0441,  0.0441,  0.0439, -0.9560],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0305,  0.1059, -0.0347],\n",
      "        [-0.0302,  0.1053, -0.0335],\n",
      "        [-0.0303,  0.1060, -0.0354],\n",
      "        [-0.0303,  0.1059, -0.0354],\n",
      "        [-0.0302,  0.1056, -0.0341],\n",
      "        [-0.0301,  0.1056, -0.0341],\n",
      "        [-0.0307,  0.1059, -0.0350],\n",
      "        [-0.0299,  0.1057, -0.0342]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1059],\n",
      "        [-0.0335],\n",
      "        [-0.0354],\n",
      "        [-0.0354],\n",
      "        [ 0.1056],\n",
      "        [-0.0301],\n",
      "        [-0.0350],\n",
      "        [-0.0342]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0454, 0.0452, 0.0450, 0.0447, 0.0446, 0.0449, 0.0452, 0.0446],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0449, 0.0447, 0.0446, 0.0443, 0.0442, 0.0444, 0.0448, 0.0441],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0310,  0.1036, -0.0314],\n",
      "        [-0.0308,  0.1031, -0.0311],\n",
      "        [-0.0307,  0.1038, -0.0321],\n",
      "        [-0.0304,  0.1033, -0.0308],\n",
      "        [-0.0306,  0.1036, -0.0318],\n",
      "        [-0.0306,  0.1037, -0.0328],\n",
      "        [-0.0307,  0.1035, -0.0322],\n",
      "        [-0.0307,  0.1037, -0.0316]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1036],\n",
      "        [-0.0311],\n",
      "        [-0.0321],\n",
      "        [-0.0308],\n",
      "        [-0.0318],\n",
      "        [-0.0328],\n",
      "        [-0.0322],\n",
      "        [-0.0316]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0460, 0.0452, 0.0456, 0.0453, 0.0459, 0.0453, 0.0455, 0.0455],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0456,  0.0447,  0.0451,  0.0449,  0.0455, -0.9552, -0.9550,  0.0451],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0309,  0.1020, -0.0300],\n",
      "        [-0.0308,  0.1020, -0.0313],\n",
      "        [-0.0304,  0.1022, -0.0307],\n",
      "        [-0.0305,  0.1019, -0.0309],\n",
      "        [-0.0311,  0.1021, -0.0305],\n",
      "        [-0.0313,  0.1024, -0.0309],\n",
      "        [-0.0310,  0.1015, -0.0295],\n",
      "        [-0.0309,  0.1023, -0.0311]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0300],\n",
      "        [-0.0313],\n",
      "        [-0.0307],\n",
      "        [ 0.1019],\n",
      "        [-0.0311],\n",
      "        [-0.0313],\n",
      "        [-0.0295],\n",
      "        [-0.0311]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0463, 0.0461, 0.0454, 0.0453, 0.0465, 0.0463, 0.0455, 0.0462],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0458, 0.0457, 0.0449, 0.0448, 0.0460, 0.0459, 0.0450, 0.0458],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  1.,  0.,  0.,  1.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0305,  0.0998, -0.0292],\n",
      "        [-0.0309,  0.1007, -0.0303],\n",
      "        [-0.0310,  0.1005, -0.0298],\n",
      "        [-0.0309,  0.1006, -0.0300],\n",
      "        [-0.0307,  0.1001, -0.0288],\n",
      "        [-0.0309,  0.1007, -0.0300],\n",
      "        [-0.0312,  0.1004, -0.0294],\n",
      "        [-0.0310,  0.1006, -0.0301]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0998],\n",
      "        [-0.0303],\n",
      "        [-0.0298],\n",
      "        [-0.0300],\n",
      "        [-0.0288],\n",
      "        [-0.0300],\n",
      "        [ 0.1004],\n",
      "        [-0.0301]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0460, 0.0467, 0.0466, 0.0464, 0.0463, 0.0467, 0.0467, 0.0464],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0455,  1.0463,  0.0461,  0.0459,  1.0458,  0.0462, -0.9538,  0.0460],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0311,  0.0956, -0.0256],\n",
      "        [-0.0317,  0.0960, -0.0254],\n",
      "        [-0.0316,  0.0960, -0.0251],\n",
      "        [-0.0311,  0.0960, -0.0257],\n",
      "        [-0.0317,  0.0957, -0.0248],\n",
      "        [-0.0312,  0.0959, -0.0257],\n",
      "        [-0.0309,  0.0954, -0.0248],\n",
      "        [-0.0310,  0.0954, -0.0243]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0256],\n",
      "        [-0.0317],\n",
      "        [-0.0316],\n",
      "        [-0.0257],\n",
      "        [-0.0317],\n",
      "        [-0.0257],\n",
      "        [ 0.0954],\n",
      "        [ 0.0954]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0470, 0.0471, 0.0473, 0.0470, 0.0466, 0.0469, 0.0464, 0.0468],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0466, 0.0467, 0.0468, 0.0465, 0.0461, 0.0464, 0.0459, 0.0463],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0315,  0.0908, -0.0197],\n",
      "        [-0.0320,  0.0915, -0.0220],\n",
      "        [-0.0313,  0.0915, -0.0217],\n",
      "        [-0.0315,  0.0917, -0.0217],\n",
      "        [-0.0313,  0.0916, -0.0217],\n",
      "        [-0.0318,  0.0916, -0.0213],\n",
      "        [-0.0313,  0.0915, -0.0217],\n",
      "        [-0.0319,  0.0915, -0.0212]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0197],\n",
      "        [-0.0220],\n",
      "        [-0.0217],\n",
      "        [-0.0315],\n",
      "        [-0.0217],\n",
      "        [-0.0318],\n",
      "        [-0.0217],\n",
      "        [ 0.0915]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0475, 0.0476, 0.0473, 0.0474, 0.0473, 0.0475, 0.0472, 0.0474],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0470, -0.9529,  0.0469,  1.0469,  0.0469,  0.0470,  0.0468,  0.0470],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  1.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0306,  0.0873, -0.0177],\n",
      "        [-0.0304,  0.0876, -0.0193],\n",
      "        [-0.0303,  0.0871, -0.0179],\n",
      "        [-0.0305,  0.0873, -0.0182],\n",
      "        [-0.0306,  0.0874, -0.0180],\n",
      "        [-0.0303,  0.0876, -0.0193],\n",
      "        [-0.0305,  0.0877, -0.0196],\n",
      "        [-0.0301,  0.0873, -0.0183]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0177],\n",
      "        [-0.0193],\n",
      "        [-0.0303],\n",
      "        [-0.0182],\n",
      "        [-0.0306],\n",
      "        [-0.0193],\n",
      "        [-0.0196],\n",
      "        [ 0.0873]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0479, 0.0476, 0.0467, 0.0477, 0.0475, 0.0476, 0.0478, 0.0477],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0474,  0.0472,  0.0462,  0.0472,  1.0470,  0.0471, -0.9527,  0.0472],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0.,  1.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0286,  0.0839, -0.0168],\n",
      "        [-0.0285,  0.0838, -0.0168],\n",
      "        [-0.0293,  0.0841, -0.0175],\n",
      "        [-0.0289,  0.0840, -0.0168],\n",
      "        [-0.0293,  0.0842, -0.0175],\n",
      "        [-0.0290,  0.0842, -0.0175],\n",
      "        [-0.0291,  0.0842, -0.0174],\n",
      "        [-0.0294,  0.0841, -0.0180]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0168],\n",
      "        [ 0.0838],\n",
      "        [ 0.0841],\n",
      "        [ 0.0840],\n",
      "        [ 0.0842],\n",
      "        [ 0.0842],\n",
      "        [ 0.0842],\n",
      "        [ 0.0841]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0478, 0.0476, 0.0481, 0.0480, 0.0480, 0.0480, 0.0481, 0.0478],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0474,  0.0471,  0.0476,  0.0475,  1.0475,  0.0475,  0.0477, -0.9527],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0277,  0.0803, -0.0148],\n",
      "        [-0.0279,  0.0809, -0.0154],\n",
      "        [-0.0280,  0.0806, -0.0155],\n",
      "        [-0.0275,  0.0807, -0.0159],\n",
      "        [-0.0275,  0.0800, -0.0141],\n",
      "        [-0.0275,  0.0801, -0.0155],\n",
      "        [-0.0274,  0.0808, -0.0160],\n",
      "        [-0.0269,  0.0795, -0.0145]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0148],\n",
      "        [-0.0279],\n",
      "        [ 0.0806],\n",
      "        [-0.0159],\n",
      "        [-0.0141],\n",
      "        [-0.0155],\n",
      "        [-0.0160],\n",
      "        [-0.0269]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0484, 0.0486, 0.0484, 0.0485, 0.0485, 0.0481, 0.0483, 0.0480],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0479,  0.0481,  0.0479, -0.9520,  0.0480,  0.0476,  0.0479,  0.0475],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0264,  0.0776, -0.0153],\n",
      "        [-0.0263,  0.0776, -0.0152],\n",
      "        [-0.0272,  0.0776, -0.0138],\n",
      "        [-0.0267,  0.0773, -0.0146],\n",
      "        [-0.0263,  0.0770, -0.0139],\n",
      "        [-0.0267,  0.0777, -0.0145],\n",
      "        [-0.0267,  0.0772, -0.0151],\n",
      "        [-0.0263,  0.0775, -0.0152]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0153],\n",
      "        [-0.0152],\n",
      "        [-0.0272],\n",
      "        [ 0.0773],\n",
      "        [ 0.0770],\n",
      "        [-0.0267],\n",
      "        [-0.0151],\n",
      "        [-0.0152]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0486, 0.0488, 0.0495, 0.0491, 0.0483, 0.0490, 0.0484, 0.0487],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9519,  0.0483,  0.0490,  0.0486,  1.0478,  0.0485,  0.0479,  0.0483],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  1.,  0.,  0.,  0., -1.,  0.,  1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0252,  0.0764, -0.0157],\n",
      "        [-0.0249,  0.0758, -0.0146],\n",
      "        [-0.0253,  0.0764, -0.0155],\n",
      "        [-0.0256,  0.0767, -0.0151],\n",
      "        [-0.0252,  0.0762, -0.0148],\n",
      "        [-0.0256,  0.0766, -0.0162],\n",
      "        [-0.0253,  0.0763, -0.0156],\n",
      "        [-0.0256,  0.0761, -0.0150]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0157],\n",
      "        [-0.0146],\n",
      "        [-0.0155],\n",
      "        [-0.0256],\n",
      "        [-0.0148],\n",
      "        [-0.0162],\n",
      "        [-0.0156],\n",
      "        [-0.0150]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0489, 0.0487, 0.0492, 0.0493, 0.0491, 0.0492, 0.0491, 0.0489],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9516,  1.0482,  0.0487,  0.0488,  0.0486, -0.9513,  0.0486,  1.0484],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0245,  0.0751, -0.0144],\n",
      "        [-0.0242,  0.0750, -0.0143],\n",
      "        [-0.0242,  0.0750, -0.0142],\n",
      "        [-0.0247,  0.0753, -0.0149],\n",
      "        [-0.0248,  0.0752, -0.0155],\n",
      "        [-0.0242,  0.0756, -0.0158],\n",
      "        [-0.0243,  0.0752, -0.0153],\n",
      "        [-0.0240,  0.0749, -0.0143]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0144],\n",
      "        [ 0.0750],\n",
      "        [-0.0142],\n",
      "        [ 0.0753],\n",
      "        [ 0.0752],\n",
      "        [-0.0158],\n",
      "        [-0.0153],\n",
      "        [ 0.0749]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0493, 0.0492, 0.0491, 0.0496, 0.0491, 0.0492, 0.0494, 0.0489],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0488,  0.0487,  0.0486,  0.0491, -0.9514,  1.0487,  0.0489,  0.0484],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0231,  0.0717, -0.0127],\n",
      "        [-0.0237,  0.0722, -0.0128],\n",
      "        [-0.0229,  0.0726, -0.0143],\n",
      "        [-0.0237,  0.0728, -0.0129],\n",
      "        [-0.0237,  0.0723, -0.0132],\n",
      "        [-0.0233,  0.0725, -0.0139],\n",
      "        [-0.0234,  0.0725, -0.0136],\n",
      "        [-0.0236,  0.0728, -0.0133]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0717],\n",
      "        [ 0.0722],\n",
      "        [-0.0143],\n",
      "        [-0.0237],\n",
      "        [ 0.0723],\n",
      "        [-0.0139],\n",
      "        [-0.0136],\n",
      "        [-0.0236]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0493, 0.0491, 0.0494, 0.0500, 0.0499, 0.0496, 0.0499, 0.0499],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0488,  0.0487, -0.9511,  0.0495,  0.0494,  0.0491,  0.0494,  0.0494],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0226,  0.0699, -0.0125],\n",
      "        [-0.0224,  0.0700, -0.0134],\n",
      "        [-0.0229,  0.0694, -0.0118],\n",
      "        [-0.0229,  0.0697, -0.0125],\n",
      "        [-0.0222,  0.0704, -0.0126],\n",
      "        [-0.0224,  0.0699, -0.0133],\n",
      "        [-0.0228,  0.0696, -0.0119],\n",
      "        [-0.0232,  0.0701, -0.0122]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0125],\n",
      "        [-0.0134],\n",
      "        [ 0.0694],\n",
      "        [ 0.0697],\n",
      "        [-0.0222],\n",
      "        [-0.0133],\n",
      "        [ 0.0696],\n",
      "        [-0.0232]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0499, 0.0499, 0.0501, 0.0501, 0.0506, 0.0499, 0.0498, 0.0503],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0494, 0.0494, 0.0496, 0.0496, 0.0501, 0.0494, 1.0493, 0.0498],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0216,  0.0690, -0.0127],\n",
      "        [-0.0217,  0.0693, -0.0120],\n",
      "        [-0.0222,  0.0690, -0.0117],\n",
      "        [-0.0212,  0.0685, -0.0114],\n",
      "        [-0.0214,  0.0686, -0.0114],\n",
      "        [-0.0215,  0.0693, -0.0130],\n",
      "        [-0.0212,  0.0693, -0.0131],\n",
      "        [-0.0213,  0.0684, -0.0111]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0127],\n",
      "        [-0.0120],\n",
      "        [-0.0117],\n",
      "        [ 0.0685],\n",
      "        [-0.0114],\n",
      "        [-0.0130],\n",
      "        [-0.0131],\n",
      "        [-0.0111]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0502, 0.0502, 0.0506, 0.0498, 0.0498, 0.0503, 0.0500, 0.0499],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0497,  0.0497,  0.0500,  0.0493,  0.0493, -0.9502, -0.9505,  0.0494],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0213,  0.0682, -0.0116],\n",
      "        [-0.0213,  0.0690, -0.0129],\n",
      "        [-0.0211,  0.0686, -0.0136],\n",
      "        [-0.0211,  0.0683, -0.0122],\n",
      "        [-0.0211,  0.0683, -0.0123],\n",
      "        [-0.0211,  0.0687, -0.0135],\n",
      "        [-0.0213,  0.0689, -0.0130],\n",
      "        [-0.0214,  0.0683, -0.0121]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0682],\n",
      "        [-0.0213],\n",
      "        [-0.0136],\n",
      "        [-0.0122],\n",
      "        [ 0.0683],\n",
      "        [-0.0135],\n",
      "        [-0.0213],\n",
      "        [ 0.0683]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0504, 0.0508, 0.0501, 0.0502, 0.0499, 0.0505, 0.0506, 0.0505],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0499, 0.0503, 0.0496, 1.0497, 1.0494, 0.0500, 0.0501, 0.0500],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0205,  0.0695, -0.0131],\n",
      "        [-0.0204,  0.0695, -0.0133],\n",
      "        [-0.0204,  0.0695, -0.0133],\n",
      "        [-0.0200,  0.0694, -0.0138],\n",
      "        [-0.0207,  0.0691, -0.0119],\n",
      "        [-0.0209,  0.0693, -0.0128],\n",
      "        [-0.0203,  0.0695, -0.0133],\n",
      "        [-0.0207,  0.0695, -0.0127]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0131],\n",
      "        [-0.0133],\n",
      "        [-0.0133],\n",
      "        [-0.0138],\n",
      "        [-0.0119],\n",
      "        [ 0.0693],\n",
      "        [-0.0133],\n",
      "        [ 0.0695]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0508, 0.0507, 0.0508, 0.0504, 0.0508, 0.0506, 0.0507, 0.0510],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0503,  0.0502,  0.0503, -0.9501,  0.0503,  0.0501,  0.0502,  0.0505],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0204,  0.0701, -0.0129],\n",
      "        [-0.0201,  0.0703, -0.0132],\n",
      "        [-0.0198,  0.0703, -0.0138],\n",
      "        [-0.0198,  0.0698, -0.0123],\n",
      "        [-0.0198,  0.0703, -0.0137],\n",
      "        [-0.0197,  0.0697, -0.0125],\n",
      "        [-0.0201,  0.0698, -0.0123],\n",
      "        [-0.0201,  0.0698, -0.0123]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0129],\n",
      "        [-0.0132],\n",
      "        [-0.0138],\n",
      "        [-0.0198],\n",
      "        [-0.0137],\n",
      "        [ 0.0697],\n",
      "        [ 0.0698],\n",
      "        [-0.0123]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0513, 0.0509, 0.0511, 0.0508, 0.0510, 0.0505, 0.0510, 0.0510],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0508, 0.0504, 0.0506, 0.0503, 0.0505, 0.0500, 0.0505, 0.0505],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0193,  0.0708, -0.0138],\n",
      "        [-0.0192,  0.0710, -0.0139],\n",
      "        [-0.0194,  0.0711, -0.0132],\n",
      "        [-0.0194,  0.0701, -0.0119],\n",
      "        [-0.0192,  0.0709, -0.0138],\n",
      "        [-0.0191,  0.0709, -0.0139],\n",
      "        [-0.0190,  0.0702, -0.0125],\n",
      "        [-0.0192,  0.0708, -0.0138]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0138],\n",
      "        [-0.0139],\n",
      "        [-0.0194],\n",
      "        [ 0.0701],\n",
      "        [-0.0138],\n",
      "        [-0.0191],\n",
      "        [ 0.0702],\n",
      "        [-0.0138]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0513, 0.0513, 0.0516, 0.0509, 0.0514, 0.0512, 0.0507, 0.0513],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0508, 0.0508, 0.0511, 0.0504, 0.0508, 0.0506, 0.0502, 0.0508],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0188,  0.0712, -0.0136],\n",
      "        [-0.0186,  0.0712, -0.0136],\n",
      "        [-0.0190,  0.0710, -0.0122],\n",
      "        [-0.0185,  0.0710, -0.0125],\n",
      "        [-0.0187,  0.0713, -0.0135],\n",
      "        [-0.0189,  0.0711, -0.0134],\n",
      "        [-0.0187,  0.0712, -0.0143],\n",
      "        [-0.0192,  0.0718, -0.0125]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0136],\n",
      "        [-0.0136],\n",
      "        [ 0.0710],\n",
      "        [-0.0185],\n",
      "        [-0.0135],\n",
      "        [ 0.0711],\n",
      "        [-0.0143],\n",
      "        [-0.0192]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0515, 0.0515, 0.0519, 0.0513, 0.0516, 0.0515, 0.0516, 0.0521],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9490,  0.0510,  0.0514,  0.0508,  0.0511,  0.0510,  0.0511,  0.0516],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0180,  0.0715, -0.0142],\n",
      "        [-0.0181,  0.0717, -0.0139],\n",
      "        [-0.0180,  0.0714, -0.0128],\n",
      "        [-0.0184,  0.0722, -0.0135],\n",
      "        [-0.0183,  0.0719, -0.0135],\n",
      "        [-0.0180,  0.0717, -0.0143],\n",
      "        [-0.0178,  0.0715, -0.0146],\n",
      "        [-0.0184,  0.0713, -0.0127]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0142],\n",
      "        [-0.0139],\n",
      "        [ 0.0714],\n",
      "        [-0.0184],\n",
      "        [-0.0183],\n",
      "        [-0.0143],\n",
      "        [-0.0146],\n",
      "        [-0.0127]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0522, 0.0519, 0.0515, 0.0523, 0.0520, 0.0518, 0.0518, 0.0518],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0517,  0.0514,  0.0510,  0.0518,  0.0515,  0.0513, -0.9488,  0.0512],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0175,  0.0716, -0.0143],\n",
      "        [-0.0175,  0.0723, -0.0155],\n",
      "        [-0.0176,  0.0726, -0.0149],\n",
      "        [-0.0176,  0.0720, -0.0146],\n",
      "        [-0.0177,  0.0724, -0.0149],\n",
      "        [-0.0174,  0.0722, -0.0155],\n",
      "        [-0.0177,  0.0718, -0.0140],\n",
      "        [-0.0177,  0.0720, -0.0146]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0143],\n",
      "        [-0.0155],\n",
      "        [-0.0176],\n",
      "        [ 0.0720],\n",
      "        [-0.0177],\n",
      "        [-0.0155],\n",
      "        [-0.0177],\n",
      "        [ 0.0720]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0513, 0.0521, 0.0527, 0.0521, 0.0525, 0.0522, 0.0521, 0.0521],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0508, 0.0516, 0.0522, 1.0515, 0.0520, 0.0517, 0.0516, 0.0516],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 1., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0165,  0.0737, -0.0151],\n",
      "        [-0.0166,  0.0735, -0.0154],\n",
      "        [-0.0166,  0.0743, -0.0164],\n",
      "        [-0.0166,  0.0737, -0.0166],\n",
      "        [-0.0168,  0.0739, -0.0158],\n",
      "        [-0.0166,  0.0740, -0.0166],\n",
      "        [-0.0166,  0.0740, -0.0167],\n",
      "        [-0.0166,  0.0741, -0.0167]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0737],\n",
      "        [-0.0154],\n",
      "        [-0.0166],\n",
      "        [-0.0166],\n",
      "        [ 0.0739],\n",
      "        [-0.0166],\n",
      "        [-0.0167],\n",
      "        [-0.0167]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0520, 0.0519, 0.0526, 0.0525, 0.0524, 0.0523, 0.0523, 0.0524],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0515, 1.0514, 1.0520, 0.0520, 0.0518, 0.0518, 0.0518, 0.0518],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0157,  0.0753, -0.0155],\n",
      "        [-0.0160,  0.0749, -0.0145],\n",
      "        [-0.0154,  0.0755, -0.0164],\n",
      "        [-0.0153,  0.0752, -0.0149],\n",
      "        [-0.0156,  0.0753, -0.0159],\n",
      "        [-0.0153,  0.0755, -0.0164],\n",
      "        [-0.0157,  0.0754, -0.0157],\n",
      "        [-0.0156,  0.0756, -0.0159]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0753],\n",
      "        [-0.0160],\n",
      "        [-0.0164],\n",
      "        [ 0.0752],\n",
      "        [ 0.0753],\n",
      "        [-0.0164],\n",
      "        [ 0.0754],\n",
      "        [-0.0159]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0526, 0.0526, 0.0524, 0.0522, 0.0527, 0.0526, 0.0526, 0.0529],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0521,  0.0521, -0.9481,  0.0517,  0.0521,  0.0521,  0.0520,  0.0523],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0141,  0.0765, -0.0168],\n",
      "        [-0.0145,  0.0768, -0.0163],\n",
      "        [-0.0140,  0.0767, -0.0172],\n",
      "        [-0.0141,  0.0769, -0.0171],\n",
      "        [-0.0147,  0.0765, -0.0160],\n",
      "        [-0.0144,  0.0765, -0.0156],\n",
      "        [-0.0144,  0.0767, -0.0162],\n",
      "        [-0.0144,  0.0760, -0.0150]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0168],\n",
      "        [ 0.0768],\n",
      "        [-0.0172],\n",
      "        [-0.0171],\n",
      "        [ 0.0765],\n",
      "        [ 0.0765],\n",
      "        [-0.0144],\n",
      "        [-0.0150]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0529, 0.0528, 0.0528, 0.0528, 0.0520, 0.0528, 0.0527, 0.0523],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0524, -0.9477, -0.9477,  0.0523,  0.0515,  0.0522,  0.0521,  0.0518],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0132,  0.0767, -0.0177],\n",
      "        [-0.0133,  0.0768, -0.0177],\n",
      "        [-0.0136,  0.0758, -0.0156],\n",
      "        [-0.0130,  0.0760, -0.0165],\n",
      "        [-0.0128,  0.0765, -0.0184],\n",
      "        [-0.0132,  0.0758, -0.0170],\n",
      "        [-0.0135,  0.0766, -0.0187],\n",
      "        [-0.0131,  0.0761, -0.0166]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0132],\n",
      "        [-0.0133],\n",
      "        [-0.0156],\n",
      "        [-0.0165],\n",
      "        [-0.0184],\n",
      "        [ 0.0758],\n",
      "        [-0.0187],\n",
      "        [-0.0166]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0533, 0.0534, 0.0532, 0.0529, 0.0531, 0.0529, 0.0533, 0.0528],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0528,  0.0528,  0.0527,  0.0523,  0.0525,  0.0524, -0.9472,  0.0523],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0119,  0.0757, -0.0184],\n",
      "        [-0.0123,  0.0764, -0.0190],\n",
      "        [-0.0121,  0.0765, -0.0192],\n",
      "        [-0.0120,  0.0762, -0.0195],\n",
      "        [-0.0124,  0.0757, -0.0186],\n",
      "        [-0.0118,  0.0758, -0.0183],\n",
      "        [-0.0116,  0.0756, -0.0184],\n",
      "        [-0.0122,  0.0761, -0.0190]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0184],\n",
      "        [-0.0123],\n",
      "        [-0.0121],\n",
      "        [-0.0195],\n",
      "        [-0.0186],\n",
      "        [ 0.0758],\n",
      "        [ 0.0756],\n",
      "        [-0.0190]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0530, 0.0535, 0.0536, 0.0532, 0.0527, 0.0529, 0.0528, 0.0531],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0524,  0.0530, -0.9469,  0.0527,  0.0522,  0.0524,  0.0523,  0.0526],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0118,  0.0758, -0.0193],\n",
      "        [-0.0120,  0.0754, -0.0189],\n",
      "        [-0.0116,  0.0750, -0.0185],\n",
      "        [-0.0119,  0.0754, -0.0186],\n",
      "        [-0.0116,  0.0757, -0.0202],\n",
      "        [-0.0120,  0.0752, -0.0180],\n",
      "        [-0.0119,  0.0750, -0.0174],\n",
      "        [-0.0116,  0.0755, -0.0200]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0118],\n",
      "        [ 0.0754],\n",
      "        [-0.0185],\n",
      "        [ 0.0754],\n",
      "        [-0.0202],\n",
      "        [ 0.0752],\n",
      "        [ 0.0750],\n",
      "        [-0.0200]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0542, 0.0538, 0.0528, 0.0537, 0.0536, 0.0532, 0.0535, 0.0537],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0536,  0.0532,  0.0523,  0.0531, -0.9469,  0.0527,  0.0529,  0.0531],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0113,  0.0745, -0.0192],\n",
      "        [-0.0109,  0.0746, -0.0187],\n",
      "        [-0.0113,  0.0744, -0.0189],\n",
      "        [-0.0114,  0.0745, -0.0191],\n",
      "        [-0.0113,  0.0745, -0.0189],\n",
      "        [-0.0115,  0.0746, -0.0198],\n",
      "        [-0.0116,  0.0750, -0.0203],\n",
      "        [-0.0113,  0.0752, -0.0207]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0745],\n",
      "        [-0.0187],\n",
      "        [ 0.0744],\n",
      "        [ 0.0745],\n",
      "        [ 0.0745],\n",
      "        [ 0.0746],\n",
      "        [-0.0203],\n",
      "        [-0.0207]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0534, 0.0534, 0.0536, 0.0537, 0.0537, 0.0539, 0.0539, 0.0540],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0529, 0.0529, 0.0531, 1.0532, 0.0532, 0.0534, 0.0534, 0.0535],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0108,  0.0759, -0.0216],\n",
      "        [-0.0108,  0.0749, -0.0196],\n",
      "        [-0.0109,  0.0752, -0.0195],\n",
      "        [-0.0109,  0.0752, -0.0194],\n",
      "        [-0.0108,  0.0758, -0.0216],\n",
      "        [-0.0112,  0.0754, -0.0195],\n",
      "        [-0.0110,  0.0760, -0.0209],\n",
      "        [-0.0108,  0.0758, -0.0216]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0216],\n",
      "        [ 0.0749],\n",
      "        [-0.0195],\n",
      "        [-0.0194],\n",
      "        [-0.0216],\n",
      "        [ 0.0754],\n",
      "        [-0.0110],\n",
      "        [-0.0216]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0540, 0.0537, 0.0538, 0.0538, 0.0540, 0.0539, 0.0544, 0.0540],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0535, 0.0532, 0.0533, 1.0533, 0.0535, 0.0534, 0.0538, 0.0535],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0105,  0.0761, -0.0210],\n",
      "        [-0.0105,  0.0758, -0.0207],\n",
      "        [-0.0109,  0.0758, -0.0197],\n",
      "        [-0.0108,  0.0755, -0.0189],\n",
      "        [-0.0109,  0.0755, -0.0193],\n",
      "        [-0.0115,  0.0760, -0.0192],\n",
      "        [-0.0109,  0.0755, -0.0186],\n",
      "        [-0.0108,  0.0763, -0.0203]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0210],\n",
      "        [-0.0207],\n",
      "        [ 0.0758],\n",
      "        [-0.0108],\n",
      "        [-0.0193],\n",
      "        [ 0.0760],\n",
      "        [ 0.0755],\n",
      "        [-0.0108]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0543, 0.0545, 0.0543, 0.0542, 0.0541, 0.0545, 0.0540, 0.0546],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0537, 0.0539, 0.0538, 0.0536, 1.0536, 0.0540, 0.0535, 0.0541],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0107,  0.0749, -0.0175],\n",
      "        [-0.0106,  0.0760, -0.0185],\n",
      "        [-0.0103,  0.0758, -0.0192],\n",
      "        [-0.0110,  0.0752, -0.0176],\n",
      "        [-0.0106,  0.0760, -0.0185],\n",
      "        [-0.0107,  0.0755, -0.0180],\n",
      "        [-0.0106,  0.0760, -0.0186],\n",
      "        [-0.0108,  0.0752, -0.0182]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0749],\n",
      "        [-0.0106],\n",
      "        [-0.0192],\n",
      "        [-0.0176],\n",
      "        [-0.0106],\n",
      "        [ 0.0755],\n",
      "        [-0.0106],\n",
      "        [-0.0182]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0538, 0.0548, 0.0545, 0.0540, 0.0548, 0.0547, 0.0549, 0.0545],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0533,  0.0543,  0.0540,  0.0534, -0.9457,  0.0541,  0.0543,  0.0540],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0108,  0.0750, -0.0174],\n",
      "        [-0.0110,  0.0751, -0.0163],\n",
      "        [-0.0108,  0.0745, -0.0151],\n",
      "        [-0.0108,  0.0745, -0.0150],\n",
      "        [-0.0109,  0.0751, -0.0176],\n",
      "        [-0.0108,  0.0753, -0.0172],\n",
      "        [-0.0108,  0.0743, -0.0149],\n",
      "        [-0.0107,  0.0745, -0.0151]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0174],\n",
      "        [ 0.0751],\n",
      "        [-0.0108],\n",
      "        [-0.0150],\n",
      "        [-0.0176],\n",
      "        [-0.0172],\n",
      "        [-0.0149],\n",
      "        [-0.0151]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0550, 0.0549, 0.0546, 0.0545, 0.0545, 0.0548, 0.0546, 0.0545],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0545,  0.0543,  0.0541,  0.0540, -0.9460,  0.0543,  0.0541,  0.0540],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0., -1.,  0.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0108,  0.0741, -0.0143],\n",
      "        [-0.0108,  0.0749, -0.0167],\n",
      "        [-0.0111,  0.0740, -0.0139],\n",
      "        [-0.0114,  0.0750, -0.0158],\n",
      "        [-0.0110,  0.0750, -0.0165],\n",
      "        [-0.0109,  0.0740, -0.0141],\n",
      "        [-0.0108,  0.0738, -0.0136],\n",
      "        [-0.0114,  0.0746, -0.0151]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0143],\n",
      "        [-0.0167],\n",
      "        [-0.0139],\n",
      "        [-0.0158],\n",
      "        [-0.0165],\n",
      "        [-0.0141],\n",
      "        [ 0.0738],\n",
      "        [ 0.0746]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0547, 0.0549, 0.0549, 0.0553, 0.0550, 0.0549, 0.0547, 0.0552],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0541, -0.9456,  0.0543,  0.0547, -0.9455,  0.0544,  1.0542,  0.0547],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1., -1.,  0.,  0., -1.,  0.,  0.,  1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0109,  0.0765, -0.0172],\n",
      "        [-0.0108,  0.0766, -0.0174],\n",
      "        [-0.0109,  0.0766, -0.0172],\n",
      "        [-0.0110,  0.0767, -0.0171],\n",
      "        [-0.0109,  0.0766, -0.0171],\n",
      "        [-0.0116,  0.0761, -0.0150],\n",
      "        [-0.0111,  0.0759, -0.0144],\n",
      "        [-0.0113,  0.0762, -0.0161]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0172],\n",
      "        [-0.0174],\n",
      "        [-0.0172],\n",
      "        [-0.0171],\n",
      "        [-0.0171],\n",
      "        [ 0.0761],\n",
      "        [ 0.0759],\n",
      "        [ 0.0762]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0551, 0.0552, 0.0552, 0.0556, 0.0552, 0.0553, 0.0551, 0.0552],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9455, -0.9453,  0.0546,  0.0550, -0.9454,  0.0547,  0.0545,  1.0546],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1., -1., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0105,  0.0802, -0.0203],\n",
      "        [-0.0104,  0.0798, -0.0181],\n",
      "        [-0.0109,  0.0802, -0.0197],\n",
      "        [-0.0110,  0.0803, -0.0190],\n",
      "        [-0.0104,  0.0803, -0.0206],\n",
      "        [-0.0104,  0.0802, -0.0206],\n",
      "        [-0.0105,  0.0803, -0.0203],\n",
      "        [-0.0105,  0.0805, -0.0203]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0203],\n",
      "        [ 0.0798],\n",
      "        [-0.0197],\n",
      "        [-0.0110],\n",
      "        [-0.0206],\n",
      "        [-0.0206],\n",
      "        [-0.0203],\n",
      "        [-0.0203]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0554, 0.0554, 0.0556, 0.0557, 0.0553, 0.0553, 0.0555, 0.0556],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0549,  0.0549,  0.0551,  0.0552, -0.9453, -0.9453, -0.9450,  0.0551],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0105,  0.0837, -0.0236],\n",
      "        [-0.0098,  0.0843, -0.0250],\n",
      "        [-0.0106,  0.0842, -0.0254],\n",
      "        [-0.0098,  0.0834, -0.0233],\n",
      "        [-0.0101,  0.0840, -0.0243],\n",
      "        [-0.0103,  0.0840, -0.0240],\n",
      "        [-0.0100,  0.0835, -0.0227],\n",
      "        [-0.0103,  0.0847, -0.0245]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0236],\n",
      "        [-0.0250],\n",
      "        [-0.0254],\n",
      "        [ 0.0834],\n",
      "        [ 0.0840],\n",
      "        [ 0.0840],\n",
      "        [-0.0100],\n",
      "        [-0.0103]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0559, 0.0557, 0.0562, 0.0555, 0.0558, 0.0557, 0.0557, 0.0564],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0554,  0.0551, -0.9443,  0.0550,  0.0553,  1.0551,  0.0551,  0.0559],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0092,  0.0893, -0.0292],\n",
      "        [-0.0091,  0.0894, -0.0300],\n",
      "        [-0.0090,  0.0898, -0.0307],\n",
      "        [-0.0089,  0.0900, -0.0312],\n",
      "        [-0.0090,  0.0890, -0.0287],\n",
      "        [-0.0092,  0.0900, -0.0304],\n",
      "        [-0.0089,  0.0898, -0.0314],\n",
      "        [-0.0094,  0.0894, -0.0301]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0292],\n",
      "        [ 0.0894],\n",
      "        [-0.0307],\n",
      "        [-0.0312],\n",
      "        [-0.0287],\n",
      "        [-0.0092],\n",
      "        [-0.0314],\n",
      "        [ 0.0894]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0559, 0.0563, 0.0561, 0.0562, 0.0559, 0.0563, 0.0560, 0.0560],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0553, 0.0557, 0.0555, 0.0557, 0.0553, 0.0558, 0.0554, 1.0554],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0080,  0.0965, -0.0357],\n",
      "        [-0.0076,  0.0968, -0.0366],\n",
      "        [-0.0080,  0.0969, -0.0360],\n",
      "        [-0.0082,  0.0967, -0.0363],\n",
      "        [-0.0079,  0.0967, -0.0363],\n",
      "        [-0.0078,  0.0958, -0.0346],\n",
      "        [-0.0077,  0.0967, -0.0369],\n",
      "        [-0.0080,  0.0965, -0.0353]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0965],\n",
      "        [-0.0366],\n",
      "        [-0.0080],\n",
      "        [-0.0363],\n",
      "        [-0.0363],\n",
      "        [-0.0346],\n",
      "        [-0.0369],\n",
      "        [ 0.0965]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0565, 0.0566, 0.0565, 0.0565, 0.0563, 0.0560, 0.0563, 0.0564],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0559, 0.0560, 0.0560, 0.0560, 0.0557, 0.0554, 0.0557, 0.0558],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0074,  0.1026, -0.0391],\n",
      "        [-0.0068,  0.1024, -0.0402],\n",
      "        [-0.0068,  0.1030, -0.0395],\n",
      "        [-0.0070,  0.1031, -0.0407],\n",
      "        [-0.0069,  0.1027, -0.0407],\n",
      "        [-0.0074,  0.1030, -0.0404],\n",
      "        [-0.0072,  0.1028, -0.0402],\n",
      "        [-0.0066,  0.1030, -0.0417]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1026],\n",
      "        [ 0.1024],\n",
      "        [ 0.1030],\n",
      "        [-0.0070],\n",
      "        [ 0.1027],\n",
      "        [-0.0074],\n",
      "        [ 0.1028],\n",
      "        [-0.0417]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0566, 0.0568, 0.0567, 0.0570, 0.0568, 0.0569, 0.0566, 0.0568],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0560, 0.0562, 0.0562, 0.0564, 0.0562, 0.0563, 0.0560, 0.0562],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0063,  0.1082, -0.0458],\n",
      "        [-0.0057,  0.1072, -0.0437],\n",
      "        [-0.0058,  0.1070, -0.0434],\n",
      "        [-0.0060,  0.1075, -0.0439],\n",
      "        [-0.0061,  0.1079, -0.0458],\n",
      "        [-0.0060,  0.1077, -0.0448],\n",
      "        [-0.0059,  0.1076, -0.0444],\n",
      "        [-0.0058,  0.1080, -0.0461]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0063],\n",
      "        [ 0.1072],\n",
      "        [-0.0434],\n",
      "        [ 0.1075],\n",
      "        [-0.0458],\n",
      "        [ 0.1077],\n",
      "        [ 0.1076],\n",
      "        [-0.0461]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0575, 0.0567, 0.0567, 0.0569, 0.0567, 0.0571, 0.0570, 0.0570],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0569, 0.0561, 0.0561, 0.0563, 0.0562, 0.0565, 0.0564, 0.0564],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0053,  0.1118, -0.0481],\n",
      "        [-0.0051,  0.1109, -0.0469],\n",
      "        [-0.0054,  0.1114, -0.0480],\n",
      "        [-0.0053,  0.1114, -0.0476],\n",
      "        [-0.0052,  0.1116, -0.0484],\n",
      "        [-0.0055,  0.1115, -0.0489],\n",
      "        [-0.0051,  0.1116, -0.0492],\n",
      "        [-0.0056,  0.1117, -0.0477]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0053],\n",
      "        [-0.0469],\n",
      "        [ 0.1114],\n",
      "        [ 0.1114],\n",
      "        [-0.0052],\n",
      "        [-0.0489],\n",
      "        [-0.0492],\n",
      "        [-0.0056]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0577, 0.0570, 0.0573, 0.0575, 0.0577, 0.0572, 0.0573, 0.0576],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0571,  0.0565, -0.9432,  0.0569,  0.0571,  0.0566,  0.0567,  0.0570],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0049,  0.1114, -0.0491],\n",
      "        [-0.0049,  0.1114, -0.0493],\n",
      "        [-0.0047,  0.1116, -0.0506],\n",
      "        [-0.0046,  0.1116, -0.0506],\n",
      "        [-0.0051,  0.1110, -0.0490],\n",
      "        [-0.0049,  0.1117, -0.0502],\n",
      "        [-0.0047,  0.1116, -0.0506],\n",
      "        [-0.0050,  0.1111, -0.0478]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1114],\n",
      "        [ 0.1114],\n",
      "        [-0.0506],\n",
      "        [-0.0506],\n",
      "        [ 0.1110],\n",
      "        [-0.0502],\n",
      "        [-0.0506],\n",
      "        [ 0.1111]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0578, 0.0578, 0.0576, 0.0576, 0.0573, 0.0579, 0.0577, 0.0576],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0572,  0.0572,  0.0571, -0.9430,  0.0567,  0.0573,  0.0571,  0.0570],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0043,  0.1112, -0.0513],\n",
      "        [-0.0041,  0.1115, -0.0527],\n",
      "        [-0.0042,  0.1115, -0.0526],\n",
      "        [-0.0043,  0.1109, -0.0502],\n",
      "        [-0.0040,  0.1114, -0.0529],\n",
      "        [-0.0042,  0.1108, -0.0502],\n",
      "        [-0.0043,  0.1115, -0.0516],\n",
      "        [-0.0049,  0.1109, -0.0506]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1112],\n",
      "        [-0.0527],\n",
      "        [-0.0526],\n",
      "        [-0.0502],\n",
      "        [-0.0529],\n",
      "        [ 0.1108],\n",
      "        [ 0.1115],\n",
      "        [ 0.1109]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0580, 0.0580, 0.0581, 0.0579, 0.0579, 0.0578, 0.0582, 0.0579],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0575,  0.0574,  0.0575,  0.0573, -0.9427,  0.0573,  0.0577,  0.0573],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0035,  0.1111, -0.0553],\n",
      "        [-0.0036,  0.1110, -0.0549],\n",
      "        [-0.0037,  0.1105, -0.0532],\n",
      "        [-0.0036,  0.1110, -0.0549],\n",
      "        [-0.0036,  0.1109, -0.0548],\n",
      "        [-0.0038,  0.1104, -0.0527],\n",
      "        [-0.0040,  0.1112, -0.0543],\n",
      "        [-0.0035,  0.1111, -0.0550]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0553],\n",
      "        [-0.0549],\n",
      "        [ 0.1105],\n",
      "        [-0.0549],\n",
      "        [-0.0548],\n",
      "        [-0.0038],\n",
      "        [-0.0040],\n",
      "        [-0.0550]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0584, 0.0583, 0.0586, 0.0583, 0.0584, 0.0583, 0.0586, 0.0585],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9421,  0.0577,  0.0581,  0.0577,  0.0579,  0.0577,  0.0580,  0.0579],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0027,  0.1115, -0.0577],\n",
      "        [-0.0033,  0.1112, -0.0565],\n",
      "        [-0.0028,  0.1116, -0.0575],\n",
      "        [-0.0024,  0.1119, -0.0588],\n",
      "        [-0.0029,  0.1115, -0.0568],\n",
      "        [-0.0026,  0.1108, -0.0570],\n",
      "        [-0.0030,  0.1110, -0.0559],\n",
      "        [-0.0025,  0.1118, -0.0584]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1115],\n",
      "        [ 0.1112],\n",
      "        [-0.0028],\n",
      "        [-0.0588],\n",
      "        [ 0.1115],\n",
      "        [ 0.1108],\n",
      "        [ 0.1110],\n",
      "        [-0.0584]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0588, 0.0591, 0.0589, 0.0586, 0.0587, 0.0582, 0.0582, 0.0587],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0582,  0.0585,  0.0584, -0.9420,  0.0581,  0.0576,  0.0576,  0.0581],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0015,  0.1124, -0.0623],\n",
      "        [-0.0016,  0.1117, -0.0601],\n",
      "        [-0.0013,  0.1123, -0.0626],\n",
      "        [-0.0016,  0.1122, -0.0618],\n",
      "        [-0.0013,  0.1125, -0.0627],\n",
      "        [-0.0014,  0.1114, -0.0603],\n",
      "        [-0.0012,  0.1123, -0.0626],\n",
      "        [-0.0014,  0.1119, -0.0617]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0623],\n",
      "        [-0.0601],\n",
      "        [-0.0626],\n",
      "        [-0.0618],\n",
      "        [-0.0627],\n",
      "        [-0.0603],\n",
      "        [-0.0626],\n",
      "        [ 0.1119]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0592, 0.0588, 0.0590, 0.0593, 0.0593, 0.0588, 0.0594, 0.0591],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0587, 1.0582, 0.0585, 0.0587, 1.0587, 0.0582, 0.0588, 0.0585],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0018,  0.1086, -0.0583],\n",
      "        [-0.0020,  0.1100, -0.0597],\n",
      "        [-0.0016,  0.1095, -0.0610],\n",
      "        [-0.0018,  0.1093, -0.0596],\n",
      "        [-0.0023,  0.1088, -0.0582],\n",
      "        [-0.0015,  0.1096, -0.0612],\n",
      "        [-0.0019,  0.1090, -0.0596],\n",
      "        [-0.0019,  0.1086, -0.0589]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1086],\n",
      "        [-0.0020],\n",
      "        [-0.0610],\n",
      "        [ 0.1093],\n",
      "        [-0.0582],\n",
      "        [-0.0612],\n",
      "        [-0.0596],\n",
      "        [-0.0589]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0593, 0.0601, 0.0593, 0.0595, 0.0593, 0.0594, 0.0591, 0.0590],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0587, 0.0595, 0.0587, 0.0589, 0.0587, 0.0588, 1.0585, 0.0585],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0026,  0.1054, -0.0565],\n",
      "        [-0.0025,  0.1053, -0.0568],\n",
      "        [-0.0026,  0.1053, -0.0568],\n",
      "        [-0.0026,  0.1045, -0.0552],\n",
      "        [-0.0028,  0.1054, -0.0562],\n",
      "        [-0.0028,  0.1052, -0.0559],\n",
      "        [-0.0025,  0.1053, -0.0569],\n",
      "        [-0.0027,  0.1047, -0.0545]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0565],\n",
      "        [-0.0568],\n",
      "        [-0.0568],\n",
      "        [-0.0552],\n",
      "        [-0.0028],\n",
      "        [-0.0028],\n",
      "        [-0.0569],\n",
      "        [ 0.1047]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0597, 0.0596, 0.0596, 0.0595, 0.0600, 0.0599, 0.0596, 0.0596],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0591, 0.0590, 0.0590, 1.0589, 0.0594, 0.0593, 0.0590, 0.0590],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0043,  0.1003, -0.0502],\n",
      "        [-0.0042,  0.0996, -0.0492],\n",
      "        [-0.0039,  0.1002, -0.0514],\n",
      "        [-0.0039,  0.0999, -0.0496],\n",
      "        [-0.0041,  0.0993, -0.0492],\n",
      "        [-0.0039,  0.1002, -0.0512],\n",
      "        [-0.0041,  0.0998, -0.0502],\n",
      "        [-0.0039,  0.1001, -0.0510]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0043],\n",
      "        [ 0.0996],\n",
      "        [-0.0514],\n",
      "        [-0.0039],\n",
      "        [ 0.0993],\n",
      "        [-0.0512],\n",
      "        [ 0.0998],\n",
      "        [-0.0510]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0606, 0.0594, 0.0600, 0.0599, 0.0593, 0.0600, 0.0601, 0.0599],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0599,  0.0588, -0.9406,  0.0593,  0.0587, -0.9406,  0.0595,  0.0593],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0045,  0.0963, -0.0466],\n",
      "        [-0.0044,  0.0970, -0.0485],\n",
      "        [-0.0045,  0.0966, -0.0468],\n",
      "        [-0.0043,  0.0969, -0.0487],\n",
      "        [-0.0043,  0.0970, -0.0486],\n",
      "        [-0.0046,  0.0968, -0.0482],\n",
      "        [-0.0044,  0.0963, -0.0466],\n",
      "        [-0.0047,  0.0965, -0.0473]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0045],\n",
      "        [-0.0485],\n",
      "        [-0.0468],\n",
      "        [-0.0487],\n",
      "        [-0.0486],\n",
      "        [-0.0482],\n",
      "        [-0.0466],\n",
      "        [-0.0473]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0601, 0.0604, 0.0600, 0.0602, 0.0602, 0.0604, 0.0601, 0.0601],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0595, 0.0598, 0.0594, 0.0596, 0.0596, 0.0597, 0.0595, 0.0595],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0048,  0.0930, -0.0439],\n",
      "        [-0.0050,  0.0931, -0.0443],\n",
      "        [-0.0050,  0.0937, -0.0448],\n",
      "        [-0.0049,  0.0932, -0.0448],\n",
      "        [-0.0048,  0.0936, -0.0463],\n",
      "        [-0.0054,  0.0939, -0.0452],\n",
      "        [-0.0051,  0.0934, -0.0449],\n",
      "        [-0.0051,  0.0936, -0.0446]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0439],\n",
      "        [-0.0443],\n",
      "        [ 0.0937],\n",
      "        [ 0.0932],\n",
      "        [-0.0463],\n",
      "        [-0.0054],\n",
      "        [-0.0449],\n",
      "        [ 0.0936]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0603, 0.0604, 0.0607, 0.0604, 0.0605, 0.0610, 0.0605, 0.0607],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0597,  0.0598,  0.0601,  0.0598, -0.9401, -0.9396,  0.0599,  0.0601],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0060,  0.0908, -0.0438],\n",
      "        [-0.0060,  0.0907, -0.0438],\n",
      "        [-0.0059,  0.0908, -0.0439],\n",
      "        [-0.0065,  0.0908, -0.0426],\n",
      "        [-0.0060,  0.0909, -0.0439],\n",
      "        [-0.0060,  0.0907, -0.0438],\n",
      "        [-0.0063,  0.0909, -0.0431],\n",
      "        [-0.0061,  0.0901, -0.0418]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0438],\n",
      "        [-0.0438],\n",
      "        [-0.0439],\n",
      "        [ 0.0908],\n",
      "        [-0.0439],\n",
      "        [-0.0438],\n",
      "        [-0.0063],\n",
      "        [ 0.0901]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0608, 0.0608, 0.0608, 0.0609, 0.0609, 0.0608, 0.0612, 0.0606],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0602, 0.0602, 1.0602, 0.0603, 0.0603, 0.0602, 0.0606, 0.0600],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0075,  0.0872, -0.0396],\n",
      "        [-0.0072,  0.0872, -0.0409],\n",
      "        [-0.0073,  0.0872, -0.0403],\n",
      "        [-0.0071,  0.0873, -0.0406],\n",
      "        [-0.0073,  0.0872, -0.0405],\n",
      "        [-0.0076,  0.0872, -0.0398],\n",
      "        [-0.0074,  0.0871, -0.0394],\n",
      "        [-0.0077,  0.0869, -0.0384]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0075],\n",
      "        [-0.0409],\n",
      "        [-0.0403],\n",
      "        [-0.0406],\n",
      "        [-0.0405],\n",
      "        [-0.0398],\n",
      "        [ 0.0871],\n",
      "        [-0.0384]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0613, 0.0611, 0.0611, 0.0611, 0.0610, 0.0613, 0.0611, 0.0610],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0607, -0.9395,  0.0605,  1.0605,  0.0604,  0.0607,  0.0605,  0.0604],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0081,  0.0840, -0.0376],\n",
      "        [-0.0087,  0.0841, -0.0365],\n",
      "        [-0.0085,  0.0840, -0.0363],\n",
      "        [-0.0083,  0.0833, -0.0351],\n",
      "        [-0.0086,  0.0836, -0.0350],\n",
      "        [-0.0087,  0.0841, -0.0365],\n",
      "        [-0.0085,  0.0842, -0.0364],\n",
      "        [-0.0087,  0.0840, -0.0362]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0376],\n",
      "        [-0.0087],\n",
      "        [ 0.0840],\n",
      "        [-0.0351],\n",
      "        [-0.0350],\n",
      "        [-0.0087],\n",
      "        [-0.0085],\n",
      "        [-0.0087]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0612, 0.0616, 0.0614, 0.0613, 0.0613, 0.0616, 0.0620, 0.0615],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9394,  0.0609,  0.0608,  0.0607,  0.0607,  1.0610,  0.0614,  0.0609],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1., -1.,  0.,  0.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0086,  0.0814, -0.0344],\n",
      "        [-0.0083,  0.0818, -0.0355],\n",
      "        [-0.0085,  0.0812, -0.0338],\n",
      "        [-0.0082,  0.0818, -0.0357],\n",
      "        [-0.0084,  0.0814, -0.0337],\n",
      "        [-0.0083,  0.0818, -0.0353],\n",
      "        [-0.0082,  0.0819, -0.0348],\n",
      "        [-0.0083,  0.0816, -0.0341]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0814],\n",
      "        [-0.0355],\n",
      "        [-0.0338],\n",
      "        [-0.0357],\n",
      "        [-0.0337],\n",
      "        [-0.0353],\n",
      "        [ 0.0819],\n",
      "        [-0.0083]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0608, 0.0614, 0.0614, 0.0617, 0.0613, 0.0616, 0.0616, 0.0614],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0602,  0.0608,  1.0608, -0.9390,  0.0607,  0.0610,  1.0610,  0.0608],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0081,  0.0814, -0.0341],\n",
      "        [-0.0083,  0.0811, -0.0332],\n",
      "        [-0.0085,  0.0814, -0.0336],\n",
      "        [-0.0082,  0.0808, -0.0322],\n",
      "        [-0.0086,  0.0813, -0.0330],\n",
      "        [-0.0084,  0.0814, -0.0329],\n",
      "        [-0.0085,  0.0814, -0.0332],\n",
      "        [-0.0081,  0.0813, -0.0340]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0341],\n",
      "        [-0.0083],\n",
      "        [-0.0336],\n",
      "        [-0.0322],\n",
      "        [-0.0086],\n",
      "        [ 0.0814],\n",
      "        [-0.0085],\n",
      "        [-0.0340]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0617, 0.0617, 0.0618, 0.0616, 0.0620, 0.0620, 0.0619, 0.0617],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0611, 0.0611, 0.0612, 1.0610, 0.0614, 0.0614, 0.0613, 0.0611],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0082,  0.0801, -0.0303],\n",
      "        [-0.0082,  0.0802, -0.0303],\n",
      "        [-0.0079,  0.0806, -0.0316],\n",
      "        [-0.0081,  0.0804, -0.0318],\n",
      "        [-0.0081,  0.0805, -0.0317],\n",
      "        [-0.0084,  0.0805, -0.0308],\n",
      "        [-0.0079,  0.0800, -0.0303],\n",
      "        [-0.0084,  0.0802, -0.0310]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0303],\n",
      "        [-0.0303],\n",
      "        [-0.0316],\n",
      "        [-0.0318],\n",
      "        [-0.0317],\n",
      "        [-0.0084],\n",
      "        [-0.0303],\n",
      "        [ 0.0802]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0617, 0.0617, 0.0620, 0.0619, 0.0619, 0.0623, 0.0617, 0.0620],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0611,  0.0611,  0.0614, -0.9387,  0.0613,  0.0617,  0.0611, -0.9387],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0082,  0.0784, -0.0300],\n",
      "        [-0.0081,  0.0778, -0.0283],\n",
      "        [-0.0081,  0.0783, -0.0294],\n",
      "        [-0.0085,  0.0782, -0.0293],\n",
      "        [-0.0083,  0.0780, -0.0289],\n",
      "        [-0.0081,  0.0783, -0.0298],\n",
      "        [-0.0083,  0.0780, -0.0289],\n",
      "        [-0.0081,  0.0783, -0.0297]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0300],\n",
      "        [-0.0283],\n",
      "        [-0.0081],\n",
      "        [ 0.0782],\n",
      "        [ 0.0780],\n",
      "        [-0.0298],\n",
      "        [ 0.0780],\n",
      "        [-0.0297]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0621, 0.0620, 0.0620, 0.0620, 0.0622, 0.0622, 0.0622, 0.0621],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9385,  0.0614,  0.0614, -0.9386,  0.0616,  0.0616,  0.0616,  0.0615],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  1.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0083,  0.0749, -0.0271],\n",
      "        [-0.0083,  0.0743, -0.0263],\n",
      "        [-0.0086,  0.0748, -0.0270],\n",
      "        [-0.0087,  0.0747, -0.0267],\n",
      "        [-0.0083,  0.0750, -0.0281],\n",
      "        [-0.0083,  0.0742, -0.0260],\n",
      "        [-0.0085,  0.0747, -0.0273],\n",
      "        [-0.0085,  0.0745, -0.0269]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0749],\n",
      "        [-0.0263],\n",
      "        [-0.0086],\n",
      "        [-0.0087],\n",
      "        [-0.0281],\n",
      "        [-0.0260],\n",
      "        [ 0.0747],\n",
      "        [ 0.0745]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0624, 0.0622, 0.0628, 0.0625, 0.0623, 0.0623, 0.0624, 0.0620],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9382,  1.0616,  0.0621,  0.0618,  0.0617,  0.0617, -0.9382,  0.0613],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0091,  0.0689, -0.0242],\n",
      "        [-0.0092,  0.0688, -0.0237],\n",
      "        [-0.0087,  0.0680, -0.0234],\n",
      "        [-0.0091,  0.0686, -0.0241],\n",
      "        [-0.0092,  0.0690, -0.0240],\n",
      "        [-0.0089,  0.0682, -0.0231],\n",
      "        [-0.0090,  0.0688, -0.0241],\n",
      "        [-0.0092,  0.0685, -0.0240]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0091],\n",
      "        [ 0.0688],\n",
      "        [-0.0234],\n",
      "        [ 0.0686],\n",
      "        [-0.0092],\n",
      "        [-0.0231],\n",
      "        [ 0.0688],\n",
      "        [ 0.0685]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0626, 0.0623, 0.0626, 0.0631, 0.0624, 0.0626, 0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0622, 0.0619, 0.0617, 0.0620, 0.0625, 0.0617, 0.0620, 0.0619],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1.,  0., -1.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0097,  0.0634, -0.0214],\n",
      "        [-0.0093,  0.0636, -0.0221],\n",
      "        [-0.0092,  0.0628, -0.0203],\n",
      "        [-0.0098,  0.0628, -0.0206],\n",
      "        [-0.0094,  0.0635, -0.0222],\n",
      "        [-0.0092,  0.0630, -0.0202],\n",
      "        [-0.0094,  0.0636, -0.0224],\n",
      "        [-0.0095,  0.0632, -0.0208]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0634],\n",
      "        [-0.0221],\n",
      "        [-0.0203],\n",
      "        [ 0.0628],\n",
      "        [-0.0222],\n",
      "        [-0.0202],\n",
      "        [-0.0224],\n",
      "        [-0.0095]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0627, 0.0627, 0.0624, 0.0623, 0.0626, 0.0618, 0.0626, 0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0621,  0.0621,  1.0618,  0.0616, -0.9380,  0.0612, -0.9380,  0.0619],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0089,  0.0590, -0.0199],\n",
      "        [-0.0092,  0.0586, -0.0187],\n",
      "        [-0.0090,  0.0584, -0.0187],\n",
      "        [-0.0090,  0.0588, -0.0190],\n",
      "        [-0.0091,  0.0589, -0.0193],\n",
      "        [-0.0092,  0.0582, -0.0183],\n",
      "        [-0.0093,  0.0586, -0.0195],\n",
      "        [-0.0093,  0.0590, -0.0193]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0199],\n",
      "        [ 0.0586],\n",
      "        [ 0.0584],\n",
      "        [-0.0090],\n",
      "        [ 0.0589],\n",
      "        [-0.0183],\n",
      "        [ 0.0586],\n",
      "        [-0.0093]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0633, 0.0625, 0.0626, 0.0623, 0.0628, 0.0625, 0.0628, 0.0630],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0626, 1.0619, 0.0619, 0.0617, 0.0622, 0.0619, 0.0622, 0.0624],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0090,  0.0568, -0.0182],\n",
      "        [-0.0093,  0.0569, -0.0183],\n",
      "        [-0.0089,  0.0569, -0.0189],\n",
      "        [-0.0089,  0.0569, -0.0188],\n",
      "        [-0.0089,  0.0570, -0.0189],\n",
      "        [-0.0089,  0.0570, -0.0188],\n",
      "        [-0.0091,  0.0566, -0.0176],\n",
      "        [-0.0088,  0.0560, -0.0175]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0568],\n",
      "        [-0.0093],\n",
      "        [-0.0189],\n",
      "        [-0.0188],\n",
      "        [-0.0189],\n",
      "        [-0.0188],\n",
      "        [-0.0176],\n",
      "        [ 0.0560]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0629, 0.0629, 0.0628, 0.0627, 0.0628, 0.0629, 0.0626, 0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0623, -0.9378, -0.9378,  0.0621,  0.0622,  0.0623,  0.0620,  0.0619],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0099,  0.0549, -0.0172],\n",
      "        [-0.0098,  0.0548, -0.0172],\n",
      "        [-0.0097,  0.0552, -0.0186],\n",
      "        [-0.0096,  0.0552, -0.0186],\n",
      "        [-0.0098,  0.0551, -0.0177],\n",
      "        [-0.0096,  0.0551, -0.0185],\n",
      "        [-0.0105,  0.0551, -0.0171],\n",
      "        [-0.0091,  0.0543, -0.0170]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0172],\n",
      "        [ 0.0548],\n",
      "        [-0.0186],\n",
      "        [-0.0186],\n",
      "        [ 0.0551],\n",
      "        [-0.0185],\n",
      "        [-0.0105],\n",
      "        [-0.0091]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0627, 0.0629, 0.0629, 0.0630, 0.0631, 0.0635, 0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0622, 1.0621, 0.0623, 0.0623, 0.0624, 0.0625, 0.0629, 1.0620],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0095,  0.0544, -0.0174],\n",
      "        [-0.0093,  0.0550, -0.0185],\n",
      "        [-0.0093,  0.0550, -0.0184],\n",
      "        [-0.0092,  0.0550, -0.0185],\n",
      "        [-0.0096,  0.0547, -0.0175],\n",
      "        [-0.0097,  0.0542, -0.0158],\n",
      "        [-0.0095,  0.0548, -0.0175],\n",
      "        [-0.0094,  0.0550, -0.0185]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0544],\n",
      "        [-0.0185],\n",
      "        [-0.0184],\n",
      "        [-0.0185],\n",
      "        [-0.0096],\n",
      "        [-0.0158],\n",
      "        [ 0.0548],\n",
      "        [-0.0185]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0629, 0.0630, 0.0629, 0.0631, 0.0630, 0.0627, 0.0627],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0622,  0.0623, -0.9376,  0.0623, -0.9375,  0.0624,  0.0621,  0.0620],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0098,  0.0546, -0.0175],\n",
      "        [-0.0097,  0.0548, -0.0188],\n",
      "        [-0.0098,  0.0547, -0.0175],\n",
      "        [-0.0096,  0.0541, -0.0169],\n",
      "        [-0.0097,  0.0548, -0.0188],\n",
      "        [-0.0099,  0.0547, -0.0175],\n",
      "        [-0.0099,  0.0548, -0.0179],\n",
      "        [-0.0098,  0.0547, -0.0175]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0546],\n",
      "        [-0.0188],\n",
      "        [ 0.0547],\n",
      "        [-0.0169],\n",
      "        [-0.0188],\n",
      "        [ 0.0547],\n",
      "        [-0.0099],\n",
      "        [ 0.0547]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0630, 0.0629, 0.0628, 0.0630, 0.0628, 0.0633, 0.0629],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0622,  0.0624,  0.0623,  0.0622,  0.0624,  0.0622, -0.9373,  0.0623],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0106,  0.0556, -0.0191],\n",
      "        [-0.0106,  0.0551, -0.0184],\n",
      "        [-0.0107,  0.0555, -0.0177],\n",
      "        [-0.0109,  0.0556, -0.0186],\n",
      "        [-0.0109,  0.0556, -0.0182],\n",
      "        [-0.0110,  0.0555, -0.0187],\n",
      "        [-0.0108,  0.0556, -0.0183],\n",
      "        [-0.0107,  0.0555, -0.0191]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0191],\n",
      "        [ 0.0551],\n",
      "        [-0.0107],\n",
      "        [-0.0109],\n",
      "        [-0.0109],\n",
      "        [-0.0110],\n",
      "        [-0.0108],\n",
      "        [-0.0191]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0631, 0.0631, 0.0629, 0.0633, 0.0634, 0.0634, 0.0634, 0.0627],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0625,  0.0625,  0.0623, -0.9374,  0.0627,  0.0628,  0.0628,  0.0621],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0118,  0.0560, -0.0182],\n",
      "        [-0.0117,  0.0559, -0.0180],\n",
      "        [-0.0115,  0.0560, -0.0191],\n",
      "        [-0.0117,  0.0560, -0.0179],\n",
      "        [-0.0116,  0.0560, -0.0190],\n",
      "        [-0.0121,  0.0560, -0.0185],\n",
      "        [-0.0119,  0.0556, -0.0178],\n",
      "        [-0.0116,  0.0560, -0.0191]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0118],\n",
      "        [-0.0117],\n",
      "        [-0.0191],\n",
      "        [-0.0117],\n",
      "        [-0.0190],\n",
      "        [ 0.0560],\n",
      "        [ 0.0556],\n",
      "        [-0.0191]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0639, 0.0630, 0.0630, 0.0630, 0.0632, 0.0632, 0.0630, 0.0631],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0633, 0.0624, 0.0624, 0.0624, 0.0626, 0.0626, 0.0624, 0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0123,  0.0563, -0.0191],\n",
      "        [-0.0123,  0.0565, -0.0186],\n",
      "        [-0.0124,  0.0559, -0.0172],\n",
      "        [-0.0121,  0.0565, -0.0185],\n",
      "        [-0.0125,  0.0564, -0.0181],\n",
      "        [-0.0123,  0.0565, -0.0192],\n",
      "        [-0.0124,  0.0564, -0.0176],\n",
      "        [-0.0125,  0.0564, -0.0184]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0191],\n",
      "        [-0.0186],\n",
      "        [ 0.0559],\n",
      "        [-0.0185],\n",
      "        [-0.0125],\n",
      "        [-0.0192],\n",
      "        [-0.0124],\n",
      "        [-0.0184]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0634, 0.0630, 0.0629, 0.0635, 0.0631, 0.0630, 0.0632],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0622,  0.0628,  0.0624,  0.0623,  0.0629, -0.9375,  0.0623,  0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0128,  0.0570, -0.0185],\n",
      "        [-0.0132,  0.0576, -0.0180],\n",
      "        [-0.0131,  0.0570, -0.0193],\n",
      "        [-0.0128,  0.0565, -0.0176],\n",
      "        [-0.0126,  0.0566, -0.0179],\n",
      "        [-0.0128,  0.0570, -0.0193],\n",
      "        [-0.0127,  0.0572, -0.0193],\n",
      "        [-0.0130,  0.0570, -0.0179]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0570],\n",
      "        [-0.0132],\n",
      "        [-0.0193],\n",
      "        [ 0.0565],\n",
      "        [ 0.0566],\n",
      "        [-0.0193],\n",
      "        [-0.0193],\n",
      "        [-0.0179]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0628, 0.0638, 0.0628, 0.0631, 0.0629, 0.0632, 0.0633, 0.0631],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0622, 0.0632, 0.0621, 0.0624, 0.0623, 0.0626, 0.0627, 0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0135,  0.0576, -0.0186],\n",
      "        [-0.0131,  0.0576, -0.0194],\n",
      "        [-0.0134,  0.0573, -0.0194],\n",
      "        [-0.0135,  0.0576, -0.0187],\n",
      "        [-0.0133,  0.0571, -0.0175],\n",
      "        [-0.0134,  0.0576, -0.0186],\n",
      "        [-0.0131,  0.0573, -0.0181],\n",
      "        [-0.0131,  0.0576, -0.0194]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0135],\n",
      "        [-0.0194],\n",
      "        [-0.0194],\n",
      "        [-0.0135],\n",
      "        [ 0.0571],\n",
      "        [-0.0134],\n",
      "        [-0.0131],\n",
      "        [-0.0194]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0638, 0.0634, 0.0630, 0.0636, 0.0630, 0.0635, 0.0632, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0632, 0.0627, 0.0624, 0.0629, 0.0624, 0.0629, 0.0626, 0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0135,  0.0574, -0.0172],\n",
      "        [-0.0134,  0.0580, -0.0187],\n",
      "        [-0.0133,  0.0579, -0.0191],\n",
      "        [-0.0136,  0.0574, -0.0175],\n",
      "        [-0.0135,  0.0579, -0.0183],\n",
      "        [-0.0133,  0.0581, -0.0193],\n",
      "        [-0.0132,  0.0575, -0.0177],\n",
      "        [-0.0137,  0.0575, -0.0168]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0172],\n",
      "        [-0.0187],\n",
      "        [-0.0191],\n",
      "        [-0.0175],\n",
      "        [ 0.0579],\n",
      "        [-0.0193],\n",
      "        [-0.0177],\n",
      "        [-0.0168]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0631, 0.0633, 0.0632, 0.0631, 0.0633, 0.0638, 0.0631, 0.0632],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0625, 0.0626, 0.0626, 1.0624, 0.0627, 0.0631, 1.0625, 0.0625],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0136,  0.0573, -0.0157],\n",
      "        [-0.0135,  0.0580, -0.0168],\n",
      "        [-0.0137,  0.0571, -0.0156],\n",
      "        [-0.0136,  0.0577, -0.0162],\n",
      "        [-0.0137,  0.0574, -0.0171],\n",
      "        [-0.0134,  0.0578, -0.0177],\n",
      "        [-0.0134,  0.0573, -0.0162],\n",
      "        [-0.0134,  0.0578, -0.0177]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0573],\n",
      "        [-0.0135],\n",
      "        [-0.0156],\n",
      "        [-0.0162],\n",
      "        [ 0.0574],\n",
      "        [-0.0177],\n",
      "        [ 0.0573],\n",
      "        [-0.0177]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0632, 0.0636, 0.0631, 0.0632, 0.0634, 0.0634, 0.0629, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0625, 0.0629, 0.0625, 0.0625, 0.0627, 0.0628, 0.0623, 0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0135,  0.0574, -0.0151],\n",
      "        [-0.0135,  0.0570, -0.0146],\n",
      "        [-0.0135,  0.0576, -0.0162],\n",
      "        [-0.0136,  0.0575, -0.0161],\n",
      "        [-0.0135,  0.0575, -0.0161],\n",
      "        [-0.0137,  0.0569, -0.0141],\n",
      "        [-0.0139,  0.0572, -0.0155],\n",
      "        [-0.0133,  0.0577, -0.0159]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0574],\n",
      "        [-0.0146],\n",
      "        [-0.0162],\n",
      "        [-0.0161],\n",
      "        [-0.0161],\n",
      "        [-0.0141],\n",
      "        [ 0.0572],\n",
      "        [-0.0159]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0634, 0.0630, 0.0633, 0.0634, 0.0628, 0.0632, 0.0632, 0.0632],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0628,  0.0624, -0.9374,  0.0627,  0.0622,  0.0625,  0.0626,  0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0137,  0.0575, -0.0150],\n",
      "        [-0.0136,  0.0574, -0.0150],\n",
      "        [-0.0136,  0.0574, -0.0149],\n",
      "        [-0.0138,  0.0573, -0.0141],\n",
      "        [-0.0139,  0.0574, -0.0139],\n",
      "        [-0.0137,  0.0574, -0.0151],\n",
      "        [-0.0137,  0.0574, -0.0150],\n",
      "        [-0.0136,  0.0569, -0.0133]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0150],\n",
      "        [-0.0150],\n",
      "        [-0.0149],\n",
      "        [ 0.0573],\n",
      "        [-0.0139],\n",
      "        [-0.0151],\n",
      "        [-0.0150],\n",
      "        [-0.0133]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0639, 0.0634, 0.0635, 0.0635, 0.0633, 0.0632, 0.0634, 0.0632],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0633,  0.0628,  0.0628,  0.0628,  0.0627, -0.9374,  0.0628,  0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0140,  0.0573, -0.0135],\n",
      "        [-0.0137,  0.0574, -0.0144],\n",
      "        [-0.0137,  0.0573, -0.0143],\n",
      "        [-0.0138,  0.0573, -0.0128],\n",
      "        [-0.0139,  0.0573, -0.0128],\n",
      "        [-0.0139,  0.0573, -0.0132],\n",
      "        [-0.0138,  0.0573, -0.0134],\n",
      "        [-0.0139,  0.0569, -0.0123]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0140],\n",
      "        [-0.0144],\n",
      "        [-0.0143],\n",
      "        [-0.0128],\n",
      "        [-0.0128],\n",
      "        [-0.0132],\n",
      "        [ 0.0573],\n",
      "        [ 0.0569]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0637, 0.0632, 0.0634, 0.0633, 0.0633, 0.0634, 0.0635, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0631, -0.9375,  0.0628,  0.0626,  0.0627,  0.0627,  0.0629,  0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0137,  0.0573, -0.0142],\n",
      "        [-0.0136,  0.0570, -0.0138],\n",
      "        [-0.0138,  0.0573, -0.0130],\n",
      "        [-0.0136,  0.0573, -0.0139],\n",
      "        [-0.0142,  0.0569, -0.0130],\n",
      "        [-0.0136,  0.0569, -0.0124],\n",
      "        [-0.0137,  0.0567, -0.0119],\n",
      "        [-0.0136,  0.0572, -0.0143]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0142],\n",
      "        [-0.0138],\n",
      "        [ 0.0573],\n",
      "        [-0.0139],\n",
      "        [ 0.0569],\n",
      "        [-0.0124],\n",
      "        [-0.0137],\n",
      "        [-0.0143]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0634, 0.0633, 0.0635, 0.0635, 0.0635, 0.0631, 0.0632, 0.0634],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9372,  0.0627,  0.0629,  0.0628,  0.0628,  0.0625,  0.0626, -0.9372],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0135,  0.0577, -0.0149],\n",
      "        [-0.0136,  0.0573, -0.0127],\n",
      "        [-0.0138,  0.0578, -0.0141],\n",
      "        [-0.0136,  0.0578, -0.0139],\n",
      "        [-0.0134,  0.0579, -0.0145],\n",
      "        [-0.0135,  0.0578, -0.0149],\n",
      "        [-0.0135,  0.0578, -0.0149],\n",
      "        [-0.0134,  0.0578, -0.0133]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0149],\n",
      "        [-0.0127],\n",
      "        [-0.0138],\n",
      "        [ 0.0578],\n",
      "        [ 0.0579],\n",
      "        [-0.0149],\n",
      "        [-0.0149],\n",
      "        [-0.0133]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0634, 0.0633, 0.0638, 0.0636, 0.0636, 0.0634, 0.0635, 0.0632],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9372,  0.0626,  0.0631,  0.0629,  1.0630,  0.0628,  0.0628,  0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0132,  0.0596, -0.0149],\n",
      "        [-0.0134,  0.0589, -0.0150],\n",
      "        [-0.0130,  0.0595, -0.0160],\n",
      "        [-0.0132,  0.0596, -0.0162],\n",
      "        [-0.0133,  0.0592, -0.0154],\n",
      "        [-0.0133,  0.0590, -0.0138],\n",
      "        [-0.0129,  0.0594, -0.0160],\n",
      "        [-0.0132,  0.0593, -0.0161]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0149],\n",
      "        [ 0.0589],\n",
      "        [-0.0160],\n",
      "        [-0.0162],\n",
      "        [ 0.0592],\n",
      "        [-0.0138],\n",
      "        [-0.0160],\n",
      "        [ 0.0593]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0635, 0.0632, 0.0635, 0.0635, 0.0636, 0.0633, 0.0633, 0.0635],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0628,  0.0626, -0.9371,  0.0629,  0.0629,  1.0627,  0.0627,  0.0628],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 1., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0127,  0.0610, -0.0153],\n",
      "        [-0.0129,  0.0604, -0.0146],\n",
      "        [-0.0126,  0.0606, -0.0152],\n",
      "        [-0.0127,  0.0607, -0.0149],\n",
      "        [-0.0126,  0.0610, -0.0169],\n",
      "        [-0.0128,  0.0611, -0.0153],\n",
      "        [-0.0128,  0.0610, -0.0151],\n",
      "        [-0.0128,  0.0605, -0.0147]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0153],\n",
      "        [-0.0146],\n",
      "        [ 0.0606],\n",
      "        [ 0.0607],\n",
      "        [-0.0169],\n",
      "        [ 0.0611],\n",
      "        [-0.0151],\n",
      "        [-0.0147]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0634, 0.0633, 0.0634, 0.0635, 0.0634, 0.0635, 0.0634, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0628, 1.0627, 0.0628, 0.0628, 0.0628, 0.0628, 0.0628, 0.0627],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0124,  0.0621, -0.0172],\n",
      "        [-0.0126,  0.0621, -0.0156],\n",
      "        [-0.0124,  0.0622, -0.0172],\n",
      "        [-0.0124,  0.0620, -0.0169],\n",
      "        [-0.0125,  0.0621, -0.0171],\n",
      "        [-0.0124,  0.0620, -0.0170],\n",
      "        [-0.0123,  0.0615, -0.0151],\n",
      "        [-0.0123,  0.0616, -0.0151]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0172],\n",
      "        [ 0.0621],\n",
      "        [-0.0172],\n",
      "        [-0.0169],\n",
      "        [-0.0171],\n",
      "        [-0.0170],\n",
      "        [ 0.0615],\n",
      "        [-0.0151]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0635, 0.0638, 0.0639, 0.0635, 0.0638, 0.0636, 0.0634, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9371,  0.0632,  1.0633,  0.0629,  0.0632,  0.0630,  0.0628,  0.0626],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  1.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0124,  0.0624, -0.0143],\n",
      "        [-0.0126,  0.0626, -0.0160],\n",
      "        [-0.0123,  0.0632, -0.0151],\n",
      "        [-0.0122,  0.0632, -0.0157],\n",
      "        [-0.0130,  0.0622, -0.0151],\n",
      "        [-0.0122,  0.0630, -0.0167],\n",
      "        [-0.0123,  0.0630, -0.0167],\n",
      "        [-0.0122,  0.0629, -0.0167]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0624],\n",
      "        [ 0.0626],\n",
      "        [ 0.0632],\n",
      "        [-0.0122],\n",
      "        [ 0.0622],\n",
      "        [-0.0167],\n",
      "        [-0.0167],\n",
      "        [-0.0167]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0634, 0.0637, 0.0637, 0.0636, 0.0640, 0.0638, 0.0638, 0.0637],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0628, -0.9369,  0.0630,  1.0630,  0.0633,  0.0632,  0.0632, -0.9370],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0118,  0.0625, -0.0168],\n",
      "        [-0.0119,  0.0625, -0.0154],\n",
      "        [-0.0119,  0.0625, -0.0159],\n",
      "        [-0.0124,  0.0622, -0.0155],\n",
      "        [-0.0118,  0.0621, -0.0146],\n",
      "        [-0.0121,  0.0623, -0.0160],\n",
      "        [-0.0118,  0.0624, -0.0167],\n",
      "        [-0.0120,  0.0626, -0.0160]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0168],\n",
      "        [ 0.0625],\n",
      "        [ 0.0625],\n",
      "        [-0.0124],\n",
      "        [-0.0146],\n",
      "        [ 0.0623],\n",
      "        [-0.0167],\n",
      "        [-0.0120]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0637, 0.0635, 0.0638, 0.0638, 0.0635, 0.0640, 0.0641, 0.0640],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0631,  0.0628, -0.9368, -0.9368,  0.0629,  0.0634,  0.0635,  0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0., -1.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0124,  0.0607, -0.0142],\n",
      "        [-0.0120,  0.0611, -0.0169],\n",
      "        [-0.0118,  0.0606, -0.0148],\n",
      "        [-0.0122,  0.0609, -0.0157],\n",
      "        [-0.0118,  0.0614, -0.0172],\n",
      "        [-0.0123,  0.0612, -0.0159],\n",
      "        [-0.0121,  0.0612, -0.0166],\n",
      "        [-0.0117,  0.0605, -0.0141]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0142],\n",
      "        [-0.0169],\n",
      "        [ 0.0606],\n",
      "        [ 0.0609],\n",
      "        [-0.0172],\n",
      "        [-0.0123],\n",
      "        [-0.0166],\n",
      "        [ 0.0605]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0638, 0.0637, 0.0634, 0.0638, 0.0639, 0.0640, 0.0638, 0.0636],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0631, -0.9369,  0.0627,  0.0632, -0.9368,  0.0634, -0.9369,  0.0629],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0120,  0.0609, -0.0181],\n",
      "        [-0.0120,  0.0609, -0.0182],\n",
      "        [-0.0121,  0.0611, -0.0163],\n",
      "        [-0.0121,  0.0609, -0.0173],\n",
      "        [-0.0121,  0.0608, -0.0177],\n",
      "        [-0.0121,  0.0607, -0.0179],\n",
      "        [-0.0120,  0.0609, -0.0182],\n",
      "        [-0.0123,  0.0605, -0.0161]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0181],\n",
      "        [-0.0182],\n",
      "        [-0.0163],\n",
      "        [ 0.0609],\n",
      "        [-0.0177],\n",
      "        [-0.0179],\n",
      "        [-0.0182],\n",
      "        [-0.0161]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0638, 0.0639, 0.0636, 0.0639, 0.0640, 0.0642, 0.0639, 0.0634],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0632, 0.0632, 0.0630, 0.0633, 0.0633, 0.0635, 0.0633, 1.0628],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  1., -1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0119,  0.0592, -0.0163],\n",
      "        [-0.0120,  0.0598, -0.0184],\n",
      "        [-0.0121,  0.0598, -0.0175],\n",
      "        [-0.0120,  0.0600, -0.0166],\n",
      "        [-0.0121,  0.0598, -0.0189],\n",
      "        [-0.0121,  0.0598, -0.0185],\n",
      "        [-0.0120,  0.0598, -0.0184],\n",
      "        [-0.0125,  0.0599, -0.0175]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0592],\n",
      "        [-0.0184],\n",
      "        [ 0.0598],\n",
      "        [ 0.0600],\n",
      "        [-0.0189],\n",
      "        [-0.0185],\n",
      "        [-0.0184],\n",
      "        [ 0.0599]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0632, 0.0638, 0.0639, 0.0637, 0.0639, 0.0638, 0.0638, 0.0637],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0625,  0.0632,  0.0633,  1.0630, -0.9368, -0.9368,  0.0632,  0.0631],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0118,  0.0612, -0.0197],\n",
      "        [-0.0116,  0.0613, -0.0201],\n",
      "        [-0.0120,  0.0612, -0.0191],\n",
      "        [-0.0117,  0.0613, -0.0198],\n",
      "        [-0.0116,  0.0613, -0.0198],\n",
      "        [-0.0118,  0.0612, -0.0190],\n",
      "        [-0.0122,  0.0608, -0.0188],\n",
      "        [-0.0117,  0.0612, -0.0199]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0197],\n",
      "        [-0.0201],\n",
      "        [-0.0120],\n",
      "        [-0.0198],\n",
      "        [-0.0198],\n",
      "        [ 0.0612],\n",
      "        [ 0.0608],\n",
      "        [-0.0199]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0638, 0.0638, 0.0641, 0.0639, 0.0638, 0.0641, 0.0643, 0.0638],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0632, -0.9368,  0.0635,  0.0632, -0.9369,  0.0634,  0.0636,  0.0631],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 1., 1., 0., 0., 1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0114,  0.0631, -0.0213],\n",
      "        [-0.0111,  0.0627, -0.0195],\n",
      "        [-0.0117,  0.0629, -0.0209],\n",
      "        [-0.0111,  0.0629, -0.0202],\n",
      "        [-0.0110,  0.0632, -0.0224],\n",
      "        [-0.0114,  0.0631, -0.0215],\n",
      "        [-0.0110,  0.0632, -0.0223],\n",
      "        [-0.0110,  0.0634, -0.0204]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0114],\n",
      "        [ 0.0627],\n",
      "        [ 0.0629],\n",
      "        [ 0.0629],\n",
      "        [-0.0224],\n",
      "        [-0.0114],\n",
      "        [-0.0223],\n",
      "        [ 0.0634]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0642, 0.0638, 0.0640, 0.0638, 0.0639, 0.0641, 0.0639, 0.0637],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0636, 0.0631, 0.0633, 1.0632, 1.0633, 0.0635, 0.0633, 1.0631],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0099,  0.0679, -0.0225],\n",
      "        [-0.0103,  0.0676, -0.0236],\n",
      "        [-0.0098,  0.0676, -0.0244],\n",
      "        [-0.0100,  0.0676, -0.0241],\n",
      "        [-0.0102,  0.0676, -0.0235],\n",
      "        [-0.0100,  0.0677, -0.0245],\n",
      "        [-0.0101,  0.0670, -0.0221],\n",
      "        [-0.0102,  0.0676, -0.0236]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0099],\n",
      "        [-0.0103],\n",
      "        [-0.0244],\n",
      "        [ 0.0676],\n",
      "        [-0.0102],\n",
      "        [-0.0245],\n",
      "        [ 0.0670],\n",
      "        [-0.0102]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0644, 0.0642, 0.0641, 0.0641, 0.0646, 0.0640, 0.0637, 0.0641],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0638,  0.0636, -0.9366,  0.0635,  0.0639,  0.0633,  0.0630,  0.0635],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0085,  0.0723, -0.0261],\n",
      "        [-0.0084,  0.0718, -0.0243],\n",
      "        [-0.0082,  0.0726, -0.0270],\n",
      "        [-0.0089,  0.0722, -0.0256],\n",
      "        [-0.0082,  0.0725, -0.0272],\n",
      "        [-0.0082,  0.0724, -0.0271],\n",
      "        [-0.0088,  0.0722, -0.0256],\n",
      "        [-0.0083,  0.0727, -0.0254]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0723],\n",
      "        [ 0.0718],\n",
      "        [-0.0270],\n",
      "        [ 0.0722],\n",
      "        [-0.0272],\n",
      "        [-0.0271],\n",
      "        [ 0.0722],\n",
      "        [-0.0254]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0641, 0.0638, 0.0640, 0.0641, 0.0641, 0.0640, 0.0640, 0.0639],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0634, 0.0631, 0.0634, 0.0634, 0.0635, 0.0633, 0.0634, 0.0633],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0066,  0.0771, -0.0296],\n",
      "        [-0.0064,  0.0772, -0.0298],\n",
      "        [-0.0065,  0.0773, -0.0278],\n",
      "        [-0.0065,  0.0771, -0.0297],\n",
      "        [-0.0068,  0.0767, -0.0281],\n",
      "        [-0.0067,  0.0764, -0.0266],\n",
      "        [-0.0070,  0.0770, -0.0286],\n",
      "        [-0.0067,  0.0773, -0.0299]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0296],\n",
      "        [-0.0298],\n",
      "        [-0.0065],\n",
      "        [-0.0297],\n",
      "        [-0.0068],\n",
      "        [ 0.0764],\n",
      "        [-0.0070],\n",
      "        [-0.0299]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0641, 0.0641, 0.0639, 0.0641, 0.0641, 0.0639, 0.0646, 0.0640],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0634, -0.9366,  0.0633,  0.0635,  0.0635,  0.0632,  0.0639, -0.9367],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0045,  0.0817, -0.0318],\n",
      "        [-0.0045,  0.0827, -0.0342],\n",
      "        [-0.0049,  0.0825, -0.0333],\n",
      "        [-0.0046,  0.0829, -0.0322],\n",
      "        [-0.0050,  0.0823, -0.0328],\n",
      "        [-0.0044,  0.0827, -0.0345],\n",
      "        [-0.0046,  0.0827, -0.0344],\n",
      "        [-0.0049,  0.0824, -0.0331]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0817],\n",
      "        [-0.0342],\n",
      "        [ 0.0825],\n",
      "        [-0.0322],\n",
      "        [-0.0328],\n",
      "        [-0.0345],\n",
      "        [-0.0344],\n",
      "        [-0.0331]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0637, 0.0641, 0.0643, 0.0642, 0.0641, 0.0642, 0.0641, 0.0641],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0631, -0.9365,  0.0637,  0.0635,  0.0635,  0.0636,  0.0635,  0.0634],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[-0.0026,  0.0884, -0.0397],\n",
      "        [-0.0024,  0.0884, -0.0403],\n",
      "        [-0.0027,  0.0884, -0.0394],\n",
      "        [-0.0026,  0.0874, -0.0375],\n",
      "        [-0.0028,  0.0882, -0.0393],\n",
      "        [-0.0025,  0.0876, -0.0369],\n",
      "        [-0.0028,  0.0882, -0.0389],\n",
      "        [-0.0030,  0.0883, -0.0388]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0026],\n",
      "        [-0.0403],\n",
      "        [-0.0027],\n",
      "        [-0.0026],\n",
      "        [-0.0028],\n",
      "        [ 0.0876],\n",
      "        [-0.0389],\n",
      "        [ 0.0883]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0644, 0.0643, 0.0645, 0.0639, 0.0645, 0.0640, 0.0642, 0.0645],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0638, 0.0637, 0.0639, 0.0633, 1.0639, 0.0634, 0.0635, 0.0638],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0., -1.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0013,  0.0946, -0.0442],\n",
      "        [ 0.0014,  0.0938, -0.0427],\n",
      "        [ 0.0012,  0.0942, -0.0449],\n",
      "        [ 0.0013,  0.0948, -0.0451],\n",
      "        [ 0.0018,  0.0945, -0.0456],\n",
      "        [ 0.0011,  0.0944, -0.0441],\n",
      "        [ 0.0013,  0.0942, -0.0450],\n",
      "        [ 0.0016,  0.0942, -0.0436]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0946],\n",
      "        [ 0.0014],\n",
      "        [ 0.0942],\n",
      "        [ 0.0013],\n",
      "        [-0.0456],\n",
      "        [ 0.0011],\n",
      "        [ 0.0942],\n",
      "        [-0.0436]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0640, 0.0645, 0.0644, 0.0646, 0.0643, 0.0647, 0.0644, 0.0641],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0634,  0.0638, -0.9363,  0.0640,  0.0637,  0.0641,  0.0638,  0.0635],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0060,  0.0998, -0.0499],\n",
      "        [ 0.0059,  0.0995, -0.0498],\n",
      "        [ 0.0059,  0.0997, -0.0499],\n",
      "        [ 0.0061,  0.0997, -0.0506],\n",
      "        [ 0.0060,  0.0994, -0.0486],\n",
      "        [ 0.0063,  0.0997, -0.0510],\n",
      "        [ 0.0061,  0.1000, -0.0510],\n",
      "        [ 0.0064,  0.0998, -0.0508]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0060],\n",
      "        [ 0.0995],\n",
      "        [ 0.0059],\n",
      "        [-0.0506],\n",
      "        [ 0.0994],\n",
      "        [-0.0510],\n",
      "        [-0.0510],\n",
      "        [-0.0508]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0651, 0.0646, 0.0647, 0.0640, 0.0640, 0.0644, 0.0649, 0.0647],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0645, 0.0639, 0.0641, 0.0633, 0.0634, 0.0637, 0.0642, 0.0640],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  1.,  0., -1.,  0., -1.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0096,  0.1025, -0.0522],\n",
      "        [ 0.0099,  0.1022, -0.0502],\n",
      "        [ 0.0102,  0.1028, -0.0533],\n",
      "        [ 0.0103,  0.1027, -0.0533],\n",
      "        [ 0.0099,  0.1028, -0.0523],\n",
      "        [ 0.0104,  0.1029, -0.0535],\n",
      "        [ 0.0102,  0.1031, -0.0514],\n",
      "        [ 0.0097,  0.1026, -0.0526]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1025],\n",
      "        [-0.0502],\n",
      "        [-0.0533],\n",
      "        [-0.0533],\n",
      "        [ 0.0099],\n",
      "        [-0.0535],\n",
      "        [-0.0514],\n",
      "        [ 0.1026]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0651, 0.0644, 0.0645, 0.0646, 0.0648, 0.0645, 0.0645, 0.0648],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0644,  1.0638,  0.0639, -0.9361,  0.0642, -0.9362,  1.0639,  0.0641],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0132,  0.1041, -0.0533],\n",
      "        [ 0.0131,  0.1038, -0.0524],\n",
      "        [ 0.0131,  0.1038, -0.0523],\n",
      "        [ 0.0133,  0.1044, -0.0515],\n",
      "        [ 0.0134,  0.1042, -0.0515],\n",
      "        [ 0.0131,  0.1039, -0.0532],\n",
      "        [ 0.0134,  0.1038, -0.0534],\n",
      "        [ 0.0131,  0.1038, -0.0532]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0533],\n",
      "        [ 0.0131],\n",
      "        [ 0.0131],\n",
      "        [ 0.1044],\n",
      "        [-0.0515],\n",
      "        [ 0.0131],\n",
      "        [-0.0534],\n",
      "        [-0.0532]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0644, 0.0649, 0.0649, 0.0646, 0.0646, 0.0649, 0.0648, 0.0644],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9362,  0.0643,  0.0642,  0.0639,  0.0639,  1.0643,  0.0641,  0.0637],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 1., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0201,  0.1082, -0.0553],\n",
      "        [ 0.0198,  0.1073, -0.0557],\n",
      "        [ 0.0195,  0.1080, -0.0562],\n",
      "        [ 0.0201,  0.1078, -0.0550],\n",
      "        [ 0.0203,  0.1078, -0.0571],\n",
      "        [ 0.0202,  0.1079, -0.0573],\n",
      "        [ 0.0199,  0.1083, -0.0555],\n",
      "        [ 0.0202,  0.1077, -0.0568]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0553],\n",
      "        [ 0.1073],\n",
      "        [ 0.1080],\n",
      "        [ 0.1078],\n",
      "        [-0.0571],\n",
      "        [-0.0573],\n",
      "        [-0.0555],\n",
      "        [-0.0568]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0648, 0.0651, 0.0648, 0.0648, 0.0649, 0.0648, 0.0643, 0.0648],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0641, 0.0644, 1.0641, 0.0641, 0.0642, 0.0642, 0.0636, 0.0641],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0261,  0.1130, -0.0539],\n",
      "        [ 0.0262,  0.1134, -0.0565],\n",
      "        [ 0.0262,  0.1136, -0.0567],\n",
      "        [ 0.0257,  0.1139, -0.0553],\n",
      "        [ 0.0264,  0.1135, -0.0569],\n",
      "        [ 0.0264,  0.1135, -0.0569],\n",
      "        [ 0.0258,  0.1133, -0.0561],\n",
      "        [ 0.0259,  0.1135, -0.0560]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1130],\n",
      "        [-0.0565],\n",
      "        [-0.0567],\n",
      "        [ 0.1139],\n",
      "        [-0.0569],\n",
      "        [-0.0569],\n",
      "        [ 0.1133],\n",
      "        [ 0.1135]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0647, 0.0648, 0.0651, 0.0649, 0.0649, 0.0649, 0.0652, 0.0650],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0641, 0.0642, 0.0644, 0.0642, 0.0642, 0.0642, 0.0646, 0.0643],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0306,  0.1150, -0.0528],\n",
      "        [ 0.0308,  0.1153, -0.0532],\n",
      "        [ 0.0307,  0.1153, -0.0529],\n",
      "        [ 0.0301,  0.1151, -0.0519],\n",
      "        [ 0.0299,  0.1147, -0.0520],\n",
      "        [ 0.0308,  0.1154, -0.0533],\n",
      "        [ 0.0307,  0.1153, -0.0529],\n",
      "        [ 0.0302,  0.1152, -0.0521]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0528],\n",
      "        [-0.0532],\n",
      "        [-0.0529],\n",
      "        [ 0.1151],\n",
      "        [ 0.1147],\n",
      "        [-0.0533],\n",
      "        [-0.0529],\n",
      "        [ 0.0302]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0650, 0.0651, 0.0651, 0.0655, 0.0651, 0.0650, 0.0649, 0.0653],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0644,  0.0645,  0.0644,  0.0648,  0.0645,  0.0644, -0.9357,  0.0647],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0347,  0.1171, -0.0491],\n",
      "        [ 0.0348,  0.1178, -0.0521],\n",
      "        [ 0.0348,  0.1167, -0.0495],\n",
      "        [ 0.0345,  0.1174, -0.0508],\n",
      "        [ 0.0344,  0.1183, -0.0507],\n",
      "        [ 0.0349,  0.1178, -0.0524],\n",
      "        [ 0.0351,  0.1177, -0.0520],\n",
      "        [ 0.0346,  0.1176, -0.0497]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0491],\n",
      "        [-0.0521],\n",
      "        [ 0.1167],\n",
      "        [ 0.1174],\n",
      "        [ 0.1183],\n",
      "        [-0.0524],\n",
      "        [-0.0520],\n",
      "        [ 0.1176]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0649, 0.0648, 0.0648, 0.0653, 0.0656, 0.0651, 0.0652, 0.0650],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0643,  0.0641,  0.0642,  0.0646,  0.0649, -0.9355,  0.0646,  0.0643],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0370,  0.1165, -0.0478],\n",
      "        [ 0.0369,  0.1157, -0.0455],\n",
      "        [ 0.0373,  0.1163, -0.0485],\n",
      "        [ 0.0371,  0.1164, -0.0480],\n",
      "        [ 0.0368,  0.1156, -0.0456],\n",
      "        [ 0.0367,  0.1160, -0.0473],\n",
      "        [ 0.0368,  0.1162, -0.0475],\n",
      "        [ 0.0368,  0.1162, -0.0474]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0478],\n",
      "        [-0.0455],\n",
      "        [-0.0485],\n",
      "        [-0.0480],\n",
      "        [-0.0456],\n",
      "        [ 0.1160],\n",
      "        [ 0.0368],\n",
      "        [ 0.0368]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0653, 0.0651, 0.0654, 0.0648, 0.0651, 0.0655, 0.0656, 0.0656],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0647, 0.0645, 0.0648, 0.0642, 1.0644, 0.0648, 0.0649, 0.0649],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0366,  0.1117, -0.0397],\n",
      "        [ 0.0361,  0.1109, -0.0381],\n",
      "        [ 0.0368,  0.1115, -0.0395],\n",
      "        [ 0.0362,  0.1114, -0.0384],\n",
      "        [ 0.0361,  0.1115, -0.0380],\n",
      "        [ 0.0362,  0.1114, -0.0385],\n",
      "        [ 0.0369,  0.1114, -0.0397],\n",
      "        [ 0.0362,  0.1118, -0.0381]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0397],\n",
      "        [ 0.1109],\n",
      "        [-0.0395],\n",
      "        [ 0.0362],\n",
      "        [ 0.1115],\n",
      "        [ 0.0362],\n",
      "        [-0.0397],\n",
      "        [ 0.0362]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0655, 0.0657, 0.0654, 0.0658, 0.0653, 0.0657, 0.0654, 0.0655],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0649, 0.0650, 0.0648, 0.0651, 1.0647, 0.0651, 0.0647, 0.0648],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0381,  0.1147, -0.0310],\n",
      "        [ 0.0387,  0.1151, -0.0321],\n",
      "        [ 0.0385,  0.1138, -0.0299],\n",
      "        [ 0.0382,  0.1144, -0.0317],\n",
      "        [ 0.0386,  0.1148, -0.0327],\n",
      "        [ 0.0383,  0.1140, -0.0299],\n",
      "        [ 0.0383,  0.1145, -0.0316],\n",
      "        [ 0.0389,  0.1147, -0.0325]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1147],\n",
      "        [-0.0321],\n",
      "        [ 0.1138],\n",
      "        [ 0.1144],\n",
      "        [-0.0327],\n",
      "        [ 0.0383],\n",
      "        [ 0.0383],\n",
      "        [-0.0325]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0655, 0.0658, 0.0652, 0.0654, 0.0656, 0.0654, 0.0655, 0.0656],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0648,  0.0651,  0.0645,  0.0647, -0.9351,  0.0647,  0.0648,  0.0649],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0439,  0.1250, -0.0306],\n",
      "        [ 0.0430,  0.1244, -0.0294],\n",
      "        [ 0.0439,  0.1253, -0.0305],\n",
      "        [ 0.0433,  0.1253, -0.0301],\n",
      "        [ 0.0432,  0.1250, -0.0295],\n",
      "        [ 0.0430,  0.1251, -0.0291],\n",
      "        [ 0.0434,  0.1250, -0.0296],\n",
      "        [ 0.0439,  0.1252, -0.0306]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0306],\n",
      "        [ 0.1244],\n",
      "        [-0.0305],\n",
      "        [ 0.1253],\n",
      "        [ 0.1250],\n",
      "        [ 0.1251],\n",
      "        [ 0.0434],\n",
      "        [-0.0306]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0657, 0.0658, 0.0657, 0.0660, 0.0660, 0.0657, 0.0656, 0.0659],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0651, 0.0652, 0.0651, 0.0653, 0.0653, 0.0651, 0.0650, 0.0653],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0475,  0.1329, -0.0265],\n",
      "        [ 0.0466,  0.1324, -0.0237],\n",
      "        [ 0.0477,  0.1328, -0.0265],\n",
      "        [ 0.0468,  0.1328, -0.0256],\n",
      "        [ 0.0478,  0.1329, -0.0266],\n",
      "        [ 0.0475,  0.1328, -0.0265],\n",
      "        [ 0.0474,  0.1328, -0.0270],\n",
      "        [ 0.0471,  0.1328, -0.0256]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0265],\n",
      "        [ 0.1324],\n",
      "        [-0.0265],\n",
      "        [ 0.1328],\n",
      "        [-0.0266],\n",
      "        [-0.0265],\n",
      "        [-0.0270],\n",
      "        [ 0.0471]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0659, 0.0655, 0.0658, 0.0659, 0.0660, 0.0660, 0.0658, 0.0662],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0653,  0.0648, -0.9348,  0.0652,  0.0653,  0.0654, -0.9348,  0.0655],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0517,  0.1403, -0.0295],\n",
      "        [ 0.0516,  0.1403, -0.0296],\n",
      "        [ 0.0524,  0.1406, -0.0307],\n",
      "        [ 0.0518,  0.1398, -0.0278],\n",
      "        [ 0.0518,  0.1406, -0.0294],\n",
      "        [ 0.0523,  0.1405, -0.0309],\n",
      "        [ 0.0523,  0.1406, -0.0305],\n",
      "        [ 0.0523,  0.1404, -0.0305]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1403],\n",
      "        [ 0.1403],\n",
      "        [-0.0307],\n",
      "        [ 0.1398],\n",
      "        [ 0.0518],\n",
      "        [-0.0309],\n",
      "        [-0.0305],\n",
      "        [-0.0305]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0662, 0.0663, 0.0662, 0.0658, 0.0663, 0.0658, 0.0661, 0.0662],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0655,  0.0656,  0.0656,  0.0652, -0.9344, -0.9349,  0.0654,  0.0655],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0503,  0.1432, -0.0345],\n",
      "        [ 0.0510,  0.1428, -0.0353],\n",
      "        [ 0.0506,  0.1436, -0.0345],\n",
      "        [ 0.0512,  0.1422, -0.0348],\n",
      "        [ 0.0509,  0.1430, -0.0357],\n",
      "        [ 0.0504,  0.1429, -0.0344],\n",
      "        [ 0.0510,  0.1432, -0.0353],\n",
      "        [ 0.0505,  0.1429, -0.0342]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0503],\n",
      "        [-0.0353],\n",
      "        [ 0.0506],\n",
      "        [-0.0348],\n",
      "        [-0.0357],\n",
      "        [ 0.1429],\n",
      "        [-0.0353],\n",
      "        [ 0.0505]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0670, 0.0663, 0.0662, 0.0661, 0.0662, 0.0667, 0.0658, 0.0665],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0663,  0.0657,  1.0655,  0.0654, -0.9345,  0.0661,  0.0651,  0.0658],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0.,  0.,  0., -1.,  1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0554,  0.1491, -0.0423],\n",
      "        [ 0.0553,  0.1483, -0.0411],\n",
      "        [ 0.0554,  0.1485, -0.0423],\n",
      "        [ 0.0558,  0.1484, -0.0430],\n",
      "        [ 0.0556,  0.1486, -0.0429],\n",
      "        [ 0.0551,  0.1482, -0.0420],\n",
      "        [ 0.0557,  0.1485, -0.0434],\n",
      "        [ 0.0551,  0.1486, -0.0415]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0554],\n",
      "        [-0.0411],\n",
      "        [-0.0423],\n",
      "        [-0.0430],\n",
      "        [-0.0429],\n",
      "        [ 0.1482],\n",
      "        [-0.0434],\n",
      "        [-0.0415]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0664, 0.0663, 0.0664, 0.0664, 0.0661, 0.0667, 0.0664, 0.0664],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0657,  0.0656,  0.0657,  0.0658,  0.0655,  0.0660, -0.9343,  1.0658],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([-1.,  0.,  0., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0634,  0.1548, -0.0490],\n",
      "        [ 0.0632,  0.1539, -0.0478],\n",
      "        [ 0.0631,  0.1543, -0.0474],\n",
      "        [ 0.0639,  0.1545, -0.0488],\n",
      "        [ 0.0640,  0.1545, -0.0490],\n",
      "        [ 0.0637,  0.1543, -0.0486],\n",
      "        [ 0.0635,  0.1547, -0.0484],\n",
      "        [ 0.0641,  0.1545, -0.0490]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0490],\n",
      "        [ 0.1539],\n",
      "        [ 0.1543],\n",
      "        [-0.0488],\n",
      "        [-0.0490],\n",
      "        [-0.0486],\n",
      "        [ 0.1547],\n",
      "        [-0.0490]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0671, 0.0667, 0.0660, 0.0665, 0.0667, 0.0668, 0.0669, 0.0665],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([-0.9336,  0.0661,  0.0653, -0.9342,  0.0661,  0.0661,  0.0662,  0.0659],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0722,  0.1603, -0.0589],\n",
      "        [ 0.0727,  0.1605, -0.0592],\n",
      "        [ 0.0729,  0.1609, -0.0594],\n",
      "        [ 0.0727,  0.1600, -0.0592],\n",
      "        [ 0.0728,  0.1604, -0.0591],\n",
      "        [ 0.0725,  0.1605, -0.0589],\n",
      "        [ 0.0733,  0.1605, -0.0605],\n",
      "        [ 0.0731,  0.1607, -0.0602]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0722],\n",
      "        [ 0.1605],\n",
      "        [-0.0594],\n",
      "        [ 0.1600],\n",
      "        [ 0.1604],\n",
      "        [ 0.1605],\n",
      "        [-0.0605],\n",
      "        [-0.0602]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0669, 0.0670, 0.0667, 0.0669, 0.0670, 0.0670, 0.0669, 0.0667],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0663, 0.0663, 0.0661, 0.0663, 0.0663, 0.0664, 1.0662, 0.0661],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0767,  0.1590, -0.0611],\n",
      "        [ 0.0774,  0.1578, -0.0605],\n",
      "        [ 0.0771,  0.1582, -0.0599],\n",
      "        [ 0.0771,  0.1578, -0.0603],\n",
      "        [ 0.0780,  0.1588, -0.0633],\n",
      "        [ 0.0780,  0.1588, -0.0634],\n",
      "        [ 0.0781,  0.1587, -0.0633],\n",
      "        [ 0.0778,  0.1588, -0.0623]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1590],\n",
      "        [-0.0605],\n",
      "        [ 0.1582],\n",
      "        [-0.0603],\n",
      "        [-0.0633],\n",
      "        [-0.0634],\n",
      "        [-0.0633],\n",
      "        [ 0.1588]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0671, 0.0667, 0.0666, 0.0669, 0.0670, 0.0670, 0.0670, 0.0669],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0665, 0.0661, 0.0660, 0.0662, 0.0663, 0.0664, 0.0663, 0.0662],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1., -1.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0797,  0.1536, -0.0611],\n",
      "        [ 0.0799,  0.1529, -0.0620],\n",
      "        [ 0.0797,  0.1529, -0.0614],\n",
      "        [ 0.0798,  0.1530, -0.0624],\n",
      "        [ 0.0796,  0.1521, -0.0603],\n",
      "        [ 0.0795,  0.1530, -0.0614],\n",
      "        [ 0.0798,  0.1529, -0.0620],\n",
      "        [ 0.0799,  0.1530, -0.0621]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0797],\n",
      "        [-0.0620],\n",
      "        [ 0.0797],\n",
      "        [-0.0624],\n",
      "        [-0.0603],\n",
      "        [-0.0614],\n",
      "        [-0.0620],\n",
      "        [-0.0621]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0671, 0.0673, 0.0672, 0.0671, 0.0670, 0.0670, 0.0672, 0.0672],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0665,  0.0666, -0.9335, -0.9335,  0.0663,  0.0664,  0.0665,  0.0666],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0736,  0.1448, -0.0585],\n",
      "        [ 0.0738,  0.1458, -0.0580],\n",
      "        [ 0.0739,  0.1453, -0.0589],\n",
      "        [ 0.0740,  0.1455, -0.0588],\n",
      "        [ 0.0739,  0.1458, -0.0581],\n",
      "        [ 0.0737,  0.1459, -0.0589],\n",
      "        [ 0.0739,  0.1457, -0.0582],\n",
      "        [ 0.0735,  0.1445, -0.0566]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1448],\n",
      "        [ 0.0738],\n",
      "        [ 0.0739],\n",
      "        [ 0.1455],\n",
      "        [ 0.1458],\n",
      "        [-0.0589],\n",
      "        [-0.0582],\n",
      "        [-0.0566]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0675, 0.0673, 0.0677, 0.0672, 0.0674, 0.0674, 0.0673, 0.0670],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0668, 0.0667, 0.0670, 0.0666, 0.0667, 0.0667, 0.0667, 0.0664],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0677,  0.1362, -0.0556],\n",
      "        [ 0.0678,  0.1363, -0.0556],\n",
      "        [ 0.0674,  0.1356, -0.0533],\n",
      "        [ 0.0673,  0.1367, -0.0539],\n",
      "        [ 0.0668,  0.1358, -0.0528],\n",
      "        [ 0.0677,  0.1363, -0.0557],\n",
      "        [ 0.0678,  0.1362, -0.0555],\n",
      "        [ 0.0672,  0.1355, -0.0527]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0556],\n",
      "        [-0.0556],\n",
      "        [ 0.1356],\n",
      "        [-0.0539],\n",
      "        [-0.0528],\n",
      "        [-0.0557],\n",
      "        [-0.0555],\n",
      "        [-0.0527]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0677, 0.0676, 0.0673, 0.0677, 0.0674, 0.0671, 0.0677, 0.0673],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0670, 0.0669, 0.0666, 0.0670, 0.0667, 0.0664, 0.0670, 0.0666],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0611,  0.1265, -0.0488],\n",
      "        [ 0.0612,  0.1264, -0.0486],\n",
      "        [ 0.0608,  0.1264, -0.0478],\n",
      "        [ 0.0612,  0.1264, -0.0489],\n",
      "        [ 0.0608,  0.1269, -0.0471],\n",
      "        [ 0.0609,  0.1272, -0.0467],\n",
      "        [ 0.0608,  0.1270, -0.0471],\n",
      "        [ 0.0607,  0.1258, -0.0461]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0488],\n",
      "        [-0.0486],\n",
      "        [ 0.0608],\n",
      "        [-0.0489],\n",
      "        [-0.0471],\n",
      "        [-0.0467],\n",
      "        [-0.0471],\n",
      "        [ 0.0607]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0677, 0.0678, 0.0680, 0.0676, 0.0677, 0.0677, 0.0676, 0.0675],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0671, -0.9329,  0.0673,  0.0669,  1.0670,  0.0670,  0.0670,  0.0668],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  1.,  0., -1.,  1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0547,  0.1174, -0.0412],\n",
      "        [ 0.0548,  0.1174, -0.0412],\n",
      "        [ 0.0544,  0.1167, -0.0384],\n",
      "        [ 0.0543,  0.1168, -0.0382],\n",
      "        [ 0.0546,  0.1168, -0.0405],\n",
      "        [ 0.0546,  0.1174, -0.0419],\n",
      "        [ 0.0545,  0.1178, -0.0396],\n",
      "        [ 0.0548,  0.1174, -0.0412]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0412],\n",
      "        [-0.0412],\n",
      "        [-0.0384],\n",
      "        [ 0.0543],\n",
      "        [ 0.0546],\n",
      "        [-0.0419],\n",
      "        [ 0.1178],\n",
      "        [-0.0412]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0681, 0.0679, 0.0676, 0.0676, 0.0675, 0.0678, 0.0679, 0.0679],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0674,  0.0672,  0.0669,  1.0669,  0.0668, -0.9329,  1.0672,  0.0672],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0.,  0.,  1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0558,  0.1181, -0.0384],\n",
      "        [ 0.0562,  0.1179, -0.0397],\n",
      "        [ 0.0557,  0.1181, -0.0391],\n",
      "        [ 0.0562,  0.1180, -0.0393],\n",
      "        [ 0.0559,  0.1171, -0.0367],\n",
      "        [ 0.0558,  0.1179, -0.0384],\n",
      "        [ 0.0562,  0.1180, -0.0393],\n",
      "        [ 0.0557,  0.1172, -0.0364]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1181],\n",
      "        [-0.0397],\n",
      "        [ 0.0557],\n",
      "        [-0.0393],\n",
      "        [ 0.1171],\n",
      "        [ 0.0558],\n",
      "        [-0.0393],\n",
      "        [-0.0364]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0679, 0.0677, 0.0682, 0.0680, 0.0676, 0.0683, 0.0680, 0.0678],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0672, -0.9330, -0.9324,  0.0673,  0.0669,  0.0676,  0.0673,  1.0671],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([1., 0., 0., 0., 0., 0., 1., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0534,  0.1156, -0.0356],\n",
      "        [ 0.0530,  0.1163, -0.0346],\n",
      "        [ 0.0534,  0.1159, -0.0355],\n",
      "        [ 0.0527,  0.1159, -0.0341],\n",
      "        [ 0.0534,  0.1159, -0.0357],\n",
      "        [ 0.0531,  0.1155, -0.0333],\n",
      "        [ 0.0528,  0.1152, -0.0325],\n",
      "        [ 0.0533,  0.1159, -0.0356]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0356],\n",
      "        [ 0.0530],\n",
      "        [-0.0355],\n",
      "        [ 0.1159],\n",
      "        [-0.0357],\n",
      "        [ 0.1155],\n",
      "        [-0.0325],\n",
      "        [-0.0356]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0682, 0.0685, 0.0683, 0.0684, 0.0685, 0.0676, 0.0680, 0.0684],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([1.0676, 0.0678, 0.0676, 0.0678, 0.0678, 0.0670, 1.0673, 0.0677],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0484,  0.1121, -0.0244],\n",
      "        [ 0.0484,  0.1107, -0.0241],\n",
      "        [ 0.0485,  0.1114, -0.0257],\n",
      "        [ 0.0483,  0.1110, -0.0256],\n",
      "        [ 0.0489,  0.1114, -0.0269],\n",
      "        [ 0.0489,  0.1115, -0.0271],\n",
      "        [ 0.0489,  0.1114, -0.0272],\n",
      "        [ 0.0487,  0.1112, -0.0256]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0244],\n",
      "        [ 0.1107],\n",
      "        [ 0.0485],\n",
      "        [ 0.1110],\n",
      "        [-0.0269],\n",
      "        [-0.0271],\n",
      "        [-0.0272],\n",
      "        [ 0.1112]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0686, 0.0680, 0.0685, 0.0684, 0.0683, 0.0683, 0.0683, 0.0681],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0679,  0.0673, -0.9322,  0.0678,  0.0677, -0.9324,  0.0676,  0.0674],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1., -1.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0418,  0.1061, -0.0190],\n",
      "        [ 0.0425,  0.1061, -0.0207],\n",
      "        [ 0.0425,  0.1060, -0.0203],\n",
      "        [ 0.0420,  0.1061, -0.0188],\n",
      "        [ 0.0421,  0.1065, -0.0195],\n",
      "        [ 0.0418,  0.1063, -0.0192],\n",
      "        [ 0.0421,  0.1061, -0.0199],\n",
      "        [ 0.0425,  0.1061, -0.0204]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.1061],\n",
      "        [-0.0207],\n",
      "        [-0.0203],\n",
      "        [ 0.0420],\n",
      "        [ 0.0421],\n",
      "        [ 0.1063],\n",
      "        [ 0.0421],\n",
      "        [-0.0204]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0690, 0.0686, 0.0684, 0.0684, 0.0687, 0.0686, 0.0687, 0.0685],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0683, -0.9321, -0.9322,  0.0677,  0.0680,  0.0679, -0.9320,  0.0678],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0354,  0.1011, -0.0172],\n",
      "        [ 0.0352,  0.1005, -0.0167],\n",
      "        [ 0.0359,  0.1009, -0.0181],\n",
      "        [ 0.0359,  0.1009, -0.0181],\n",
      "        [ 0.0353,  0.1015, -0.0170],\n",
      "        [ 0.0355,  0.1013, -0.0163],\n",
      "        [ 0.0359,  0.1009, -0.0179],\n",
      "        [ 0.0353,  0.1011, -0.0171]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0172],\n",
      "        [ 0.1005],\n",
      "        [-0.0181],\n",
      "        [-0.0181],\n",
      "        [ 0.0353],\n",
      "        [ 0.0355],\n",
      "        [-0.0179],\n",
      "        [ 0.1011]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0684, 0.0686, 0.0685, 0.0686, 0.0689, 0.0688, 0.0687, 0.0688],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0677, 0.0679, 0.0678, 0.0679, 0.0682, 0.0681, 0.0680, 0.0681],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 1.,  0.,  0.,  0.,  0.,  0., -1.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0300,  0.0963, -0.0138],\n",
      "        [ 0.0304,  0.0965, -0.0155],\n",
      "        [ 0.0305,  0.0965, -0.0153],\n",
      "        [ 0.0296,  0.0957, -0.0122],\n",
      "        [ 0.0298,  0.0967, -0.0141],\n",
      "        [ 0.0300,  0.0970, -0.0134],\n",
      "        [ 0.0304,  0.0965, -0.0153],\n",
      "        [ 0.0304,  0.0966, -0.0154]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0138],\n",
      "        [-0.0155],\n",
      "        [-0.0153],\n",
      "        [-0.0122],\n",
      "        [ 0.0967],\n",
      "        [-0.0134],\n",
      "        [-0.0153],\n",
      "        [-0.0154]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0684, 0.0687, 0.0686, 0.0685, 0.0686, 0.0682, 0.0687, 0.0687],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 1.0678,  0.0680,  0.0680,  0.0679,  0.0679,  0.0675, -0.9320,  0.0680],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0258,  0.0927, -0.0123],\n",
      "        [ 0.0259,  0.0928, -0.0128],\n",
      "        [ 0.0253,  0.0928, -0.0099],\n",
      "        [ 0.0257,  0.0925, -0.0104],\n",
      "        [ 0.0252,  0.0924, -0.0110],\n",
      "        [ 0.0258,  0.0926, -0.0122],\n",
      "        [ 0.0249,  0.0926, -0.0104],\n",
      "        [ 0.0253,  0.0918, -0.0097]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0123],\n",
      "        [-0.0128],\n",
      "        [-0.0099],\n",
      "        [-0.0104],\n",
      "        [ 0.0924],\n",
      "        [-0.0122],\n",
      "        [ 0.0926],\n",
      "        [ 0.0918]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0688, 0.0688, 0.0687, 0.0681, 0.0690, 0.0686, 0.0681, 0.0683],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0681, -0.9319,  0.0681,  0.0674,  0.0683,  0.0679,  0.0674,  0.0676],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0., -1.,  1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0210,  0.0883, -0.0082],\n",
      "        [ 0.0210,  0.0882, -0.0081],\n",
      "        [ 0.0216,  0.0889, -0.0113],\n",
      "        [ 0.0216,  0.0889, -0.0115],\n",
      "        [ 0.0212,  0.0890, -0.0111],\n",
      "        [ 0.0210,  0.0882, -0.0082],\n",
      "        [ 0.0211,  0.0886, -0.0101],\n",
      "        [ 0.0207,  0.0887, -0.0097]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0082],\n",
      "        [ 0.0882],\n",
      "        [-0.0113],\n",
      "        [-0.0115],\n",
      "        [-0.0111],\n",
      "        [-0.0082],\n",
      "        [ 0.0886],\n",
      "        [ 0.0887]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0686, 0.0686, 0.0688, 0.0689, 0.0687, 0.0686, 0.0690, 0.0686],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0679,  0.0679, -0.9319,  0.0682, -0.9319,  1.0679,  0.0683,  0.0679],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0177,  0.0848, -0.0092],\n",
      "        [ 0.0182,  0.0855, -0.0119],\n",
      "        [ 0.0181,  0.0855, -0.0118],\n",
      "        [ 0.0177,  0.0856, -0.0108],\n",
      "        [ 0.0181,  0.0857, -0.0119],\n",
      "        [ 0.0177,  0.0859, -0.0096],\n",
      "        [ 0.0180,  0.0859, -0.0119],\n",
      "        [ 0.0182,  0.0856, -0.0119]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0848],\n",
      "        [-0.0119],\n",
      "        [-0.0118],\n",
      "        [ 0.0177],\n",
      "        [-0.0119],\n",
      "        [-0.0096],\n",
      "        [-0.0119],\n",
      "        [-0.0119]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0681, 0.0689, 0.0692, 0.0692, 0.0691, 0.0692, 0.0688, 0.0690],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0674, 0.0682, 0.0685, 0.0685, 0.0684, 0.0685, 0.0681, 0.0683],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0156,  0.0825, -0.0113],\n",
      "        [ 0.0149,  0.0818, -0.0083],\n",
      "        [ 0.0149,  0.0830, -0.0089],\n",
      "        [ 0.0154,  0.0826, -0.0113],\n",
      "        [ 0.0147,  0.0823, -0.0087],\n",
      "        [ 0.0149,  0.0830, -0.0090],\n",
      "        [ 0.0154,  0.0826, -0.0113],\n",
      "        [ 0.0153,  0.0826, -0.0113]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0113],\n",
      "        [-0.0083],\n",
      "        [ 0.0830],\n",
      "        [-0.0113],\n",
      "        [ 0.0823],\n",
      "        [-0.0090],\n",
      "        [-0.0113],\n",
      "        [-0.0113]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0694, 0.0685, 0.0689, 0.0692, 0.0687, 0.0690, 0.0690, 0.0693],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0687, 0.0678, 0.0682, 0.0685, 0.0680, 0.0683, 0.0683, 0.0686],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  1.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0129,  0.0797, -0.0094],\n",
      "        [ 0.0121,  0.0791, -0.0060],\n",
      "        [ 0.0124,  0.0797, -0.0086],\n",
      "        [ 0.0119,  0.0795, -0.0071],\n",
      "        [ 0.0126,  0.0797, -0.0102],\n",
      "        [ 0.0126,  0.0797, -0.0080],\n",
      "        [ 0.0126,  0.0798, -0.0083],\n",
      "        [ 0.0130,  0.0796, -0.0094]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0094],\n",
      "        [ 0.0791],\n",
      "        [ 0.0124],\n",
      "        [ 0.0795],\n",
      "        [-0.0102],\n",
      "        [-0.0080],\n",
      "        [-0.0083],\n",
      "        [-0.0094]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0693, 0.0689, 0.0694, 0.0693, 0.0690, 0.0692, 0.0686, 0.0695],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0686,  0.0682,  1.0687,  0.0686, -0.9317,  0.0685,  0.0680,  0.0688],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([0., 0., 0., 0., 1., 0., 0., 0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0126,  0.0789, -0.0095],\n",
      "        [ 0.0121,  0.0794, -0.0073],\n",
      "        [ 0.0122,  0.0789, -0.0077],\n",
      "        [ 0.0126,  0.0790, -0.0095],\n",
      "        [ 0.0120,  0.0791, -0.0080],\n",
      "        [ 0.0117,  0.0787, -0.0076],\n",
      "        [ 0.0119,  0.0787, -0.0077],\n",
      "        [ 0.0120,  0.0788, -0.0085]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0095],\n",
      "        [ 0.0121],\n",
      "        [-0.0077],\n",
      "        [-0.0095],\n",
      "        [ 0.0791],\n",
      "        [ 0.0117],\n",
      "        [ 0.0787],\n",
      "        [-0.0085]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0692, 0.0692, 0.0689, 0.0693, 0.0691, 0.0693, 0.0694, 0.0690],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([0.0685, 0.0685, 0.0682, 0.0686, 1.0684, 0.0686, 0.0687, 0.0684],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0., -1.,  0.,  1.,  0.,  0.,  0.,  1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0131,  0.0808, -0.0079],\n",
      "        [ 0.0136,  0.0807, -0.0089],\n",
      "        [ 0.0131,  0.0808, -0.0078],\n",
      "        [ 0.0132,  0.0807, -0.0073],\n",
      "        [ 0.0132,  0.0808, -0.0071],\n",
      "        [ 0.0131,  0.0808, -0.0079],\n",
      "        [ 0.0131,  0.0812, -0.0077],\n",
      "        [ 0.0131,  0.0802, -0.0063]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0131],\n",
      "        [-0.0089],\n",
      "        [ 0.0131],\n",
      "        [-0.0073],\n",
      "        [-0.0071],\n",
      "        [ 0.0131],\n",
      "        [-0.0077],\n",
      "        [ 0.0131]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0695, 0.0692, 0.0695, 0.0690, 0.0690, 0.0695, 0.0692, 0.0687],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0688, -0.9315,  0.0688,  1.0683,  0.0683,  0.0688,  0.0685,  1.0680],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[2],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0.,  0., -1.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0161,  0.0834, -0.0080],\n",
      "        [ 0.0155,  0.0834, -0.0068],\n",
      "        [ 0.0156,  0.0833, -0.0062],\n",
      "        [ 0.0161,  0.0835, -0.0081],\n",
      "        [ 0.0156,  0.0838, -0.0057],\n",
      "        [ 0.0158,  0.0834, -0.0089],\n",
      "        [ 0.0152,  0.0833, -0.0064],\n",
      "        [ 0.0154,  0.0835, -0.0054]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[-0.0080],\n",
      "        [-0.0068],\n",
      "        [ 0.0833],\n",
      "        [-0.0081],\n",
      "        [ 0.0838],\n",
      "        [-0.0089],\n",
      "        [ 0.0833],\n",
      "        [-0.0054]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0694, 0.0695, 0.0694, 0.0694, 0.0693, 0.0692, 0.0690, 0.0691],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0687,  0.0688, -0.9313,  0.0687,  0.0686, -0.9314,  0.0683,  0.0684],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[0],\n",
      "        [0],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0168,  0.0832, -0.0072],\n",
      "        [ 0.0170,  0.0828, -0.0078],\n",
      "        [ 0.0173,  0.0833, -0.0083],\n",
      "        [ 0.0167,  0.0827, -0.0052],\n",
      "        [ 0.0173,  0.0833, -0.0086],\n",
      "        [ 0.0168,  0.0823, -0.0053],\n",
      "        [ 0.0171,  0.0834, -0.0087],\n",
      "        [ 0.0173,  0.0832, -0.0084]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0168],\n",
      "        [ 0.0170],\n",
      "        [-0.0083],\n",
      "        [ 0.0827],\n",
      "        [-0.0086],\n",
      "        [ 0.0168],\n",
      "        [-0.0087],\n",
      "        [-0.0084]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0697, 0.0693, 0.0695, 0.0692, 0.0696, 0.0689, 0.0698, 0.0694],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0690,  0.0686,  0.0688,  0.0685, -0.9311,  0.0682,  0.0691,  0.0687],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "non_final_mask.shape: torch.Size([8])\n",
      "State batch before stack length: 8\n",
      "State batch after stack shape: torch.Size([8, 4, 84, 84])\n",
      "Action batch: tensor([[1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [2],\n",
      "        [0],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]], device='cuda:0'), shape: torch.Size([8, 1])\n",
      "reward batch: tensor([ 0.,  0., -1.,  0., -1., -1.,  0., -1.], device='cuda:0'), shape: torch.Size([8])\n",
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n",
      "policy_net(state): tensor([[ 0.0176,  0.0828, -0.0085],\n",
      "        [ 0.0176,  0.0830, -0.0086],\n",
      "        [ 0.0176,  0.0834, -0.0091],\n",
      "        [ 0.0178,  0.0830, -0.0079],\n",
      "        [ 0.0179,  0.0834, -0.0095],\n",
      "        [ 0.0183,  0.0832, -0.0100],\n",
      "        [ 0.0178,  0.0837, -0.0078],\n",
      "        [ 0.0184,  0.0833, -0.0105]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Q Value: tensor([[ 0.0828],\n",
      "        [ 0.0830],\n",
      "        [ 0.0176],\n",
      "        [-0.0079],\n",
      "        [ 0.0179],\n",
      "        [-0.0100],\n",
      "        [-0.0078],\n",
      "        [-0.0105]], device='cuda:0', grad_fn=<GatherBackward0>), shape: torch.Size([8, 1])\n",
      "Next state values: tensor([0.0692, 0.0699, 0.0694, 0.0693, 0.0697, 0.0695, 0.0694, 0.0695],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Expected Q values: tensor([ 0.0685,  0.0692, -0.9313,  0.0686, -0.9310, -0.9312,  0.0687, -0.9312],\n",
      "       device='cuda:0'), shape: torch.Size([8])\n",
      "Episode 0, duration: 1250, total reward: -41.00, steps: 1250\n",
      "Training Complete!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved as 'models/dino_final.pth'\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# Training configuration\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "    num_episodes =  1\n",
    "else:\n",
    "    num_episodes = 1\n",
    "\n",
    "print(f\"Starting training for {num_episodes} episodes...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Learning will start after {LEARNING_START} steps\")\n",
    "\n",
    "n_steps = 0  # Global step counter\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = preprocess_observation(state)\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for t in count():\n",
    "        n_steps += 1\n",
    "        \n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(dino.Action(action.item()))\n",
    "        \n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Start learning after collecting enough experiences\n",
    "        if n_steps > LEARNING_START:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            if n_steps % OPTIMIZE_FREQ == 0:\n",
    "                optimize_model()\n",
    "\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if n_steps % TARGET_UPDATE_FREQ == 0:\n",
    "                # target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "                target_net_state_dict = target_net.state_dict()\n",
    "                policy_net_state_dict = policy_net.state_dict()\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "                target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(total_reward)\n",
    "            if i_episode % 100 == 0:\n",
    "                print(f\"Episode {i_episode}, duration: {t + 1}, total reward: {total_reward:.2f}, steps: {n_steps}\")\n",
    "            # plot_durations()\n",
    "            break\n",
    "        else:\n",
    "            next_state = preprocess_observation(observation)\n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            # Store the transition in memory\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "    # Save model periodically\n",
    "    # if i_episode % 200 == 0 and i_episode > 0:\n",
    "    #     torch.save(policy_net.state_dict(), f\"models/dino_episode_{i_episode}.pth\")\n",
    "    #     # save_obs_result(i_episode, env.frames)\n",
    "    #     print(f\"Model saved at episode {i_episode}\")\n",
    "env.close()\n",
    "\n",
    "print('Training Complete!')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Save the final trained model\n",
    "# torch.save(policy_net.state_dict(), \"models/dino_final.pth\")\n",
    "# print(\"Final model saved as 'models/dino_final.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "709396b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0af78540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 84, 84])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_observation(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29ab95cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = select_action(state)\n",
    "action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "653ff412",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = memory.sample(BATCH_SIZE)\n",
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "95e571de",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f710dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = []\n",
    "for state in batch.state:\n",
    "    if isinstance(state, np.ndarray):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    state_batch.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "af1f143a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bdcec0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = torch.stack(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ac9cbbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 84, 84])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2e283a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_batch = torch.cat(batch.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5b1f292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "41e2de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-final next states shape: torch.Size([8, 4, 84, 84])\n"
     ]
    }
   ],
   "source": [
    "non_final_next_states = []\n",
    "for state in batch.next_state:\n",
    "    if state is not None:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        non_final_next_states.append(state)\n",
    "\n",
    "if len(non_final_next_states) > 0:\n",
    "    non_final_next_states = torch.stack(non_final_next_states)\n",
    "else:\n",
    "    non_final_next_states = torch.empty(0, n_observations, 84, 84, device=device)\n",
    "print(f\"Non-final next states shape: {non_final_next_states.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "14681ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = policy_net(state_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46427b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0158,  0.0821, -0.0120],\n",
       "        [ 0.0161,  0.0814, -0.0142],\n",
       "        [ 0.0158,  0.0812, -0.0118],\n",
       "        [ 0.0157,  0.0810, -0.0110],\n",
       "        [ 0.0159,  0.0817, -0.0138],\n",
       "        [ 0.0164,  0.0816, -0.0146],\n",
       "        [ 0.0159,  0.0814, -0.0122],\n",
       "        [ 0.0157,  0.0818, -0.0131]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "87601377",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = raw.gather(1, action_batch.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2d8f53a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ce8ae9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "with torch.no_grad():\n",
    "    if len(non_final_next_states) > 0:\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8589c98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0696, 0.0694, 0.0693, 0.0686, 0.0698, 0.0696, 0.0693, 0.0696],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "064acf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model management utilities ready!\n"
     ]
    }
   ],
   "source": [
    "# Model management utilities\n",
    "def save_model(model, filename):\n",
    "    \"\"\"Save model with timestamp\"\"\"\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(f\"Model saved as: {filename}\")\n",
    "\n",
    "def load_model(filename, n_observations, n_actions):\n",
    "    \"\"\"Load model from file\"\"\"\n",
    "    model = DQN(n_observations, n_actions).to(device)\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# save_model(policy_net, \"my_model.pth\")\n",
    "# loaded_model = load_model(\"my_model.pth\", n_observations, n_actions)\n",
    "print(\"Model management utilities ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13fc6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy action selection for evaluation (no exploration)\n",
    "def select_action_greedy(state, model):\n",
    "    \"\"\"Select action greedily without exploration for evaluation\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor if needed\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Ensure state has correct batch dimension\n",
    "        if len(state.shape) == 3:  # [C, H, W] -> [1, C, H, W]\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        # Get Q-values and select the action with highest Q-value\n",
    "        q_values = model(state)\n",
    "        action = q_values.max(dim=1)[1]\n",
    "        return action\n",
    "\n",
    "# Evaluation and performance metrics\n",
    "def evaluate_agent(model, num_episodes=10, max_steps=1000):\n",
    "    \"\"\"Evaluate agent performance over multiple episodes\"\"\"\n",
    "    print(f\"📊 Evaluating agent over {num_episodes} episodes...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, info = env.reset()\n",
    "        state = preprocess_observation(state)\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select action greedily (no exploration)\n",
    "            action = select_action_greedy(state, model)\n",
    "            \n",
    "            # Take action\n",
    "            observation, reward, terminated, truncated, _ = env.step(dino.Action(action.item()))\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Update state\n",
    "            if not (terminated or truncated):\n",
    "                state = preprocess_observation(observation)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        \n",
    "        if (episode + 1) % 5 == 0:\n",
    "            print(f\"  Episode {episode + 1}/{num_episodes} completed - Length: {steps}, Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    avg_length = np.mean(episode_lengths)\n",
    "    max_length = np.max(episode_lengths)\n",
    "    min_length = np.min(episode_lengths)\n",
    "    \n",
    "    print(\"\\n🏆 Performance Summary:\")\n",
    "    print(f\"   📈 Average Reward: {avg_reward:.2f} ± {std_reward:.2f}\")\n",
    "    print(f\"   ⏱️  Average Survival: {avg_length:.1f} steps\")\n",
    "    print(f\"   🥇 Best Performance: {max_length} steps\")\n",
    "    print(f\"   📉 Worst Performance: {min_length} steps\")\n",
    "    print(f\"   💯 Success Rate: {(np.array(episode_lengths) > 100).mean()*100:.1f}% (>100 steps)\")\n",
    "    \n",
    "    # Plot performance distribution\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(episode_rewards, bins=10, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.title('Reward Distribution')\n",
    "    plt.xlabel('Total Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(avg_reward, color='red', linestyle='--', label=f'Mean: {avg_reward:.2f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(episode_lengths, bins=10, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.title('Survival Time Distribution')\n",
    "    plt.xlabel('Steps Survived')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(avg_length, color='red', linestyle='--', label=f'Mean: {avg_length:.1f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(range(1, num_episodes + 1), episode_lengths, 'o-', alpha=0.7)\n",
    "    plt.title('Performance Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Survived')\n",
    "    plt.axhline(avg_length, color='red', linestyle='--', label=f'Average: {avg_length:.1f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'lengths': episode_lengths,\n",
    "        'avg_reward': avg_reward,\n",
    "        'avg_length': avg_length,\n",
    "        'max_length': max_length\n",
    "    }\n",
    "\n",
    "# Load and evaluate the trained agent\n",
    "def load_and_evaluate(model_path, num_episodes=20):\n",
    "    print(f\"🧪 Loading and evaluating trained Chrome Dino agent from {model_path}...\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    trained_model = DQN(n_observations, n_actions).to(device)\n",
    "    trained_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    trained_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    performance = evaluate_agent(trained_model, num_episodes=num_episodes)\n",
    "    return performance\n",
    "\n",
    "# You can evaluate the model after training with:\n",
    "# performance = load_and_evaluate(\"dino_final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ddc79ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Loading and evaluating trained Chrome Dino agent from dino_final_model.pth...\n",
      "📊 Evaluating agent over 20 episodes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m performance \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdino_final_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 117\u001b[0m, in \u001b[0;36mload_and_evaluate\u001b[0;34m(model_path, num_episodes)\u001b[0m\n\u001b[1;32m    114\u001b[0m trained_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m    115\u001b[0m trained_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m performance \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m performance\n",
      "Cell \u001b[0;32mIn[77], line 37\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(model, num_episodes, max_steps)\u001b[0m\n\u001b[1;32m     34\u001b[0m action \u001b[38;5;241m=\u001b[39m select_action_greedy(state, model)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Take action\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdino\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     39\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/media/hungchan/Storage/Works/xai/X AI/Code/Chrome Dino/dino.py:482\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    479\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk):\n\u001b[0;32m--> 482\u001b[0m     obs, reward, term, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(obs)\n\u001b[1;32m    484\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(obs)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/gymnasium/core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_env/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/hungchan/Storage/Works/xai/X AI/Code/Chrome Dino/dino.py:377\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# Should we spawn a new obstacle?\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spawn_obstacle_maybe()\n\u001b[0;32m--> 377\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/media/hungchan/Storage/Works/xai/X AI/Code/Chrome Dino/dino.py:420\u001b[0m, in \u001b[0;36mEnv._render_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# Return the canvas as a rgb array\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixels3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "performance = load_and_evaluate(\"dino_final_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbabd89",
   "metadata": {},
   "source": [
    "# 🔍 Deep Q-Network (DQN) Algorithm Explained\n",
    "\n",
    "## Overview\n",
    "The Deep Q-Network (DQN) algorithm combines Q-learning with deep neural networks to learn optimal policies in environments with high-dimensional state spaces (like images). Let's break down each step and examine the tensor sizes throughout the process.\n",
    "\n",
    "## 🏗️ Architecture Components\n",
    "\n",
    "### 1. **Neural Network Architecture**\n",
    "```\n",
    "Input: RGB Image (512, 1024, 3) → Preprocessing → Grayscale (84, 84, 1)\n",
    "\n",
    "CNN Architecture:\n",
    "- Conv2d(1, 32, kernel_size=8, stride=4): (1, 84, 84) → (32, 20, 20)\n",
    "- Conv2d(32, 64, kernel_size=4, stride=2): (32, 20, 20) → (64, 9, 9) \n",
    "- Conv2d(64, 64, kernel_size=3, stride=1): (64, 9, 9) → (64, 7, 7)\n",
    "- Flatten: (64, 7, 7) → (3136,)\n",
    "- Linear(3136, 256): (3136,) → (256,)\n",
    "- Linear(256, 3): (256,) → (3,)  # 3 actions: STAND, JUMP, DUCK\n",
    "```\n",
    "\n",
    "### 2. **Key Hyperparameters**\n",
    "- **BATCH_SIZE = 64**: Number of experiences sampled from replay buffer\n",
    "- **GAMMA = 0.99**: Discount factor for future rewards\n",
    "- **EPS_START = 0.9**: Initial exploration probability\n",
    "- **EPS_END = 0.01**: Final exploration probability\n",
    "- **TAU = 0.005**: Soft update rate for target network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e1bbf",
   "metadata": {},
   "source": [
    "# 🚀 Key Improvements Made to Fix DQN Performance\n",
    "\n",
    "## 🔧 **Major Issues Fixed:**\n",
    "\n",
    "### 1. **Environment Setup**\n",
    "- ✅ **Fixed import**: Changed from `import dino` to `import dino` \n",
    "- ✅ **Added train mode**: Using `game_mode='train'` for better learning (negative rewards instead of episode termination)\n",
    "- ✅ **Frame stacking**: Added `dino.Wrapper(env, k=4, image_size=(84, 84))` for temporal information\n",
    "- ✅ **Proper episode limits**: Set `train_frame_limit=1000` for consistent training episodes\n",
    "\n",
    "### 2. **Network Architecture** \n",
    "- ✅ **Fixed linear layer size**: Corrected from 3136 to 3072 to match the reference implementation\n",
    "- ✅ **Simplified preprocessing**: Removed complex tensor reshaping since wrapper handles it\n",
    "- ✅ **Proper input handling**: Network now expects `(batch, 4, 84, 84)` from frame stacking\n",
    "\n",
    "### 3. **Training Parameters**\n",
    "- ✅ **Better hyperparameters**: \n",
    "  - Batch size: 64 → 32 (more stable)\n",
    "  - Learning rate: 3e-4 → 1e-4 (more conservative)\n",
    "  - Epsilon start: 0.9 → 1.0 (full exploration initially)\n",
    "  - Epsilon decay: 2500 → 10000 (longer exploration period)\n",
    "- ✅ **Experience replay**: 10,000 buffer size with proper learning start after 10,000 steps\n",
    "- ✅ **Target network updates**: Hard updates every 1,000 steps instead of soft updates\n",
    "\n",
    "### 4. **Training Loop**\n",
    "- ✅ **Proper action handling**: Using `dino.Action(action.item())` for environment steps\n",
    "- ✅ **Learning schedule**: Only start optimization after collecting enough experiences\n",
    "- ✅ **Progress monitoring**: Better logging and model saving every 50 episodes\n",
    "- ✅ **Reduced episodes**: 1,000 episodes instead of 10,000 for faster, more focused training\n",
    "\n",
    "### 5. **Data Processing**\n",
    "- ✅ **Simplified preprocessing**: Wrapper handles grayscale conversion and frame stacking\n",
    "- ✅ **Consistent tensor handling**: Proper numpy-to-tensor conversions throughout\n",
    "- ✅ **Robust batch processing**: Better handling of terminal states and tensor shapes\n",
    "\n",
    "## 🎯 **Expected Performance Improvements:**\n",
    "\n",
    "1. **Faster Learning**: Train mode prevents early termination, allowing more learning per episode\n",
    "2. **Better Temporal Understanding**: 4-frame stacking helps agent understand movement and timing\n",
    "3. **More Stable Training**: Proper hyperparameters reduce training instability\n",
    "4. **Efficient Exploration**: Longer epsilon decay ensures thorough exploration of the action space\n",
    "5. **Robust Architecture**: Fixed network dimensions prevent runtime errors\n",
    "\n",
    "## 🏆 **Training Tips:**\n",
    "\n",
    "- **Monitor epsilon**: Should start at 1.0 and gradually decrease to 0.01\n",
    "- **Watch episode lengths**: Should gradually increase as agent learns\n",
    "- **Check memory usage**: Should start learning after 10,000 steps\n",
    "- **Save regularly**: Models are saved every 50 episodes for safety\n",
    "\n",
    "The implementation now follows the proven patterns from the reference code and should achieve much better performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0c17c5",
   "metadata": {},
   "source": [
    "## 🔄 Training Loop: Step-by-Step Breakdown\n",
    "\n",
    "### **Step 1: Environment Interaction**\n",
    "```python\n",
    "state, info = env.reset()  # Initial state\n",
    "state = preprocess_observation(state)  # (512, 1024, 3) → (1, 84, 84)\n",
    "```\n",
    "- **Input**: Raw RGB image from Chrome Dino game\n",
    "- **Output**: Preprocessed grayscale tensor `(1, 84, 84)`\n",
    "\n",
    "### **Step 2: Action Selection (ε-greedy)**\n",
    "```python\n",
    "action = select_action(state)  # Uses ε-greedy policy\n",
    "```\n",
    "- **Input**: State tensor `(1, 84, 84)`\n",
    "- **Process**: \n",
    "  - Forward pass through policy network: `(1, 84, 84)` → `(3,)` Q-values\n",
    "  - With probability ε: select random action\n",
    "  - With probability (1-ε): select argmax(Q-values)\n",
    "- **Output**: Action tensor `(1,)` with value in {0, 1, 2}\n",
    "\n",
    "### **Step 3: Environment Step**\n",
    "```python\n",
    "observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "```\n",
    "- **Input**: Action integer (0=STAND, 1=JUMP, 2=DUCK)\n",
    "- **Output**: \n",
    "  - New observation: `(512, 1024, 3)`\n",
    "  - Reward: scalar (typically 0.0, 1.0 for passing obstacle, -1.0 for collision)\n",
    "  - Terminal flags: boolean\n",
    "\n",
    "### **Step 4: Experience Storage**\n",
    "```python\n",
    "memory.push(state, action, next_state, reward)\n",
    "```\n",
    "- **Stored transition**: `(state, action, next_state, reward)`\n",
    "- **Sizes**: `((1,84,84), (1,), (1,84,84), (1,))`\n",
    "- **Memory capacity**: 10,000 transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582d815",
   "metadata": {},
   "source": [
    "## 🎯 Optimization Step: The Heart of DQN\n",
    "\n",
    "### **Step 5: Batch Sampling & Processing**\n",
    "```python\n",
    "transitions = memory.sample(BATCH_SIZE)  # Sample 64 random experiences\n",
    "batch = Transition(*zip(*transitions))   # Transpose to batch format\n",
    "```\n",
    "\n",
    "**Batch Components After Processing:**\n",
    "- **state_batch**: `(64, 1, 84, 84)` - 64 current states\n",
    "- **action_batch**: `(64, 1)` - 64 actions taken  \n",
    "- **next_state_batch**: `(N, 1, 84, 84)` - N non-terminal next states (N ≤ 64)\n",
    "- **reward_batch**: `(64,)` - 64 immediate rewards\n",
    "- **non_final_mask**: `(64,)` - Boolean mask for non-terminal states\n",
    "\n",
    "### **Step 6: Current Q-Value Computation**\n",
    "```python\n",
    "state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "```\n",
    "- **policy_net(state_batch)**: `(64, 1, 84, 84)` → `(64, 3)` Q-values for all actions\n",
    "- **gather(1, action_batch)**: `(64, 3)` + `(64, 1)` → `(64, 1)` selected Q-values\n",
    "- **Output**: Q(s,a) for the actions actually taken\n",
    "\n",
    "### **Step 7: Target Q-Value Computation**\n",
    "```python\n",
    "next_state_values = torch.zeros(BATCH_SIZE, device=device)  # (64,)\n",
    "next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "```\n",
    "- **target_net(non_final_next_states)**: `(N, 1, 84, 84)` → `(N, 3)` \n",
    "- **.max(1).values**: `(N, 3)` → `(N,)` - Best Q-values for next states\n",
    "- **Expected Q-values**: `(64,)` using Bellman equation: Q_target = r + γ * max_a'(Q(s',a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070dfd8",
   "metadata": {},
   "source": [
    "### **Step 8: Loss Computation & Backpropagation**\n",
    "```python\n",
    "criterion = nn.SmoothL1Loss()\n",
    "loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "```\n",
    "- **state_action_values**: `(64, 1)` - Current Q-values Q(s,a)\n",
    "- **expected_state_action_values**: `(64, 1)` - Target Q-values \n",
    "- **loss**: Scalar - Huber loss between predicted and target Q-values\n",
    "- **SmoothL1Loss**: More robust than MSE, less sensitive to outliers\n",
    "\n",
    "### **Step 9: Gradient Descent Update**\n",
    "```python\n",
    "optimizer.zero_grad()           # Clear previous gradients\n",
    "loss.backward()                 # Compute gradients via backpropagation\n",
    "torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)  # Gradient clipping\n",
    "optimizer.step()                # Update network weights\n",
    "```\n",
    "- **Gradient clipping**: Prevents exploding gradients by limiting magnitude to 100\n",
    "- **AdamW optimizer**: Adaptive learning rate with weight decay\n",
    "\n",
    "### **Step 10: Target Network Soft Update**\n",
    "```python\n",
    "target_net_state_dict = target_net.state_dict()\n",
    "policy_net_state_dict = policy_net.state_dict()\n",
    "for key in policy_net_state_dict:\n",
    "    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "target_net.load_state_dict(target_net_state_dict)\n",
    "```\n",
    "- **Soft update**: θ_target = τ * θ_policy + (1-τ) * θ_target\n",
    "- **TAU = 0.005**: Very slow update to maintain stability\n",
    "- **Purpose**: Reduces correlation between current and target Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53707b39",
   "metadata": {},
   "source": [
    "## 🧠 Key DQN Innovations & Insights\n",
    "\n",
    "### **1. Experience Replay**\n",
    "- **Problem**: Sequential experiences are correlated, leading to unstable learning\n",
    "- **Solution**: Store experiences in replay buffer, sample randomly for training\n",
    "- **Benefit**: Breaks correlation, enables multiple learning from same experience\n",
    "\n",
    "### **2. Target Network**\n",
    "- **Problem**: Using same network for current and target Q-values causes instability\n",
    "- **Solution**: Separate target network updated slowly (τ = 0.005)\n",
    "- **Benefit**: Stabilizes training by providing consistent targets\n",
    "\n",
    "### **3. ε-greedy Exploration**\n",
    "- **Schedule**: ε decreases from 0.9 → 0.01 over 2500 steps\n",
    "- **Purpose**: Balance exploration (random actions) vs exploitation (greedy actions)\n",
    "- **Decay**: ε = 0.01 + (0.9 - 0.01) * exp(-steps/2500)\n",
    "\n",
    "## 📊 Complete Tensor Flow Summary\n",
    "\n",
    "```\n",
    "🎮 Game State (RGB Image)\n",
    "    ↓ (512, 1024, 3)\n",
    "🔄 Preprocessing \n",
    "    ↓ (1, 84, 84)\n",
    "🧠 Policy Network Forward Pass\n",
    "    ↓ (3,) Q-values\n",
    "🎯 Action Selection (ε-greedy)\n",
    "    ↓ (1,) Action\n",
    "🎮 Environment Step\n",
    "    ↓ Reward, Next State, Done\n",
    "💾 Store in Replay Buffer\n",
    "    ↓ (state, action, next_state, reward)\n",
    "🎲 Sample Batch (when buffer > 64)\n",
    "    ↓ Batch of 64 transitions\n",
    "🔧 Optimize Model:\n",
    "    • state_batch: (64, 1, 84, 84)\n",
    "    • Current Q-values: (64, 1) \n",
    "    • Target Q-values: (64, 1)\n",
    "    • Loss: Scalar (Huber Loss)\n",
    "    • Gradients: Backpropagate through CNN\n",
    "⚙️ Update Networks:\n",
    "    • Policy network: Full gradient update\n",
    "    • Target network: Soft update (τ=0.005)\n",
    "```\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "The agent learns to:\n",
    "1. **Recognize obstacles** in preprocessed grayscale images\n",
    "2. **Predict Q-values** for each action (STAND, JUMP, DUCK)\n",
    "3. **Maximize long-term reward** by avoiding collisions\n",
    "4. **Balance exploration vs exploitation** through ε-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365537a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 DQN Tensor Flow Demonstration\n",
      "==================================================\n",
      "\n",
      "📸 Step 1: Environment & Preprocessing\n",
      "   Raw observation: (4, 84, 84)\n",
      "   Processed state: torch.Size([4, 84, 84])\n",
      "\n",
      "🎯 Step 2: Action Selection\n",
      "   Input state: torch.Size([4, 84, 84])\n",
      "   Batched state: torch.Size([1, 4, 84, 84])\n",
      "   Q-values output: torch.Size([1, 3])\n",
      "   Q-values: [[0.28131863 0.29467183 0.2594876 ]]\n",
      "   Selected action: torch.Size([1]) (value: 1)\n",
      "\n",
      "🎮 Step 3: Environment Interaction\n",
      "   New observation: (4, 84, 84)\n",
      "   Reward: 0.0\n",
      "   Terminated: False\n",
      "\n",
      "🔧 Step 4: Batch Processing Example\n",
      "   Sampled 8 transitions\n",
      "   State batch: torch.Size([8, 4, 84, 84])\n",
      "   Action batch: torch.Size([8, 1])\n",
      "   Current Q-values: torch.Size([8, 3])\n",
      "   Selected Q-values: torch.Size([8, 1])\n",
      "   Next state batch: torch.Size([8, 4, 84, 84])\n",
      "   Next Q-values: torch.Size([8, 3])\n",
      "   Max next Q-values: torch.Size([8])\n",
      "\n",
      "✅ Tensor flow demonstration complete!\n",
      "   All shapes are consistent and ready for training! 🎉\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Practical Example: Tracing Tensor Shapes Through One Training Step\n",
    "\n",
    "def demonstrate_tensor_flow():\n",
    "    \"\"\"\n",
    "    Demonstrate the complete tensor flow through one training iteration\n",
    "    \"\"\"\n",
    "    print(\"🚀 DQN Tensor Flow Demonstration\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Environment reset and preprocessing\n",
    "    print(\"\\n📸 Step 1: Environment & Preprocessing\")\n",
    "    state, _ = env.reset()\n",
    "    print(f\"   Raw observation: {np.array(state).shape}\")\n",
    "    state = preprocess_observation(state)\n",
    "    print(f\"   Processed state: {state.shape}\")\n",
    "    \n",
    "    # Step 2: Action selection\n",
    "    print(\"\\n🎯 Step 2: Action Selection\")\n",
    "    print(f\"   Input state: {state.shape}\")\n",
    "    if len(state.shape) == 3:\n",
    "        state_batch = state.unsqueeze(0)\n",
    "    else:\n",
    "        state_batch = state\n",
    "    print(f\"   Batched state: {state_batch.shape}\")\n",
    "    \n",
    "    q_values = policy_net(state_batch)\n",
    "    print(f\"   Q-values output: {q_values.shape}\")\n",
    "    print(f\"   Q-values: {q_values.detach().cpu().numpy()}\")\n",
    "    \n",
    "    action = q_values.max(1)[1]\n",
    "    print(f\"   Selected action: {action.shape} (value: {action.item()})\")\n",
    "    \n",
    "    # Step 3: Environment step\n",
    "    print(\"\\n🎮 Step 3: Environment Interaction\")\n",
    "    observation, reward, terminated, _, _ = env.step(action.item())\n",
    "    print(f\"   New observation: {np.array(observation).shape}\")\n",
    "    print(f\"   Reward: {reward}\")\n",
    "    print(f\"   Terminated: {terminated}\")\n",
    "    \n",
    "    # Step 4: Demonstrate batch processing (if we have enough samples)\n",
    "    if len(memory) >= BATCH_SIZE:\n",
    "        print(\"\\n🔧 Step 4: Batch Processing Example\")\n",
    "        transitions = memory.sample(min(8, BATCH_SIZE))  # Small batch for demo\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        \n",
    "        print(f\"   Sampled {len(transitions)} transitions\")\n",
    "        \n",
    "        # Process states\n",
    "        fixed_states = []\n",
    "        for s in batch.state:\n",
    "            if len(s.shape) == 2:\n",
    "                s = s.unsqueeze(0)\n",
    "            elif len(s.shape) == 4:\n",
    "                s = s.squeeze(0)\n",
    "            fixed_states.append(s)\n",
    "        state_batch = torch.stack(fixed_states)\n",
    "        print(f\"   State batch: {state_batch.shape}\")\n",
    "        \n",
    "        # Process actions\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        if len(action_batch.shape) == 1:\n",
    "            action_batch = action_batch.unsqueeze(1)\n",
    "        print(f\"   Action batch: {action_batch.shape}\")\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = policy_net(state_batch)\n",
    "        print(f\"   Current Q-values: {current_q.shape}\")\n",
    "        \n",
    "        state_action_values = current_q.gather(1, action_batch)\n",
    "        print(f\"   Selected Q-values: {state_action_values.shape}\")\n",
    "        \n",
    "        # Process next states\n",
    "        valid_next_states = [s for s in batch.next_state if s is not None]\n",
    "        if len(valid_next_states) > 0:\n",
    "            fixed_next_states = []\n",
    "            for s in valid_next_states:\n",
    "                if len(s.shape) == 2:\n",
    "                    s = s.unsqueeze(0)\n",
    "                elif len(s.shape) == 4:\n",
    "                    s = s.squeeze(0)\n",
    "                fixed_next_states.append(s)\n",
    "            next_state_batch = torch.stack(fixed_next_states)\n",
    "            print(f\"   Next state batch: {next_state_batch.shape}\")\n",
    "            \n",
    "            # Target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q = target_net(next_state_batch)\n",
    "                print(f\"   Next Q-values: {next_q.shape}\")\n",
    "                max_next_q = next_q.max(1).values\n",
    "                print(f\"   Max next Q-values: {max_next_q.shape}\")\n",
    "    \n",
    "    print(\"\\n✅ Tensor flow demonstration complete!\")\n",
    "    print(\"   All shapes are consistent and ready for training! 🎉\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_tensor_flow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
